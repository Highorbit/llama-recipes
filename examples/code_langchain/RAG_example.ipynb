{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abcae6be-83d5-409e-8889-e3060b278d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import configparser\n",
    "import pymysql\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import configparser\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from user_info import get_user_data_search_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693682ef-21ed-455e-b669-f5c6fcb2c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b1e522-d2dc-450a-ac94-84f24155d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81cd19-28f1-4dcc-8bd1-d0ba726db55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b8f8c-4fe1-415c-aafe-414c0703b53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1aa842b-db71-4b76-b893-184aeb7c768c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/infoedge/llama-recipes/examples/configs/config.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config=configparser.ConfigParser()\n",
    "config.read('/home/ubuntu/infoedge/llama-recipes/examples/configs/config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193575dd-5f30-454a-a43d-0647bc5fe718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_key=config['key']['infoedge']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218333c1-fe7b-4346-a576-b1d6d1b9022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['OPENAI_API_KEY'] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ea02bc-124d-44ae-85e7-2faab22acd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a28f3eb-184a-43ab-bdc7-130aa1a4e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyPDFDirectoryLoader('/home/ubuntu/infoedge/llama-recipes/examples/custom_data/user_resume_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f5d528-483e-4141-9920-7118d367f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta-llama/Llama-2-7b-chat-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e85adfa4-a6b0-4071-8f15-2cea6d47da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/home/ubuntu/infoedge/llama-recipes/examples/custom_data/user_resume_data/uid_1103243.txt' #Your root data folder path\n",
    "# DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "\n",
    "DATA_PATH = '/home/ubuntu/infoedge/llama-recipes/examples/custom_data/user_resume_data_pdf_v1/' #Your root data folder path\n",
    "DB_FAISS_PATH = 'vectorstore/db_faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a28a1f-1f84-4f10-84c8-b3c1b20d7bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken:  1234.5676562786102\n"
     ]
    }
   ],
   "source": [
    "# loader = TextLoader(DATA_PATH)\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"Total time taken: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f29f3fe-0b94-40cb-9bf1-0ee3d53b7a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543363"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ffce76d-5eb4-49ec-9cc6-ce705954bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823465 page_content='User ID: 2762754\\nNAGARJUNA. S. BANAGAR Mob. +91 9820104547 | Email: nsbanagar@yahoo.co.in CAREER\\nSUMMARY________________________________________________________________ A Versatile\\nChemical Engineer having 32 yrs. of experience with expertise in Chemical &amp; Lubricating Oil Plant\\nOperations including Production, Maintenance, cost control, Supply Chain, Quality Control, EHS, Brown\\nfield Projects management, Statutory compliance, Lean Six Sigma greenbelt certified, seeks to cut costs,\\nstreamline Operations and increase Productivity through Process improvement &amp; Lean\\nManufacturing. CORE COMPETENCIES &amp;\\nSKILLS____________________________________________________________ Plant Operations,\\nPresentation People Management Vendor Negotiations Project Management ISO &amp; Statutory' metadata={'source': '/home/ubuntu/infoedge/llama-recipes/examples/custom_data/user_resume_data_pdf_v1/uid_2762754.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=5)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(len(splits), splits[325])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abd7fa8d-69cc-4d34-b1e3-8d41b4d63352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543363 User ID: 2527068\n",
      "Mohd Sheikh Sahil Lucknow, Uttarpradesh, India +91 9519265025 | shiekhsahil46@gmail\n"
     ]
    }
   ],
   "source": [
    "print(len(documents), documents[0].page_content[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28010b55-f12a-4409-bdab-7b5a3e6f01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8de5982-7c6e-412f-87e3-fc9ae99a270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={'device': 'cuda'}, encode_kwargs={}, multi_process=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3590c93-1c9c-4b7c-a8fd-62f055727e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(splits, embeddings)\n",
    "db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d73516-a0a4-419f-9b4e-5740a9549c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecbb7006-2d73-4277-9afd-1fdea1b85820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl localhost:5001/generate -X POST -H 'Content-Type: application/json' -d '{\"inputs\": \"What is good about Beijing?\", \"parameters\": { \"max_new_tokens\":64}}' #Replace the locahost with the IP visible to the machine running the notebook     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19f57d-e869-4e77-9f01-f6eedb9b9c36",
   "metadata": {},
   "source": [
    "## Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340e4d98-fe06-4c98-9b92-0b1d2c34dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from queue import Queue\n",
    "from typing import Any\n",
    "from langchain.llms.huggingface_text_gen_inference import HuggingFaceTextGenInference\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from anyio.from_thread import start_blocking_portal #For model callback streaming\n",
    "import re\n",
    "import gradio as gr\n",
    "langchain.debug=True \n",
    "\n",
    "#vector db path\n",
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "\n",
    "# #Llama2 TGI models host port\n",
    "# LLAMA2_7B_HOSTPORT = \"http://localhost:8080/\" #Replace the locahost with the IP visible to the machine running the notebook\n",
    "# LLAMA2_13B_HOSTPORT = \"http://localhost:8080/\" #Add your own host ports for model switching. You can host another TGI model on same instance on a different port.\n",
    "\n",
    "\n",
    "# model_dict = {\n",
    "#     # \"7b-chat\" : LLAMA2_7B_HOSTPORT,\n",
    "#     # \"13b-chat\" : LLAMA2_13B_HOSTPORT,\n",
    "#     \"7b-chat\" : 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# }\n",
    "\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff28be1d-2d46-4742-8cf2-c8cd02f39fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': 'cuda'})\n",
    "db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5aa8abd-c0db-4e62-bc42-0feab779cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-7\"b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9748225-b528-4c07-be3d-734e7de7aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta-llama/Llama-2-7b-chat-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20939c6b-c8e8-4d23-9927-87dfd80a834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-05 17:01:39 llm_engine.py:73] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-chat-hf', tokenizer='meta-llama/Llama-2-7b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 02-05 17:01:45 llm_engine.py:223] # GPU blocks: 815, # CPU blocks: 512\n",
      "INFO 02-05 17:01:48 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-05 17:03:10 model_runner.py:437] Graph capturing finished in 82 secs.\n"
     ]
    }
   ],
   "source": [
    "# llm = HuggingFaceTextGenInference(\n",
    "#     inference_server_url=y,\n",
    "#     max_new_tokens=512,\n",
    "#     top_k=10,\n",
    "#     top_p=0.9,\n",
    "#     typical_p=0.95,\n",
    "#     temperature=0.6,\n",
    "#     repetition_penalty=1,\n",
    "#     do_sample=True,\n",
    "#     streaming=True\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "from langchain.llms import VLLM\n",
    "\n",
    "llm = VLLM(model=model,\n",
    "           trust_remote_code=True,  # mandatory for hf models\n",
    "           max_new_tokens=128,\n",
    "           top_k=10,\n",
    "           top_p=0.95,\n",
    "           temperature=0.6,\n",
    "           # tensor_parallel_size=... # for distributed inference\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67004494-9d3f-418c-86bd-3f2e17e0d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import VLLM\n",
    "\n",
    "\n",
    "\n",
    "# llm = VLLM(model='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "#            trust_remote_code=True,  # mandatory for hf models\n",
    "#            max_new_tokens=128,\"\"\n",
    "#            top_k=10,\n",
    "#            top_p=0.95,\n",
    "#            temperature=0.6,\n",
    "#            # tensor_parallel_size=... # for distributed inference\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688362de-23b3-4680-b682-91d9169fe7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(llm(\"What is the capital of France ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c9cedf-8ab3-4107-97fc-80e1870d7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST]Use the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\n",
    "{context}\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "        search_kwargs={\"k\": 10}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c8cc2ec-f801-47e5-8a7b-69d6d562abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,     \n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e3da6b-52e4-4928-b804-85e9cbf6ccda",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain\u001b[49m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgive me all those User IDs which have worked in investment banking\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qa_chain' is not defined"
     ]
    }
   ],
   "source": [
    "# result = qa_chain({\"query\": \"give me all those User IDs minimum 6 who have worked as a data scientist team manager\"})\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbc86e81-bfa9-4e16-b8e4-b43ce6ff9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f783adc2-9e80-401d-997e-bb4e3bda30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3cf50bb-2dc9-4ec3-ba1e-e516b6b6443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# user_id_pattern = r'User ID: (\\d+)'\n",
    "\n",
    "# user_ids = re.findall(user_id_pattern, result['result'])\n",
    "\n",
    "# print(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d124b3-60ba-4a4b-9546-ced2019c641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uid = []\n",
    "# for idx in user_ids:\n",
    "#     u = int(idx)\n",
    "#     uid.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daefd826-4603-4784-8a96-a8f65837ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5d97c2b-a192-4cd7-8036-ea4bb9e1e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from user_info import get_user_data_search_embed\n",
    "# df = get_user_data_search_embed(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d1a7b07-6cbf-4dd4-812a-b9b5efa2debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fcef2-ba96-41bf-b045-ddc6f44a602b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59af5adc-a96f-4d7e-9a9c-4395de30d538",
   "metadata": {},
   "source": [
    "# Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "137779c8-6272-4e69-b3ff-5ff65cb880ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_info import get_user_data_search_embed\n",
    "import pandas as pd\n",
    "\n",
    "def make_clickable(link):\n",
    "    return f'<a href=\"{link}\" target=\"_blank\">{link}</a>'\n",
    "\n",
    "\n",
    "def query(question):\n",
    "\n",
    "    result = qa_chain({\"query\": question})\n",
    "\n",
    "    user_id_pattern = r'User ID: (\\d+)'\n",
    "\n",
    "    user_ids = re.findall(user_id_pattern, result['result'])\n",
    "\n",
    "    uid = []\n",
    "    for idx in user_ids:\n",
    "        u = int(idx)\n",
    "        uid.append(u)\n",
    "        \n",
    "    df = get_user_data_search_embed(uid)\n",
    "    \n",
    "    final_usr=pd.DataFrame()\n",
    "    \n",
    "    # for x in df['id']:\n",
    "    #     print(x)\n",
    "    #     hy =  \"https://search.iimjobs.com/profile/userid\"\n",
    "    #     mini = df.loc[df['id']==x]\n",
    "    #     mini['user_profile'] = [hy.replace(\"userid\", str(x))]\n",
    "    #     final_usr = pd.concat([final_usr, mini], ignore_index=True)\n",
    "    \n",
    "    # final_usr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    for x in df['id']:\n",
    "        print(x)\n",
    "        hy = \"https://search.iimjobs.com/profile/userid\"\n",
    "        mini = df.loc[df['id'] == x].copy()\n",
    "        mini['user_profile'] = [hy.replace(\"userid\", str(x))]\n",
    "        final_usr = pd.concat([final_usr, mini], ignore_index=True)\n",
    "    \n",
    "    final_usr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_usr['user_profile'] = final_usr['user_profile'].apply(lambda x: make_clickable(x))\n",
    "    \n",
    "\n",
    "    final_usr = final_usr[['id','current_designation','user_experience','user_profile']]\n",
    "\n",
    "\n",
    "    df_str = final_usr.to_html(escape=False)\n",
    "\n",
    "    return df_str\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90169e16-e14c-43a0-9810-d7fcca8282f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = query('give me all those User IDs minimum 6 who have worked as a data scientist team manager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "916d7f50-bd88-4008-9792-2d309d55aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540809d1-3adb-4dc2-bf08-23f2e23c99e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4cf1889-7027-4d45-8109-c83b94d2395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd3f9bc-6af4-4fca-862e-991e900665ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo = gr.Interface(\n",
    "#     fn=query,\n",
    "#     inputs=[gr.Textbox(label=\"Query\")],\n",
    "#     outputs=[gr.DataFrame(label=\"Details\")],\n",
    "# )\n",
    "# demo.launch('0.0.0.0',share=True)\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=query,\n",
    "    inputs=[gr.Textbox(label=\"Query\")],\n",
    "    outputs=[gr.HTML(label=\"Details\")]\n",
    ")\n",
    "\n",
    "iface.launch('0.0.0.0', share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68a8bb-74b9-4919-ba47-817e689e21cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a44ba8-82bc-4b96-888e-bfcb30ec6cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6344396-37e5-4aa1-a4e5-84495141fa72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f1a05-3349-4f9c-abb5-c67862c24043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3104a1f6-8b97-440d-8f3d-8e946442da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[['resume']].values[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b0c48-16b3-4563-bd20-03c0d27e6b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a780a4-7d90-4d98-a927-f22ff03e963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## questions\n",
    "# give that User ID which has Assistant Sales Manager Mid-Level and also worked in client relationship managerment\n",
    "# give me all User ID which have data analyst profile\n",
    "# give me all User ID which has Assistant Sales Manager Mid-Level and also worked in client relationship managerment\n",
    "# give me all that User IDs which have worked in client relationship managerment\n",
    "# give me all that User IDs which have worked as business analyst\n",
    "# give me all those User IDs who have managed machine learning team\n",
    "# give me all those User IDs who have worked as a management consultant with FMCG brands\n",
    "# give me all those User IDs who have worked as a management consultant with FMCG brands\n",
    "# give me all those User IDs who have worked as a data scientist team manager\n",
    "# give me all those User IDs who have managed a data science team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f22bb1-670f-4cec-ace6-436893c7e9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c6f8f-f74c-4393-a7c0-cc5eb9f4a626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69223ce0-2d1d-4ec3-bc1e-aa05b9456d81",
   "metadata": {},
   "source": [
    "## Direct RAG example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67534d9b-8f23-4cf7-a3a8-e15d7f789c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=config['key']['infoedge']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0054de75-09fb-4a94-966d-74f1b2c698d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffa3d3-15f1-4b8b-90c0-651a0ca5681f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5892b3-3840-4ef8-b31c-4618845eb09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The career objective of a relationship manager is to pursue an ambitious career in an organization where there are ample learning opportunities, upward mobility, exposure to innovation, and motivation. They aim to exhibit their resourcefulness in both technical and organizational skills and to take on challenging roles in key account management, travel and expense management solutions, and relationship management.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "loader = TextLoader(\"/home/ubuntu/infoedge/llama-recipes/examples/custom_data/user_resume_data/uid_1103243.txt\")\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "index.query(\"What are relationship manager career objectives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2bea6d-dcd4-4f09-a937-fb113ba6e51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efc9f6-1437-40d4-8cea-dbedb6748965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd7772-31b4-4006-9f21-933b065df2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c5bfb-5f4e-4fc1-b3ea-80f56edd2f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
