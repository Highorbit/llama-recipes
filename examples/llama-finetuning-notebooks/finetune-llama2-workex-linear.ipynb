{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "This notebook shows how to train a Llama 2 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA.\n",
    "\n",
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "We use the Hugging Face trainer and model which means that the checkpoint has to be converted from its original format into the dedicated Hugging Face format.\n",
    "The conversion can be achieved by running the `convert_llama_weights_to_hf.py` script provided with the transformer package.\n",
    "Given that the original checkpoint resides under `models/7B` we can install all requirements and convert the checkpoint with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets tqdm vllm\n",
    "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weighjts_to_hf.py')\"`\n",
    "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llama-recipes/src/llama_recipes/utils/dataset_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point model_id to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train_data = load_from_disk(\"../custom_data/linear_work_data.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Important \n",
    "\n",
    "It is important to consider here which model we're using to parse the resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1e66ae7b3641199d0a5f1b6060d5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16, token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/SageMaker/llama_root/src')\n",
    "sys.path.append('../llama-recipes/src/llama_recipes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt = f'''\n",
    "You are an accurate agent working for a job platform. You will be given the raw \n",
    "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
    "user from the resume. The response should be broken into a numbered list with each item of the list \n",
    "containing the complete and accurate information about the work experience of the users.\n",
    "1. Designation 1 @ Company 1 [From \"mm/yyy\" to \"mm/yyyy\"] : \"complete job description as given in resume\"\\n\n",
    "2. Designation 2 @ Company 2 [From \"mm/yyy\" to \"mm/yyyy\"] :  \"complete job description as given in resume\"\\n\n",
    "Please follow this structure closely and keep the response within the token limit.\" \n",
    "\n",
    "This is the resume text:\\n{{resume_text}}\\n\n",
    "This is the output in the required_format:\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_resume_text = '''\n",
    " S\\n EVANAND\\n\\n\\nEmail: sevaanand863@gmail.com\\nMobile: +919110416415\\n\\n\\nPROFESSIONAL SUMMARY:\\n Having 2+ years of technical experience in Analysis, Design, Development, Testing\\n and Implementation of Client Server Application and Data warehousing ETL (Extract,\\n Transform and Load) in Informatica Power Center 10.4 and INFORMATICA intelligent\\n cloud services.\\n Main areas of expertise are Developing and Testing the data warehousing\\n projects with data quality standards.\\n Extensive experience in Extraction, Transformation and Loading of data\\n directly from heterogeneous source systems like fat fles, Oracle by using\\n Informatica power center.\\n Tuned several mappings for the better performance and involved in Performance\\n Testing.\\n Implemented exceptional handling mechanism by using Exception transformation &amp;\\n Human Task.\\n Creating Informatica IICS mappings for the diferent plans using various\\n transformations.\\n Have working experience in Informatica Intelligent Cloud Services IICS components -\\n application integration, data integration, Informatica data quality and Informatica\\n power center and CRM application - Salesforce.\\n Worked on SCD Type1,SCD Type2 in IICS\\n Worked on Mapping, Mapping Task, Mapplet, Task Flows\\n Experience on all important General transformations.\\n Used informatica developer tool to develop the mapping with power center\\n transformations.\\n Customized SQL override queries where ever possible to minimize the use of Joiner,\\n Aggregator and Lookup Transformations.\\n Developed all the mappings according to the design document and mapping specs\\n provided and performed unit testing.\\n Used Parameterization for Mapping, Workfows and sessions.\\n Worked on running &amp; scheduling the Informatica jobs using Shell Scripts written on\\n the UNIX box.\\n Error handling &amp; issue analysis during the testing and maintenance.\\n Hands on dynamic parameter fle creation.\\n Identifying the bottlenecks and implement the Performance tuning &amp;\\n Optimization techniques in power center.\\n Review and initial approval for various Docs like IDS, IRS, PDI, KEDB, Mapping\\n sheets.\\n Good Knowledge on Data Warehousing concepts like Star Schema, Dimensions\\n and Fact tables.\\n Optimizing Informatica Mappings and Sessions to improve the performance.\\n Experience of handling slowly changing dimensions to maintain complete\\n history using Type I, Type II and Type III strategies.\\n Created UNIX Shell scripts to run the Informatica Workfows &amp; controlling the ETL\\n fow.\\n Hands on Admin activities.\\n Excellent problem-solving skills with strong technical background and good\\n interpersonal skills.\\n\\n\\n\\n\\nEXPERIENCE SUMMARY:\\n, Worked as a Programmer Analyst with COGNIZANT from Jan 2022 to April 2023.\\n\\n Worked as a Software Engineer with Birla Soft LTD from Jan 2021 to Jan 2022.\\n\\n\\n\\n\\n TECHNICAL ENVIRONMENT:\\nOperating System : Windows, Linux\\nTools : Informatica developer, IICS, PUTTY, SQL Developer and WinSCP\\nRDBMS : Oracle ,SQL, PostgreSQL\\nLanguages : Unix,\\nScheduling Tools : Autosys, Control-M\\n\\n\\n\\n PROJECT PROFILE:\\n\\n\\n #PROJECT 1\\n\\n Client : Verizon\\n Project Name : HR Union Recruit in\\n Domain : Telecom\\n Role : IICS Developer\\n Environment : IICS, Oracle 11g, PostgreSQL , Windows 10\\n\\nProject Description:\\n The Project HR Union involves the migration of severance&rsquo;s data in PeopleSoft to\\nPostgreSQL.\\n\\nInformatica Cloud&rsquo;s Data Integration Services consume the Data from Peoplesoft system\\nand perform the\\n\\nbusiness logic to load in Severance&rsquo;s database (PostgreSQL) and then provide the data to\\ndownstream\\n\\nvendors in the form of Files.\\n Responsibilities:\\n\\n Creating Informatica IICS mappings for the diferent plans using various\\n transformations.\\n Have working experience in Informatica Intelligent Cloud Services IICS\\n components - application integration, data integration, Informatica data quality\\n and Informatica power center and CRM application - Salesforce.\\n Analysis of the specifcations provided by the clients.\\n Used Various Transformations such as Sorted, Lookup, Joiner, Aggregator,\\n Sequence Generator. Lookup, Normalizer, Transaction Control Transformation.\\n Worked on Diferent tasks like Mapping Task Replication Task, Synchronization\\n Task, Power Center Task in IICS.\\n Designed, Developed and implemented ETL Processes using IICS Data\\n Integration\\n Created IICS connection using various cloud connectors in IICS Administrator\\n Extensively used informatica IICS&ndash; Mapping, Mapping Task, Task Flow.\\n, Developed complex mappings using transformations such as the Source\\n qualifer, Joiner, Aggregator, Update Strategy, Expression, Connected Lookup,\\n Unconnected Lookup and Router transformations.\\n Created informatica mappings for stage, Dimensions and Fact table loads.\\n Created SCD type-1 and type-2 mappings for loading the dimension tables.\\n Done extensive testing and wrote queries in SQL to ensure the loading of the\\n data.\\n Developed and implemented the coding of Informatica Mapping for the\\n diferent stages of ETL.\\n Involved in Unit testing\\n On-time Production migration without defects\\n Involved in Post production Support.\\n\\n\\n\\n\\n#Project 2\\n\\n Client : Discover Fin bank\\n Domain : Banking\\n Environment : Informatica power center 9.X, Oracle10g\\n Role : Informatica Support and Developer\\n\\n\\n\\nDISCRIPTIOIN:\\n\\n This application was designed to load member and subscriber eligibility information\\nas received from the customers in the form of fat fles and oracle database. The system\\nwas designed to store the eligibility information of the members belonging to the various\\ncontracts for the various vendor customer services being provided to them by the client.\\nIt was used to store the historical information pertaining to each and every member who\\nwas entitled to receive the customer services. The various other front-end applications\\nwould access this database to determine the authenticity of the members and the type of\\nservices they were entitled to the system.\\nResponsibilities:\\n\\n Understanding existing business model and customer requirements.\\n Understanding the mapping specifcations and requirements.\\n Managing priorities of tasks, scheduling and tracking progress.\\n Extraction of data from various sources using Informatica.\\n Designed various mappings for extracting data from various sources involving fat\\n fles and relational tables.\\n Used Source Analyzer and Warehouse Designer to import the source and target\\n database schemas and the mapping designer to map source to the target.\\n Used Transformation Developer to create the flters, joiner, update strategy, lookups\\n and\\n Aggregation transformations, which are used in mappings.\\n Created various tasks like sessions, worklets, and workfows in the workfow\\n manager to test the mapping during development.\\n To keep track of historical data slowly changing dimensions are implemented.\\n Created and Monitored Batches and Sessions using Informatica Power Centre.\\n Created and executed sessions and batches using Server Manager.\\n Worked with Mapping Variables and Mapping Parameters.\\n Developed all the mappings according to the design document and mapping specs\\n provided and performed unit testing.\\n, Created test plan, Test Design, Test scripts and responsible for implementation of\\n Test cases as Manual test scripts.\\n Developed mapping to load the data in slowly changing dimension.\\n Checked the output according to the specifcations.\\n Confgured and ran the Debugger from within the Mapping Designer to troubleshoot\\n the mapping before the normal run of the workfow.\\n Tuned several mappings for the better performance and involved in Performance\\n Testing.\\n Documenting test cases and Informatica mappings\\n Prepared documentation for business data fow from source to target and also for\\n the changes made to the mappings/sessions existing to eliminate the errors.\\n Provide weekly status report to the Project Manager and discuss issues related to\\n quality and deadlines.'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ep = eval_prompt.format(resume_text=example_resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 14:52:20.288245: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-03 14:52:20.339346: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-03 14:52:20.993326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an accurate agent working for a job platform. You will be given the raw \n",
      "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
      "user from the resume. The response should be broken into a numbered list with each item of the list \n",
      "containing the complete and accurate information about the work experience of the users.\n",
      "1. Designation 1 @ Company 1 [From \"mm/yyy\" to \"mm/yyyy\"] : \"complete job description as given in resume\"\n",
      "\n",
      "2. Designation 2 @ Company 2 [From \"mm/yyy\" to \"mm/yyyy\"] :  \"complete job description as given in resume\"\n",
      "\n",
      "Please follow this structure closely and keep the response within the token limit.\" \n",
      "\n",
      "This is the resume text:\n",
      "\n",
      " S\n",
      " EVANAND\n",
      "\n",
      "\n",
      "Email: sevaanand863@gmail.com\n",
      "Mobile: +919110416415\n",
      "\n",
      "\n",
      "PROFESSIONAL SUMMARY:\n",
      " Having 2+ years of technical experience in Analysis, Design, Development, Testing\n",
      " and Implementation of Client Server Application and Data warehousing ETL (Extract,\n",
      " Transform and Load) in Informatica Power Center 10.4 and INFORMATICA intelligent\n",
      " cloud services.\n",
      " Main areas of expertise are Developing and Testing the data warehousing\n",
      " projects with data quality standards.\n",
      " Extensive experience in Extraction, Transformation and Loading of data\n",
      " directly from heterogeneous source systems like fat fles, Oracle by using\n",
      " Informatica power center.\n",
      " Tuned several mappings for the better performance and involved in Performance\n",
      " Testing.\n",
      " Implemented exceptional handling mechanism by using Exception transformation &amp;\n",
      " Human Task.\n",
      " Creating Informatica IICS mappings for the diferent plans using various\n",
      " transformations.\n",
      " Have working experience in Informatica Intelligent Cloud Services IICS components -\n",
      " application integration, data integration, Informatica data quality and Informatica\n",
      " power center and CRM application - Salesforce.\n",
      " Worked on SCD Type1,SCD Type2 in IICS\n",
      " Worked on Mapping, Mapping Task, Mapplet, Task Flows\n",
      " Experience on all important General transformations.\n",
      " Used informatica developer tool to develop the mapping with power center\n",
      " transformations.\n",
      " Customized SQL override queries where ever possible to minimize the use of Joiner,\n",
      " Aggregator and Lookup Transformations.\n",
      " Developed all the mappings according to the design document and mapping specs\n",
      " provided and performed unit testing.\n",
      " Used Parameterization for Mapping, Workfows and sessions.\n",
      " Worked on running &amp; scheduling the Informatica jobs using Shell Scripts written on\n",
      " the UNIX box.\n",
      " Error handling &amp; issue analysis during the testing and maintenance.\n",
      " Hands on dynamic parameter fle creation.\n",
      " Identifying the bottlenecks and implement the Performance tuning &amp;\n",
      " Optimization techniques in power center.\n",
      " Review and initial approval for various Docs like IDS, IRS, PDI, KEDB, Mapping\n",
      " sheets.\n",
      " Good Knowledge on Data Warehousing concepts like Star Schema, Dimensions\n",
      " and Fact tables.\n",
      " Optimizing Informatica Mappings and Sessions to improve the performance.\n",
      " Experience of handling slowly changing dimensions to maintain complete\n",
      " history using Type I, Type II and Type III strategies.\n",
      " Created UNIX Shell scripts to run the Informatica Workfows &amp; controlling the ETL\n",
      " fow.\n",
      " Hands on Admin activities.\n",
      " Excellent problem-solving skills with strong technical background and good\n",
      " interpersonal skills.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "EXPERIENCE SUMMARY:\n",
      ", Worked as a Programmer Analyst with COGNIZANT from Jan 2022 to April 2023.\n",
      "\n",
      " Worked as a Software Engineer with Birla Soft LTD from Jan 2021 to Jan 2022.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " TECHNICAL ENVIRONMENT:\n",
      "Operating System : Windows, Linux\n",
      "Tools : Informatica developer, IICS, PUTTY, SQL Developer and WinSCP\n",
      "RDBMS : Oracle ,SQL, PostgreSQL\n",
      "Languages : Unix,\n",
      "Scheduling Tools : Autosys, Control-M\n",
      "\n",
      "\n",
      "\n",
      " PROJECT PROFILE:\n",
      "\n",
      "\n",
      " #PROJECT 1\n",
      "\n",
      " Client : Verizon\n",
      " Project Name : HR Union Recruit in\n",
      " Domain : Telecom\n",
      " Role : IICS Developer\n",
      " Environment : IICS, Oracle 11g, PostgreSQL , Windows 10\n",
      "\n",
      "Project Description:\n",
      " The Project HR Union involves the migration of severance&rsquo;s data in PeopleSoft to\n",
      "PostgreSQL.\n",
      "\n",
      "Informatica Cloud&rsquo;s Data Integration Services consume the Data from Peoplesoft system\n",
      "and perform the\n",
      "\n",
      "business logic to load in Severance&rsquo;s database (PostgreSQL) and then provide the data to\n",
      "downstream\n",
      "\n",
      "vendors in the form of Files.\n",
      " Responsibilities:\n",
      "\n",
      " Creating Informatica IICS mappings for the diferent plans using various\n",
      " transformations.\n",
      " Have working experience in Informatica Intelligent Cloud Services IICS\n",
      " components - application integration, data integration, Informatica data quality\n",
      " and Informatica power center and CRM application - Salesforce.\n",
      " Analysis of the specifcations provided by the clients.\n",
      " Used Various Transformations such as Sorted, Lookup, Joiner, Aggregator,\n",
      " Sequence Generator. Lookup, Normalizer, Transaction Control Transformation.\n",
      " Worked on Diferent tasks like Mapping Task Replication Task, Synchronization\n",
      " Task, Power Center Task in IICS.\n",
      " Designed, Developed and implemented ETL Processes using IICS Data\n",
      " Integration\n",
      " Created IICS connection using various cloud connectors in IICS Administrator\n",
      " Extensively used informatica IICS&ndash; Mapping, Mapping Task, Task Flow.\n",
      ", Developed complex mappings using transformations such as the Source\n",
      " qualifer, Joiner, Aggregator, Update Strategy, Expression, Connected Lookup,\n",
      " Unconnected Lookup and Router transformations.\n",
      " Created informatica mappings for stage, Dimensions and Fact table loads.\n",
      " Created SCD type-1 and type-2 mappings for loading the dimension tables.\n",
      " Done extensive testing and wrote queries in SQL to ensure the loading of the\n",
      " data.\n",
      " Developed and implemented the coding of Informatica Mapping for the\n",
      " diferent stages of ETL.\n",
      " Involved in Unit testing\n",
      " On-time Production migration without defects\n",
      " Involved in Post production Support.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Project 2\n",
      "\n",
      " Client : Discover Fin bank\n",
      " Domain : Banking\n",
      " Environment : Informatica power center 9.X, Oracle10g\n",
      " Role : Informatica Support and Developer\n",
      "\n",
      "\n",
      "\n",
      "DISCRIPTIOIN:\n",
      "\n",
      " This application was designed to load member and subscriber eligibility information\n",
      "as received from the customers in the form of fat fles and oracle database. The system\n",
      "was designed to store the eligibility information of the members belonging to the various\n",
      "contracts for the various vendor customer services being provided to them by the client.\n",
      "It was used to store the historical information pertaining to each and every member who\n",
      "was entitled to receive the customer services. The various other front-end applications\n",
      "would access this database to determine the authenticity of the members and the type of\n",
      "services they were entitled to the system.\n",
      "Responsibilities:\n",
      "\n",
      " Understanding existing business model and customer requirements.\n",
      " Understanding the mapping specifcations and requirements.\n",
      " Managing priorities of tasks, scheduling and tracking progress.\n",
      " Extraction of data from various sources using Informatica.\n",
      " Designed various mappings for extracting data from various sources involving fat\n",
      " fles and relational tables.\n",
      " Used Source Analyzer and Warehouse Designer to import the source and target\n",
      " database schemas and the mapping designer to map source to the target.\n",
      " Used Transformation Developer to create the flters, joiner, update strategy, lookups\n",
      " and\n",
      " Aggregation transformations, which are used in mappings.\n",
      " Created various tasks like sessions, worklets, and workfows in the workfow\n",
      " manager to test the mapping during development.\n",
      " To keep track of historical data slowly changing dimensions are implemented.\n",
      " Created and Monitored Batches and Sessions using Informatica Power Centre.\n",
      " Created and executed sessions and batches using Server Manager.\n",
      " Worked with Mapping Variables and Mapping Parameters.\n",
      " Developed all the mappings according to the design document and mapping specs\n",
      " provided and performed unit testing.\n",
      ", Created test plan, Test Design, Test scripts and responsible for implementation of\n",
      " Test cases as Manual test scripts.\n",
      " Developed mapping to load the data in slowly changing dimension.\n",
      " Checked the output according to the specifcations.\n",
      " Confgured and ran the Debugger from within the Mapping Designer to troubleshoot\n",
      " the mapping before the normal run of the workfow.\n",
      " Tuned several mappings for the better performance and involved in Performance\n",
      " Testing.\n",
      " Documenting test cases and Informatica mappings\n",
      " Prepared documentation for business data fow from source to target and also for\n",
      " the changes made to the mappings/sessions existing to eliminate the errors.\n",
      " Provide weekly status report to the Project Manager and discuss issues related to\n",
      " quality and deadlines.'\n",
      "\n",
      "\n",
      "This is the output in the required_format:\n",
      "\n",
      "1. Designation: Programmer Analyst\n",
      "Company: Cognizant\n",
      "From: Jan 2022 to April 2023\n",
      "\n",
      "2. Designation: IICS Developer\n",
      "Client: Verizon\n",
      "Project Name: HR Union Recruit in\n",
      "Domain: Telecom\n",
      "Role: IICS Developer\n",
      "Environment: IICS, Oracle 11g, PostgreSQL, Windows 10\n",
      "\n",
      "3. Designation: Informatica Support and Developer\n",
      "Client: Discover Fin bank\n",
      "Domain: Banking\n",
      "Environment: Informatica Power center 9.X, Oracle10g\n",
      "\n",
      "Please note that the above output is in the required format, but the actual response should be tailored to the specific job position and requirements.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# streamer = TextStreamer(tokenizer)\n",
    "model_input = tokenizer(ep,return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "    \n",
    "    # peft_config = LoraConfig(\n",
    "    #     task_type=TaskType.CAUSAL_LM,\n",
    "    #     inference_mode=False,\n",
    "    #     r=8,\n",
    "    #     lora_alpha=32,\n",
    "    #     lora_dropout=0.05,\n",
    "    #     target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    # )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 5: Define an optional profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/linear_workex\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 3,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  3 12:13:36 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   31C    P0              60W / 300W |  12966MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     19821      C   ...oedge/llama-recipes/env/bin/python3    12958MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 66/177 15:22 < 26:40, 0.07 it/s, Epoch 1.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.819500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 7:\n",
    "Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 8:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('../custom_data/model_eval_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import html, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ADARSH JADHAV\n",
      " +91 8668242753 Pune, India\n",
      " adarshjadhav33@gmail.com Linkedin GitHub Portfolio\n",
      "\n",
      "OBJECTIVE\n",
      "As a goal-driven Software Engineering graduate, I am dedicated to leveraging my proficiency in Java, Python and\n",
      "my strong foundation in software development to contribute effectively as a Software Developer.\n",
      "\n",
      "EDUCATION\n",
      "Bachelor of Engineering in Information Technology ,\n",
      "D Y Patil College Of Engineering Akurdi Aug 2019- July 2023\n",
      "CGPA - 9.09\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Programming Languages: Python (Programming Language) ,Java\n",
      "Database Management Systems : MySQL, MongoDB\n",
      "Web Development: HTML, CSS, Javascript, Django,,React, REST APIs,Bootstrap\n",
      "Version Control: Git\n",
      "Familarity with: Docker,Kubernets.AWS\n",
      "Operating Systems : Linux\n",
      "Software Development Methodology: OOPS, SDLC,Testing\n",
      "Soft skills : Strong communication and collaboration skills ,Rapid learner\n",
      "\n",
      "EXPERIENCE\n",
      "Software Development Intern Aug 2022 - Oct 2022\n",
      "Elite Softwares Pune\n",
      " • Collaborated within an Agile team to build a robust web application using Django framework.\n",
      " • Demonstrated versatility in both frontend and backend development, contributing to the full-stack lifecycle.\n",
      " • Assisted in troubleshooting, optimizing code, and enhancing overall application performance.\n",
      "\n",
      "PROJECTS\n",
      "Automated Skin Disease Prediction Using Deep Learning.\n",
      "Tech Stack: HTML, CSS, MySQL, Django,Bootstrap, Python, Javascript, Deep Learning,Pandas, Numpy, Tensorflow\n",
      " • Engineered a robust deep learning solution for accurate skin disease prediction from images.\n",
      " • Achieved model accuracy of 97.05 with 91.17 validation accuracy.\n",
      " • Implemented user authentication, disease info dissemination, chat support, and doctor recommendations.\n",
      "Caloriefy - Calorie Tracking Web App\n",
      "Tech Stack:HTML, CSS,BootStrap, MySQL,Django, Python, Javascript\n",
      " • Developed a Django-based web app for monitoring and managing daily calorie intake..\n",
      " • Empowers users to effortlessly track dietary habits, promoting healthier lifestyles.\n",
      "Task management application\n",
      "Tech Stack:HTML,React, CSS,BootStrap, MySQL,Django, Python, Javascript\n",
      " • Developed a task management application using Django and React, combining robust backend functionality\n",
      " with an intuitive front-end interface.\n",
      " • Utilized Django’s REST API for seamless data communication, delivering an efficient and user-friendly task\n",
      " management solution.\n"
     ]
    }
   ],
   "source": [
    "rt = eval_df.sample(random_state=random.randint(0,10000))['resume'].values[0]\n",
    "rt = html.unescape(rt)\n",
    "\n",
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt = f'''\n",
    "You are an accurate agent working for a job platform. You will be given the raw \n",
    "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
    "user from the resume. The response should be presented into a numbered list with each item of the list \n",
    "being an unbroken line of text containing the complete and accurate information about the work experience of the users. \n",
    "Here is an example structure:\\n\n",
    "1. Designation 1 @ Company 1 [From \"mm/yyy\" to \"mm/yyyy\"] : \"complete job description as given in resume\"\\n\n",
    "2. Designation 2 @ Company 2 [From \"mm/yyy\" to \"mm/yyyy\"] :  \"complete job description as given in resume\"\\n\n",
    "Please follow this structure accurately and keep the response within the token limit.\" \n",
    "\n",
    "This is the resume text:\\n{{resume_text}}\\n\n",
    "This is the output in the required_format:\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ep = eval_prompt.format(resume_text=rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an accurate agent working for a job platform. You will be given the raw \n",
      "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
      "user from the resume. The response should be presented into a numbered list with each item of the list \n",
      "being an unbroken line of text containing the complete and accurate information about the work experience of the users. \n",
      "Here is an example structure:\n",
      "\n",
      "1. Designation 1 @ Company 1 [From \"mm/yyy\" to \"mm/yyyy\"] : \"complete job description as given in resume\"\n",
      "\n",
      "2. Designation 2 @ Company 2 [From \"mm/yyy\" to \"mm/yyyy\"] :  \"complete job description as given in resume\"\n",
      "\n",
      "Please follow this structure accurately and keep the response within the token limit.\" \n",
      "\n",
      "This is the resume text:\n",
      " ADARSH JADHAV\n",
      " +91 8668242753 Pune, India\n",
      " adarshjadhav33@gmail.com Linkedin GitHub Portfolio\n",
      "\n",
      "OBJECTIVE\n",
      "As a goal-driven Software Engineering graduate, I am dedicated to leveraging my proficiency in Java, Python and\n",
      "my strong foundation in software development to contribute effectively as a Software Developer.\n",
      "\n",
      "EDUCATION\n",
      "Bachelor of Engineering in Information Technology ,\n",
      "D Y Patil College Of Engineering Akurdi Aug 2019- July 2023\n",
      "CGPA - 9.09\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Programming Languages: Python (Programming Language) ,Java\n",
      "Database Management Systems : MySQL, MongoDB\n",
      "Web Development: HTML, CSS, Javascript, Django,,React, REST APIs,Bootstrap\n",
      "Version Control: Git\n",
      "Familarity with: Docker,Kubernets.AWS\n",
      "Operating Systems : Linux\n",
      "Software Development Methodology: OOPS, SDLC,Testing\n",
      "Soft skills : Strong communication and collaboration skills ,Rapid learner\n",
      "\n",
      "EXPERIENCE\n",
      "Software Development Intern Aug 2022 - Oct 2022\n",
      "Elite Softwares Pune\n",
      " • Collaborated within an Agile team to build a robust web application using Django framework.\n",
      " • Demonstrated versatility in both frontend and backend development, contributing to the full-stack lifecycle.\n",
      " • Assisted in troubleshooting, optimizing code, and enhancing overall application performance.\n",
      "\n",
      "PROJECTS\n",
      "Automated Skin Disease Prediction Using Deep Learning.\n",
      "Tech Stack: HTML, CSS, MySQL, Django,Bootstrap, Python, Javascript, Deep Learning,Pandas, Numpy, Tensorflow\n",
      " • Engineered a robust deep learning solution for accurate skin disease prediction from images.\n",
      " • Achieved model accuracy of 97.05 with 91.17 validation accuracy.\n",
      " • Implemented user authentication, disease info dissemination, chat support, and doctor recommendations.\n",
      "Caloriefy - Calorie Tracking Web App\n",
      "Tech Stack:HTML, CSS,BootStrap, MySQL,Django, Python, Javascript\n",
      " • Developed a Django-based web app for monitoring and managing daily calorie intake..\n",
      " • Empowers users to effortlessly track dietary habits, promoting healthier lifestyles.\n",
      "Task management application\n",
      "Tech Stack:HTML,React, CSS,BootStrap, MySQL,Django, Python, Javascript\n",
      " • Developed a task management application using Django and React, combining robust backend functionality\n",
      " with an intuitive front-end interface.\n",
      " • Utilized Django’s REST API for seamless data communication, delivering an efficient and user-friendly task\n",
      " management solution.\n",
      "\n",
      "This is the output in the required_format:\n",
      "[{'company': 'Elite Softwares Pune', 'role': 'Software Development Intern', 'start_date': '08/2022', 'end_date': '10/2022'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer)\n",
    "model_input = tokenizer(ep,return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_prompt = f'''\n",
    "# You are a helpful language model working for a job platform. You will be given the raw \n",
    "#  unstructured text of a user's resume, and the task is to extract the work experience of the \n",
    "#  user from the raw text in the following format: \\n{{work_format}}\\n\n",
    "\n",
    "#  This is the resume text:\\n{{resume_text}}\\n\n",
    "#  This is the output in the required format:\\n\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# work_format = '''{\n",
    "#     'work_experience': [{'company': 'company Name 1',\n",
    "#                          'role': 'job designation 1',\n",
    "#                          'start_date': 'mm/yyyy',\n",
    "#                          'end_date': 'mm/yyyy',\n",
    "#                          'description': 'complete Job description taken from resume'},\n",
    "#                         {'company': 'company name 2',\n",
    "#                          'role': 'job designation 2',\n",
    "#                          'start_date': mm/yyyy',\n",
    "#                          'end_date': 'mm/yyyy',\n",
    "#                          'description': 'complete Job description taken from resume'}]\n",
    "# }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060ff99b9cd8410aaeca05b62bd1ab17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lakshay/linear-work-peft/commit/dd2610e6d1fdc7621e824849db8762d0d6b21200', commit_message='Upload model', commit_description='', oid='dd2610e6d1fdc7621e824849db8762d0d6b21200', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('lakshay/linear-work-peft',token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Information Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75804f0963d448fbb0e7ba1e1d423bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lakshay/llama2-test/commit/9460af41bdcca6c6b9cafac27d3ee09a4bd6c36a', commit_message='Upload model', commit_description='', oid='9460af41bdcca6c6b9cafac27d3ee09a4bd6c36a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('lakshay/llama2-test',token='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', max_shard_size='2GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PI validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('custom_data/validation_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data.resume.values[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "error_list = list()\n",
    "correct_list = list()\n",
    "\n",
    "for uid,rt in tqdm(validation_data[['id','resume']].sample(frac=1).values[:200]):\n",
    "\n",
    "    eval_prompt = pi_eval_prompt.substitute(\n",
    "                pi_format=pi_format,\n",
    "                resume_text=rt)\n",
    "\n",
    "    sample_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            full_document = tokenizer.decode(model.generate(**sample_input, max_new_tokens=200)[0], skip_special_tokens=True)\n",
    "    except:\n",
    "        print('feck')\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        out_str = full_document.replace(eval_prompt,'').replace('$','')\n",
    "        out_json = ast.literal_eval(out_str)\n",
    "        u_info = {}\n",
    "        u_info[uid] = out_json\n",
    "        correct_list.append(u_info)\n",
    "    except:\n",
    "        error_list.append(full_document)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# correct_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hello there, $, yes'.replace('there,','').replace('$','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_list\n",
    "\n",
    "with open('custom_data/validation_output.pkl','wb') as f:\n",
    "    pickle.dump(correct_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b66f79022e4d62b7980aaa0d6e8ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/581 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlakshay/llama2-test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# tokenizer = LlamaTokenizer.from_pretrained(model_id)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhf_rthVXJBMwUqJSEayJxkiKZtRSIwFLEVwot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/modeling_utils.py:3646\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3642\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3643\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   3644\u001b[0m         }\n\u001b[1;32m   3645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 3646\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3647\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3648\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   3649\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   3650\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m             )\n\u001b[1;32m   3656\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   3658\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"lakshay/llama2-test\"\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16, token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
