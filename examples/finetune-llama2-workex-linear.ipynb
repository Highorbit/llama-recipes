{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "This notebook shows how to train a Llama 2 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA.\n",
    "\n",
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "We use the Hugging Face trainer and model which means that the checkpoint has to be converted from its original format into the dedicated Hugging Face format.\n",
    "The conversion can be achieved by running the `convert_llama_weights_to_hf.py` script provided with the transformer package.\n",
    "Given that the original checkpoint resides under `models/7B` we can install all requirements and convert the checkpoint with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets tqdm vllm\n",
    "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weighjts_to_hf.py')\"`\n",
    "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llama-recipes/src/llama_recipes/utils/dataset_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point model_id to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "train_data = load_from_disk(\"custom_data/linear_work_data.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  9 10:38:13 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   21C    P8              19W / 300W |      2MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Important \n",
    "\n",
    "It is important to consider here which model we're using to parse the resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f2aff8875f40d9ac5c1e688bd43162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16, token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/SageMaker/llama_root/src')\n",
    "sys.path.append('../llama-recipes/src/llama_recipes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt =work_prompt = f'''\n",
    "You are an accurate agent working for a job platform. You will be given the raw \n",
    "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
    "user from the resume. The response should be broken into a numbered list with each item of the list \n",
    "containing the complete and accurate information about the work experience of the users.\n",
    "1. Designation 1 @ Company 1 [From \"mm/yyy\" to \"mm/yyyy\"] : \"complete job description as given in resume\"\\n\n",
    "2. Designation 2 @ Company 2 [From \"mm/yyy\" to \"mm/yyyy\"] :  \"complete job description as given in resume\"\\n\n",
    "Please follow this structure closely and keep the response within the token limit.\" \n",
    "\n",
    "This is the resume text:\\n{{resume_text}}\\n\n",
    "This is the output in the required_format:\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_resume_text = '''\n",
    " S\\n EVANAND\\n\\n\\nEmail: sevaanand863@gmail.com\\nMobile: +919110416415\\n\\n\\nPROFESSIONAL SUMMARY:\\n Having 2+ years of technical experience in Analysis, Design, Development, Testing\\n and Implementation of Client Server Application and Data warehousing ETL (Extract,\\n Transform and Load) in Informatica Power Center 10.4 and INFORMATICA intelligent\\n cloud services.\\n Main areas of expertise are Developing and Testing the data warehousing\\n projects with data quality standards.\\n Extensive experience in Extraction, Transformation and Loading of data\\n directly from heterogeneous source systems like fat fles, Oracle by using\\n Informatica power center.\\n Tuned several mappings for the better performance and involved in Performance\\n Testing.\\n Implemented exceptional handling mechanism by using Exception transformation &amp;\\n Human Task.\\n Creating Informatica IICS mappings for the diferent plans using various\\n transformations.\\n Have working experience in Informatica Intelligent Cloud Services IICS components -\\n application integration, data integration, Informatica data quality and Informatica\\n power center and CRM application - Salesforce.\\n Worked on SCD Type1,SCD Type2 in IICS\\n Worked on Mapping, Mapping Task, Mapplet, Task Flows\\n Experience on all important General transformations.\\n Used informatica developer tool to develop the mapping with power center\\n transformations.\\n Customized SQL override queries where ever possible to minimize the use of Joiner,\\n Aggregator and Lookup Transformations.\\n Developed all the mappings according to the design document and mapping specs\\n provided and performed unit testing.\\n Used Parameterization for Mapping, Workfows and sessions.\\n Worked on running &amp; scheduling the Informatica jobs using Shell Scripts written on\\n the UNIX box.\\n Error handling &amp; issue analysis during the testing and maintenance.\\n Hands on dynamic parameter fle creation.\\n Identifying the bottlenecks and implement the Performance tuning &amp;\\n Optimization techniques in power center.\\n Review and initial approval for various Docs like IDS, IRS, PDI, KEDB, Mapping\\n sheets.\\n Good Knowledge on Data Warehousing concepts like Star Schema, Dimensions\\n and Fact tables.\\n Optimizing Informatica Mappings and Sessions to improve the performance.\\n Experience of handling slowly changing dimensions to maintain complete\\n history using Type I, Type II and Type III strategies.\\n Created UNIX Shell scripts to run the Informatica Workfows &amp; controlling the ETL\\n fow.\\n Hands on Admin activities.\\n Excellent problem-solving skills with strong technical background and good\\n interpersonal skills.\\n\\n\\n\\n\\nEXPERIENCE SUMMARY:\\n, Worked as a Programmer Analyst with COGNIZANT from Jan 2022 to April 2023.\\n\\n Worked as a Software Engineer with Birla Soft LTD from Jan 2021 to Jan 2022.\\n\\n\\n\\n\\n TECHNICAL ENVIRONMENT:\\nOperating System : Windows, Linux\\nTools : Informatica developer, IICS, PUTTY, SQL Developer and WinSCP\\nRDBMS : Oracle ,SQL, PostgreSQL\\nLanguages : Unix,\\nScheduling Tools : Autosys, Control-M\\n\\n\\n\\n PROJECT PROFILE:\\n\\n\\n #PROJECT 1\\n\\n Client : Verizon\\n Project Name : HR Union Recruit in\\n Domain : Telecom\\n Role : IICS Developer\\n Environment : IICS, Oracle 11g, PostgreSQL , Windows 10\\n\\nProject Description:\\n The Project HR Union involves the migration of severance&rsquo;s data in PeopleSoft to\\nPostgreSQL.\\n\\nInformatica Cloud&rsquo;s Data Integration Services consume the Data from Peoplesoft system\\nand perform the\\n\\nbusiness logic to load in Severance&rsquo;s database (PostgreSQL) and then provide the data to\\ndownstream\\n\\nvendors in the form of Files.\\n Responsibilities:\\n\\n Creating Informatica IICS mappings for the diferent plans using various\\n transformations.\\n Have working experience in Informatica Intelligent Cloud Services IICS\\n components - application integration, data integration, Informatica data quality\\n and Informatica power center and CRM application - Salesforce.\\n Analysis of the specifcations provided by the clients.\\n Used Various Transformations such as Sorted, Lookup, Joiner, Aggregator,\\n Sequence Generator. Lookup, Normalizer, Transaction Control Transformation.\\n Worked on Diferent tasks like Mapping Task Replication Task, Synchronization\\n Task, Power Center Task in IICS.\\n Designed, Developed and implemented ETL Processes using IICS Data\\n Integration\\n Created IICS connection using various cloud connectors in IICS Administrator\\n Extensively used informatica IICS&ndash; Mapping, Mapping Task, Task Flow.\\n, Developed complex mappings using transformations such as the Source\\n qualifer, Joiner, Aggregator, Update Strategy, Expression, Connected Lookup,\\n Unconnected Lookup and Router transformations.\\n Created informatica mappings for stage, Dimensions and Fact table loads.\\n Created SCD type-1 and type-2 mappings for loading the dimension tables.\\n Done extensive testing and wrote queries in SQL to ensure the loading of the\\n data.\\n Developed and implemented the coding of Informatica Mapping for the\\n diferent stages of ETL.\\n Involved in Unit testing\\n On-time Production migration without defects\\n Involved in Post production Support.\\n\\n\\n\\n\\n#Project 2\\n\\n Client : Discover Fin bank\\n Domain : Banking\\n Environment : Informatica power center 9.X, Oracle10g\\n Role : Informatica Support and Developer\\n\\n\\n\\nDISCRIPTIOIN:\\n\\n This application was designed to load member and subscriber eligibility information\\nas received from the customers in the form of fat fles and oracle database. The system\\nwas designed to store the eligibility information of the members belonging to the various\\ncontracts for the various vendor customer services being provided to them by the client.\\nIt was used to store the historical information pertaining to each and every member who\\nwas entitled to receive the customer services. The various other front-end applications\\nwould access this database to determine the authenticity of the members and the type of\\nservices they were entitled to the system.\\nResponsibilities:\\n\\n Understanding existing business model and customer requirements.\\n Understanding the mapping specifcations and requirements.\\n Managing priorities of tasks, scheduling and tracking progress.\\n Extraction of data from various sources using Informatica.\\n Designed various mappings for extracting data from various sources involving fat\\n fles and relational tables.\\n Used Source Analyzer and Warehouse Designer to import the source and target\\n database schemas and the mapping designer to map source to the target.\\n Used Transformation Developer to create the flters, joiner, update strategy, lookups\\n and\\n Aggregation transformations, which are used in mappings.\\n Created various tasks like sessions, worklets, and workfows in the workfow\\n manager to test the mapping during development.\\n To keep track of historical data slowly changing dimensions are implemented.\\n Created and Monitored Batches and Sessions using Informatica Power Centre.\\n Created and executed sessions and batches using Server Manager.\\n Worked with Mapping Variables and Mapping Parameters.\\n Developed all the mappings according to the design document and mapping specs\\n provided and performed unit testing.\\n, Created test plan, Test Design, Test scripts and responsible for implementation of\\n Test cases as Manual test scripts.\\n Developed mapping to load the data in slowly changing dimension.\\n Checked the output according to the specifcations.\\n Confgured and ran the Debugger from within the Mapping Designer to troubleshoot\\n the mapping before the normal run of the workfow.\\n Tuned several mappings for the better performance and involved in Performance\\n Testing.\\n Documenting test cases and Informatica mappings\\n Prepared documentation for business data fow from source to target and also for\\n the changes made to the mappings/sessions existing to eliminate the errors.\\n Provide weekly status report to the Project Manager and discuss issues related to\\n quality and deadlines.'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ep = eval_prompt.format(resume_text=example_resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'streamer': <transformers.generation.streamers.TextStreamer object at 0x7f74253eaf50>} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an accurate agent working for a job platform. You will be given the raw \n",
      "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
      "user from the resume. The response should be broken into a numbered list with each item of the list \n",
      "containing the complete and accurate information about the work experience of the users.\n",
      "1. Designation 1 @ Company 1 [From \"mm/yyy\" to \"mm/yyyy\"] : \"complete job description as given in resume\"\n",
      "\n",
      "2. Designation 2 @ Company 2 [From \"mm/yyy\" to \"mm/yyyy\"] :  \"complete job description as given in resume\"\n",
      "\n",
      "Please follow this structure closely and keep the response within the token limit.\" \n",
      "\n",
      "This is the resume text:\n",
      "\n",
      " S\n",
      " EVANAND\n",
      "\n",
      "\n",
      "Email: sevaanand863@gmail.com\n",
      "Mobile: +919110416415\n",
      "\n",
      "\n",
      "PROFESSIONAL SUMMARY:\n",
      " Having 2+ years of technical experience in Analysis, Design, Development, Testing\n",
      " and Implementation of Client Server Application and Data warehousing ETL (Extract,\n",
      " Transform and Load) in Informatica Power Center 10.4 and INFORMATICA intelligent\n",
      " cloud services.\n",
      " Main areas of expertise are Developing and Testing the data warehousing\n",
      " projects with data quality standards.\n",
      " Extensive experience in Extraction, Transformation and Loading of data\n",
      " directly from heterogeneous source systems like fat fles, Oracle by using\n",
      " Informatica power center.\n",
      " Tuned several mappings for the better performance and involved in Performance\n",
      " Testing.\n",
      " Implemented exceptional handling mechanism by using Exception transformation &amp;\n",
      " Human Task.\n",
      " Creating Informatica IICS mappings for the diferent plans using various\n",
      " transformations.\n",
      " Have working experience in Informatica Intelligent Cloud Services IICS components -\n",
      " application integration, data integration, Informatica data quality and Informatica\n",
      " power center and CRM application - Salesforce.\n",
      " Worked on SCD Type1,SCD Type2 in IICS\n",
      " Worked on Mapping, Mapping Task, Mapplet, Task Flows\n",
      " Experience on all important General transformations.\n",
      " Used informatica developer tool to develop the mapping with power center\n",
      " transformations.\n",
      " Customized SQL override queries where ever possible to minimize the use of Joiner,\n",
      " Aggregator and Lookup Transformations.\n",
      " Developed all the mappings according to the design document and mapping specs\n",
      " provided and performed unit testing.\n",
      " Used Parameterization for Mapping, Workfows and sessions.\n",
      " Worked on running &amp; scheduling the Informatica jobs using Shell Scripts written on\n",
      " the UNIX box.\n",
      " Error handling &amp; issue analysis during the testing and maintenance.\n",
      " Hands on dynamic parameter fle creation.\n",
      " Identifying the bottlenecks and implement the Performance tuning &amp;\n",
      " Optimization techniques in power center.\n",
      " Review and initial approval for various Docs like IDS, IRS, PDI, KEDB, Mapping\n",
      " sheets.\n",
      " Good Knowledge on Data Warehousing concepts like Star Schema, Dimensions\n",
      " and Fact tables.\n",
      " Optimizing Informatica Mappings and Sessions to improve the performance.\n",
      " Experience of handling slowly changing dimensions to maintain complete\n",
      " history using Type I, Type II and Type III strategies.\n",
      " Created UNIX Shell scripts to run the Informatica Workfows &amp; controlling the ETL\n",
      " fow.\n",
      " Hands on Admin activities.\n",
      " Excellent problem-solving skills with strong technical background and good\n",
      " interpersonal skills.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "EXPERIENCE SUMMARY:\n",
      ", Worked as a Programmer Analyst with COGNIZANT from Jan 2022 to April 2023.\n",
      "\n",
      " Worked as a Software Engineer with Birla Soft LTD from Jan 2021 to Jan 2022.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " TECHNICAL ENVIRONMENT:\n",
      "Operating System : Windows, Linux\n",
      "Tools : Informatica developer, IICS, PUTTY, SQL Developer and WinSCP\n",
      "RDBMS : Oracle ,SQL, PostgreSQL\n",
      "Languages : Unix,\n",
      "Scheduling Tools : Autosys, Control-M\n",
      "\n",
      "\n",
      "\n",
      " PROJECT PROFILE:\n",
      "\n",
      "\n",
      " #PROJECT 1\n",
      "\n",
      " Client : Verizon\n",
      " Project Name : HR Union Recruit in\n",
      " Domain : Telecom\n",
      " Role : IICS Developer\n",
      " Environment : IICS, Oracle 11g, PostgreSQL , Windows 10\n",
      "\n",
      "Project Description:\n",
      " The Project HR Union involves the migration of severance&rsquo;s data in PeopleSoft to\n",
      "PostgreSQL.\n",
      "\n",
      "Informatica Cloud&rsquo;s Data Integration Services consume the Data from Peoplesoft system\n",
      "and perform the\n",
      "\n",
      "business logic to load in Severance&rsquo;s database (PostgreSQL) and then provide the data to\n",
      "downstream\n",
      "\n",
      "vendors in the form of Files.\n",
      " Responsibilities:\n",
      "\n",
      " Creating Informatica IICS mappings for the diferent plans using various\n",
      " transformations.\n",
      " Have working experience in Informatica Intelligent Cloud Services IICS\n",
      " components - application integration, data integration, Informatica data quality\n",
      " and Informatica power center and CRM application - Salesforce.\n",
      " Analysis of the specifcations provided by the clients.\n",
      " Used Various Transformations such as Sorted, Lookup, Joiner, Aggregator,\n",
      " Sequence Generator. Lookup, Normalizer, Transaction Control Transformation.\n",
      " Worked on Diferent tasks like Mapping Task Replication Task, Synchronization\n",
      " Task, Power Center Task in IICS.\n",
      " Designed, Developed and implemented ETL Processes using IICS Data\n",
      " Integration\n",
      " Created IICS connection using various cloud connectors in IICS Administrator\n",
      " Extensively used informatica IICS&ndash; Mapping, Mapping Task, Task Flow.\n",
      ", Developed complex mappings using transformations such as the Source\n",
      " qualifer, Joiner, Aggregator, Update Strategy, Expression, Connected Lookup,\n",
      " Unconnected Lookup and Router transformations.\n",
      " Created informatica mappings for stage, Dimensions and Fact table loads.\n",
      " Created SCD type-1 and type-2 mappings for loading the dimension tables.\n",
      " Done extensive testing and wrote queries in SQL to ensure the loading of the\n",
      " data.\n",
      " Developed and implemented the coding of Informatica Mapping for the\n",
      " diferent stages of ETL.\n",
      " Involved in Unit testing\n",
      " On-time Production migration without defects\n",
      " Involved in Post production Support.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Project 2\n",
      "\n",
      " Client : Discover Fin bank\n",
      " Domain : Banking\n",
      " Environment : Informatica power center 9.X, Oracle10g\n",
      " Role : Informatica Support and Developer\n",
      "\n",
      "\n",
      "\n",
      "DISCRIPTIOIN:\n",
      "\n",
      " This application was designed to load member and subscriber eligibility information\n",
      "as received from the customers in the form of fat fles and oracle database. The system\n",
      "was designed to store the eligibility information of the members belonging to the various\n",
      "contracts for the various vendor customer services being provided to them by the client.\n",
      "It was used to store the historical information pertaining to each and every member who\n",
      "was entitled to receive the customer services. The various other front-end applications\n",
      "would access this database to determine the authenticity of the members and the type of\n",
      "services they were entitled to the system.\n",
      "Responsibilities:\n",
      "\n",
      " Understanding existing business model and customer requirements.\n",
      " Understanding the mapping specifcations and requirements.\n",
      " Managing priorities of tasks, scheduling and tracking progress.\n",
      " Extraction of data from various sources using Informatica.\n",
      " Designed various mappings for extracting data from various sources involving fat\n",
      " fles and relational tables.\n",
      " Used Source Analyzer and Warehouse Designer to import the source and target\n",
      " database schemas and the mapping designer to map source to the target.\n",
      " Used Transformation Developer to create the flters, joiner, update strategy, lookups\n",
      " and\n",
      " Aggregation transformations, which are used in mappings.\n",
      " Created various tasks like sessions, worklets, and workfows in the workfow\n",
      " manager to test the mapping during development.\n",
      " To keep track of historical data slowly changing dimensions are implemented.\n",
      " Created and Monitored Batches and Sessions using Informatica Power Centre.\n",
      " Created and executed sessions and batches using Server Manager.\n",
      " Worked with Mapping Variables and Mapping Parameters.\n",
      " Developed all the mappings according to the design document and mapping specs\n",
      " provided and performed unit testing.\n",
      ", Created test plan, Test Design, Test scripts and responsible for implementation of\n",
      " Test cases as Manual test scripts.\n",
      " Developed mapping to load the data in slowly changing dimension.\n",
      " Checked the output according to the specifcations.\n",
      " Confgured and ran the Debugger from within the Mapping Designer to troubleshoot\n",
      " the mapping before the normal run of the workfow.\n",
      " Tuned several mappings for the better performance and involved in Performance\n",
      " Testing.\n",
      " Documenting test cases and Informatica mappings\n",
      " Prepared documentation for business data fow from source to target and also for\n",
      " the changes made to the mappings/sessions existing to eliminate the errors.\n",
      " Provide weekly status report to the Project Manager and discuss issues related to\n",
      " quality and deadlines.'\n",
      "\n",
      "\n",
      "This is the output in the required_format:\n",
      "\n",
      "1. Designation: Programmer Analyst\n",
      " Company: Cognizant\n",
      " From: Jan 2022 To: April 2023\n",
      "\n",
      "2. Designation: Software Engineer\n",
      " Company: Birla Soft LTD\n",
      " From: Jan 2021 To: Jan 2022\n",
      "\n",
      "3. Designation: IICS Developer\n",
      " Client: Verizon\n",
      " Project Name: HR Union Recruit in\n",
      " Domain: Telecom\n",
      " Role: IICS Developer\n",
      " Environment: IICS, Oracle 11g, PostgreSQL , Windows 10\n",
      "\n",
      "4. Designation: Informatica Support and Developer\n",
      " Client: Discover Fin bank\n",
      " Domain: Banking\n",
      " Environment: Informatica power center 9.X, Oracle10g\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer)\n",
    "model_input = tokenizer(ep,streamer=streamer,return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "    \n",
    "    # peft_config = LoraConfig(\n",
    "    #     task_type=TaskType.CAUSAL_LM,\n",
    "    #     inference_mode=False,\n",
    "    #     r=8,\n",
    "    #     lora_alpha=32,\n",
    "    #     lora_dropout=0.05,\n",
    "    #     target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    # )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 5: Define an optional profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/linear_workex\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  9 10:41:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   27C    P0              74W / 300W |  16496MiB / 23028MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     19548      C   ...r/anaconda3/envs/python3/bin/python    16488MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 16:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.912200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.846700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.840800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, on the 9th of Jan, the year of our lord 2024\n"
     ]
    }
   ],
   "source": [
    "print('done, on the 9th of Jan, the year of our lord 2024')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 7:\n",
    "Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 8:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('custom_data/model_eval_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRADEEP KUMAR\n",
      "Project Manager- IT Infra\n",
      " pkindians@gmail.com\n",
      " 9910393860\n",
      " 1097/2 FF Pinewood Enclave Sec-2 Wave City Ghaziabad U.P.-201002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Professional Experience\n",
      "Project Manager-IT Infra, 04/2022  present | Noida, India\n",
      "iBoss Tech Solutions Private Limited\n",
      " Work closely with IT Director to leverage IT for business benefit.\n",
      " Build system and process for smooth operations.\n",
      " Run projects to ensure that they meet deadline, customer requirements and organizational goals in\n",
      " efficient manner.\n",
      " Plan, schedule and supervise the work of each tech team to ensure the services are provided on time and\n",
      " in efficient manner\n",
      " Evaluate, plan and procure, operationalize and retire appropriate technology solutions.\n",
      " Mange relevant contracts and ensure compliance and governance.\n",
      " Control cost and budgeting regarding IT systems.\n",
      " Responsible for managing Operations and Projects ensure highest uptime of IT services. Service call\n",
      " closure to meet business SLAs and ensure all systems are as per latest standards.\n",
      " Conduct Kick-off meetings to review Project Objectives, SoW, Change Control Policy.\n",
      " Take care of Vendor Management, Incident Management, Change Management and Provisioning\n",
      " requests related to inter dept work.\n",
      "\n",
      "IT PROJECT COORDINATOR, Bharat Electronics Limited 07/2021  04/2022 | GHAZIABAD, India\n",
      "IT-Administration and DataBase Management for Command Center.\n",
      "Monitoring and Maintenance of ERP/non-ERP hardware consisting of servers, storage system at data\n",
      "center including replication at DR site.\n",
      "Monitoring of Data Center IT Infrastructure and coordination with different OEM in case of maintenance like\n",
      "breakdown/preventive, up-gradation of system hardware/firmware/OS, etc as well as installation and\n",
      "commissioning of new systems at the Data Center.\n",
      "Key Tasks:\n",
      "ESXi installation, creating VM using Vmware, and running IT monitoring applications\n",
      "Maintaining the running condition of the Data Centre to follow up SLA.\n",
      "L2/L3 Switch Configuration and Server Management using UCS Manager\n",
      "Vsphere and Load Balancer for Cluster-based Virtualisation monitoring.\n",
      "Tools used: ITOM, AM, SM, UCMDB, AuditVault, EMS, Vmware, Vsphere,Vcentre Server, UCS Manager, ERP,\n",
      "ESXi Bare Metal Hypervisor.\n",
      "\n",
      "Deputy Engineer, Bharat Electronics Limited 03/2018  03/2021 | Bangalore, India\n",
      "1.Project- Election Commission VVPAT 2014, worked with the module designing team. Planned layout for\n",
      "mass manufacturing of the final product.\n",
      "2. Kerala Optical Fiber Network & Data Acquisition Project for KERALA under KSITL and KSEB:\n",
      "For around 2yrs worked in a Project Management position for the Optical Fiber Project owned by Kerala\n",
      "State Govt. undertaken by Bharat Electronics Limited, Bangalore.\n",
      "Expertise in:\n",
      "1. Skilled in OFC Technologies (Optical Fiber Cable properties, Ducts, FDMS, etc in Project and O&M)\n",
      "2. Experience in leading OFC Network Rollout Project desired. 3. Installation, commissioning, provisioning\n",
      "and acceptance testing of the OFC Network.\n",
      "4. Installation, commissioning, provisioning and acceptance testing of the OFC Network.\n",
      "5. Follow the schedule for Timely Completion of the Link survey and Project Deployment.\n",
      "6. Identification and mitigation of risks during the project\n",
      "7. Excellent communications and presentation skills.\n",
      "8. Working knowledge of EMS and NMS for remote management of OFC Network\n",
      "9. Reporting and taking corrective/preventive action during project and Network, Equipment and Service\n",
      "level faults.\n",
      ",10. Manage assets, materials and contracts for OFC Network Rollout.\n",
      "11. Monitor performance of fiber network KPIs against defined SLAs\n",
      "12. Coordinate with network operations center and field team for issue resolution.\n",
      "13. Ensure sufficient material in the Central/Zonal warehouse during OFC Network rollout and O&M.\n",
      "14. Ensure implementation of quality, health, safety, environmental, and fire policies\n",
      "15. Maintain quality of fiber network and ensure the quality of work during fault restoration.\n",
      "\n",
      "System Engineer, ITI Limited 08/2015  03/2018 | Delhi, India\n",
      "1. Project Operations and maintenance, Telecom Switch (OCB-283) All Switching system and\n",
      "PSTN servers running Telephone exchanges of MTNL(Delhi) and BSNL(UP-W).\n",
      "Data Centre and NOC Management for BSNL and MTNL\n",
      "KEY FEATURES OF THE TASK:\n",
      "As a Project cum site in charge of all the regular and annual activities to be done by.\n",
      "Under the Annual Maintenance Contract, we look for all the intricacies related to hardware and\n",
      "software.\n",
      "i. CCR (Call completion ratio)\n",
      "ii. Buffer storage of the stations.\n",
      "iii. Hard disk status in the maintenance station(SMM)\n",
      "2. Project GPON (Gigabyte passive optical network)\n",
      "Under this project, all the rural regions of the country are connected to high-speed broadband networks.\n",
      "This is\n",
      "under the initiative of the ministry running a project named NOFN.\n",
      "Under NOFN all the local villages under Gram Panchayat are agreed to be connected to high-speed\n",
      "broadband network so that the internet facility could be easily accessible to all the local villages of India.\n",
      "We\n",
      "as a team completed AT in UP(W) and other regions of North India for OLT and ONT installed by the\n",
      "consortium partner.\n",
      "\n",
      "Education\n",
      "B-Tech, Uttar Pradesh Technical University 06/2010  07/2014 | Ghaziabad, India\n",
      "\n",
      "Skills\n",
      "Telecom O&M Service Level Maintenance\n",
      "\n",
      "Production Control Project Coordination\n",
      "\n",
      "Vendor Management System Integrator\n",
      "\n",
      "Maintenance of FFTH N/w Team Management\n",
      "\n",
      "Electronics Measuring Instruments PCB Troubuleshooting\n",
      "\n",
      "Data centre Supervision, Coordination & Tech\n",
      " Support\n",
      "Liasoning\n",
      "local authority/building owners etc OTDR and LPSM\n",
      "\n",
      "GPON, OLT & ONT RHEL\n",
      "\n",
      "EMS Virtual Machines/ Vmware sphere\n",
      "\n",
      "NMS Asset Manager\n",
      "\n",
      "CISCO Blade server Shell Scripting, Patch Upgradation for DC/DR,\n",
      " SLA Management and Preventing Down Time\n",
      "\n",
      "Languages\n",
      "Shell Scripting\n",
      ",Certificates\n",
      " PSTN Installation and Maintenance SPV Designing, Planning, Installation and\n",
      " Maintenance\n",
      " Agile Scrum Foundation\n",
      " PMP Basics\n",
      " Agile Scrum Master\n",
      " Introduction to Six Sigma\n",
      " Project Execution: Running the Project - Google\n",
      " Project Planning: Putting it all together- Google\n",
      " Program Stake Holder Mgt\n",
      "\n",
      "Interests\n",
      "Reading, Writing\n",
      "\n",
      "Operations and Projects\n",
      "Kerala Fiber Optics N/w 01/2020  03/2021\n",
      "Optical Fiber Project owned by Kerala State Govt. undertaken by Bharat Electronics Limited, Bangalore. The\n",
      "project aims to provide connectivity to 30K govt. and educational institutes as part of the Right to Internet\n",
      "Access.\n",
      "Taking from route survey to capturing the GIS coordinates and handling out NOC installation process with\n",
      "network element integration comes under the roles and responsibilities of District In charge.\n",
      "\n",
      "Project Small Arms Training System 09/2018  12/2019\n",
      "Designed and developed Electronics Target System under Project Small Arms Training Simulator as a Firing\n",
      "range simulation to be used in Armed forces.\n",
      "System sizing and selection of components for the up-gradation of old model has also been done with the\n",
      "Simulator team.\n",
      "\n",
      "Voter Verified Paper Audit Trial 04/2018  08/2018\n",
      "For the Election Commission VVPAT 2014 Project, worked with the module designing team. Planned layout\n",
      "for mass manufacturing of the final product.Finalized assembly line work structure and handled supply\n",
      "chain management and quality management.\n",
      "\n",
      "GPON 06/2016  03/2018\n",
      "Under this project, all the rural regions of the country are connected to high-speed broadband networks.\n",
      "This is\n",
      "under the initiative of the ministry running a project named NOFN.\n",
      "Under NOFN all the local villages under Gram Panchayat are agreed to be connected to high-speed\n",
      "broadband network so that the internet facility could be easily accessible to all the local villages of India.\n",
      "We\n",
      "as a team completed AT in UP(W) and other regions of North India for OLT and ONT installed by the\n",
      "consortium partner.\n",
      "\n",
      "PSTN Switch Preventive Maintenance 08/2015  05/2016\n",
      "Project Installation and maintenance of Telecom Switch (OCB-283).\n",
      "Annual Maintenance Contract for H/w and S/w of All Switching systems and PSTN servers running under\n",
      "the roof of MTNL(Delhi) and BSNL(UP-W) telephone exchanges. Maintaining the running condition of the\n",
      "systems and managing call traffic flow through tandem exchanges. Provisioning and implementing\n",
      "connectivity of Remote Subscriber Unit to the main telephone exchanges\n",
      "\n",
      "Declaration\n",
      "\n",
      "PRADEEPKUMAR\n",
      "GHAZIABAD\n"
     ]
    }
   ],
   "source": [
    "rt = eval_df.sample()['resume'].values[0]\n",
    "rt = html.unescape(rt)\n",
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt = f'''\n",
    "You are an accurate agent working for a job platform. You will be given the raw \n",
    "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
    "user from the resume. The response should be presented into a numbered list with each item of the list \n",
    "being an unbroken line of text containing the complete and accurate information about the work experience of the users. \n",
    "Here is an example structure:\\n\n",
    "1. Designation 1 @ Company 1 [From \"mm/yyy\" to \"mm/yyyy\"] : \"complete job description as given in resume\"\\n\n",
    "2. Designation 2 @ Company 2 [From \"mm/yyy\" to \"mm/yyyy\"] :  \"complete job description as given in resume\"\\n\n",
    "Please follow this structure accurately and keep the response within the token limit.\" \n",
    "\n",
    "This is the resume text:\\n{{resume_text}}\\n\n",
    "This is the output in the required_format:\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ep = eval_prompt.format(resume_text=rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'streamer': <transformers.generation.streamers.TextStreamer object at 0x7f74cc601510>} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an accurate agent working for a job platform. You will be given the raw \n",
      "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
      "user from the resume. The response should be presented into a numbered list with each item of the list \n",
      "being an unbroken line of text containing the complete and accurate information about the work experience of the users. \n",
      "Here is an example structure:\n",
      "\n",
      "1. Designation 1 @ Company 1 [From \"mm/yyy\" to \"mm/yyyy\"] : \"complete job description as given in resume\"\n",
      "\n",
      "2. Designation 2 @ Company 2 [From \"mm/yyy\" to \"mm/yyyy\"] :  \"complete job description as given in resume\"\n",
      "\n",
      "Please follow this structure accurately and keep the response within the token limit.\" \n",
      "\n",
      "This is the resume text:\n",
      "PRADEEP KUMAR\n",
      "Project Manager- IT Infra\n",
      " pkindians@gmail.com\n",
      " 9910393860\n",
      " 1097/2 FF Pinewood Enclave Sec-2 Wave City Ghaziabad U.P.-201002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Professional Experience\n",
      "Project Manager-IT Infra, 04/2022  present | Noida, India\n",
      "iBoss Tech Solutions Private Limited\n",
      " Work closely with IT Director to leverage IT for business benefit.\n",
      " Build system and process for smooth operations.\n",
      " Run projects to ensure that they meet deadline, customer requirements and organizational goals in\n",
      " efficient manner.\n",
      " Plan, schedule and supervise the work of each tech team to ensure the services are provided on time and\n",
      " in efficient manner\n",
      " Evaluate, plan and procure, operationalize and retire appropriate technology solutions.\n",
      " Mange relevant contracts and ensure compliance and governance.\n",
      " Control cost and budgeting regarding IT systems.\n",
      " Responsible for managing Operations and Projects ensure highest uptime of IT services. Service call\n",
      " closure to meet business SLAs and ensure all systems are as per latest standards.\n",
      " Conduct Kick-off meetings to review Project Objectives, SoW, Change Control Policy.\n",
      " Take care of Vendor Management, Incident Management, Change Management and Provisioning\n",
      " requests related to inter dept work.\n",
      "\n",
      "IT PROJECT COORDINATOR, Bharat Electronics Limited 07/2021  04/2022 | GHAZIABAD, India\n",
      "IT-Administration and DataBase Management for Command Center.\n",
      "Monitoring and Maintenance of ERP/non-ERP hardware consisting of servers, storage system at data\n",
      "center including replication at DR site.\n",
      "Monitoring of Data Center IT Infrastructure and coordination with different OEM in case of maintenance like\n",
      "breakdown/preventive, up-gradation of system hardware/firmware/OS, etc as well as installation and\n",
      "commissioning of new systems at the Data Center.\n",
      "Key Tasks:\n",
      "ESXi installation, creating VM using Vmware, and running IT monitoring applications\n",
      "Maintaining the running condition of the Data Centre to follow up SLA.\n",
      "L2/L3 Switch Configuration and Server Management using UCS Manager\n",
      "Vsphere and Load Balancer for Cluster-based Virtualisation monitoring.\n",
      "Tools used: ITOM, AM, SM, UCMDB, AuditVault, EMS, Vmware, Vsphere,Vcentre Server, UCS Manager, ERP,\n",
      "ESXi Bare Metal Hypervisor.\n",
      "\n",
      "Deputy Engineer, Bharat Electronics Limited 03/2018  03/2021 | Bangalore, India\n",
      "1.Project- Election Commission VVPAT 2014, worked with the module designing team. Planned layout for\n",
      "mass manufacturing of the final product.\n",
      "2. Kerala Optical Fiber Network & Data Acquisition Project for KERALA under KSITL and KSEB:\n",
      "For around 2yrs worked in a Project Management position for the Optical Fiber Project owned by Kerala\n",
      "State Govt. undertaken by Bharat Electronics Limited, Bangalore.\n",
      "Expertise in:\n",
      "1. Skilled in OFC Technologies (Optical Fiber Cable properties, Ducts, FDMS, etc in Project and O&M)\n",
      "2. Experience in leading OFC Network Rollout Project desired. 3. Installation, commissioning, provisioning\n",
      "and acceptance testing of the OFC Network.\n",
      "4. Installation, commissioning, provisioning and acceptance testing of the OFC Network.\n",
      "5. Follow the schedule for Timely Completion of the Link survey and Project Deployment.\n",
      "6. Identification and mitigation of risks during the project\n",
      "7. Excellent communications and presentation skills.\n",
      "8. Working knowledge of EMS and NMS for remote management of OFC Network\n",
      "9. Reporting and taking corrective/preventive action during project and Network, Equipment and Service\n",
      "level faults.\n",
      ",10. Manage assets, materials and contracts for OFC Network Rollout.\n",
      "11. Monitor performance of fiber network KPIs against defined SLAs\n",
      "12. Coordinate with network operations center and field team for issue resolution.\n",
      "13. Ensure sufficient material in the Central/Zonal warehouse during OFC Network rollout and O&M.\n",
      "14. Ensure implementation of quality, health, safety, environmental, and fire policies\n",
      "15. Maintain quality of fiber network and ensure the quality of work during fault restoration.\n",
      "\n",
      "System Engineer, ITI Limited 08/2015  03/2018 | Delhi, India\n",
      "1. Project Operations and maintenance, Telecom Switch (OCB-283) All Switching system and\n",
      "PSTN servers running Telephone exchanges of MTNL(Delhi) and BSNL(UP-W).\n",
      "Data Centre and NOC Management for BSNL and MTNL\n",
      "KEY FEATURES OF THE TASK:\n",
      "As a Project cum site in charge of all the regular and annual activities to be done by.\n",
      "Under the Annual Maintenance Contract, we look for all the intricacies related to hardware and\n",
      "software.\n",
      "i. CCR (Call completion ratio)\n",
      "ii. Buffer storage of the stations.\n",
      "iii. Hard disk status in the maintenance station(SMM)\n",
      "2. Project GPON (Gigabyte passive optical network)\n",
      "Under this project, all the rural regions of the country are connected to high-speed broadband networks.\n",
      "This is\n",
      "under the initiative of the ministry running a project named NOFN.\n",
      "Under NOFN all the local villages under Gram Panchayat are agreed to be connected to high-speed\n",
      "broadband network so that the internet facility could be easily accessible to all the local villages of India.\n",
      "We\n",
      "as a team completed AT in UP(W) and other regions of North India for OLT and ONT installed by the\n",
      "consortium partner.\n",
      "\n",
      "Education\n",
      "B-Tech, Uttar Pradesh Technical University 06/2010  07/2014 | Ghaziabad, India\n",
      "\n",
      "Skills\n",
      "Telecom O&M Service Level Maintenance\n",
      "\n",
      "Production Control Project Coordination\n",
      "\n",
      "Vendor Management System Integrator\n",
      "\n",
      "Maintenance of FFTH N/w Team Management\n",
      "\n",
      "Electronics Measuring Instruments PCB Troubuleshooting\n",
      "\n",
      "Data centre Supervision, Coordination & Tech\n",
      " Support\n",
      "Liasoning\n",
      "local authority/building owners etc OTDR and LPSM\n",
      "\n",
      "GPON, OLT & ONT RHEL\n",
      "\n",
      "EMS Virtual Machines/ Vmware sphere\n",
      "\n",
      "NMS Asset Manager\n",
      "\n",
      "CISCO Blade server Shell Scripting, Patch Upgradation for DC/DR,\n",
      " SLA Management and Preventing Down Time\n",
      "\n",
      "Languages\n",
      "Shell Scripting\n",
      ",Certificates\n",
      " PSTN Installation and Maintenance SPV Designing, Planning, Installation and\n",
      " Maintenance\n",
      " Agile Scrum Foundation\n",
      " PMP Basics\n",
      " Agile Scrum Master\n",
      " Introduction to Six Sigma\n",
      " Project Execution: Running the Project - Google\n",
      " Project Planning: Putting it all together- Google\n",
      " Program Stake Holder Mgt\n",
      "\n",
      "Interests\n",
      "Reading, Writing\n",
      "\n",
      "Operations and Projects\n",
      "Kerala Fiber Optics N/w 01/2020  03/2021\n",
      "Optical Fiber Project owned by Kerala State Govt. undertaken by Bharat Electronics Limited, Bangalore. The\n",
      "project aims to provide connectivity to 30K govt. and educational institutes as part of the Right to Internet\n",
      "Access.\n",
      "Taking from route survey to capturing the GIS coordinates and handling out NOC installation process with\n",
      "network element integration comes under the roles and responsibilities of District In charge.\n",
      "\n",
      "Project Small Arms Training System 09/2018  12/2019\n",
      "Designed and developed Electronics Target System under Project Small Arms Training Simulator as a Firing\n",
      "range simulation to be used in Armed forces.\n",
      "System sizing and selection of components for the up-gradation of old model has also been done with the\n",
      "Simulator team.\n",
      "\n",
      "Voter Verified Paper Audit Trial 04/2018  08/2018\n",
      "For the Election Commission VVPAT 2014 Project, worked with the module designing team. Planned layout\n",
      "for mass manufacturing of the final product.Finalized assembly line work structure and handled supply\n",
      "chain management and quality management.\n",
      "\n",
      "GPON 06/2016  03/2018\n",
      "Under this project, all the rural regions of the country are connected to high-speed broadband networks.\n",
      "This is\n",
      "under the initiative of the ministry running a project named NOFN.\n",
      "Under NOFN all the local villages under Gram Panchayat are agreed to be connected to high-speed\n",
      "broadband network so that the internet facility could be easily accessible to all the local villages of India.\n",
      "We\n",
      "as a team completed AT in UP(W) and other regions of North India for OLT and ONT installed by the\n",
      "consortium partner.\n",
      "\n",
      "PSTN Switch Preventive Maintenance 08/2015  05/2016\n",
      "Project Installation and maintenance of Telecom Switch (OCB-283).\n",
      "Annual Maintenance Contract for H/w and S/w of All Switching systems and PSTN servers running under\n",
      "the roof of MTNL(Delhi) and BSNL(UP-W) telephone exchanges. Maintaining the running condition of the\n",
      "systems and managing call traffic flow through tandem exchanges. Provisioning and implementing\n",
      "connectivity of Remote Subscriber Unit to the main telephone exchanges\n",
      "\n",
      "Declaration\n",
      "\n",
      "PRADEEPKUMAR\n",
      "GHAZIABAD\n",
      "\n",
      "This is the output in the required_format:\n",
      "\n",
      "1. Project Manager- IT Infra, 04/2022  present | Noida, India : Work closely with IT Director to leverage IT for business benefit. Build system and process for smooth operations. Run projects to ensure that they meet deadline, customer requirements and organizational goals in efficient manner. Plan, schedule and supervise the work of each tech team to ensure the services are provided on time and in efficient manner. Evaluate, plan and procure, operationalize and retire appropriate technology solutions. Manage relevant contracts and ensure compliance and governance. Control cost and budgeting regarding IT systems. Responsible for managing Operations and Projects ensure highest uptime of IT services. Service call closure to meet business SLAs and ensure all systems are as per latest standards. Conduct Kick-off meetings to review Project Objectives, SoW, Change Control Policy. Take care of Vendor Management, Incident Management, Change Management and Provisioning requests related to inter dept work.\n",
      "\n",
      "2. IT PROJECT COORDINATOR, Bharat Electronics Limited 07/2021  04/2022 | GHAZIABAD, India : Monitoring and Maintenance of ERP/non-ERP hardware consisting of servers, storage system at data center including replication at DR site. Monitoring of Data Center IT Infrastructure and coordination with different OEM in case of maintenance like breakdown/preventive, up-gradation of system hardware/firmware/OS, etc as well as installation and commissioning of new systems at the Data Center. Key Tasks: ESXi installation, creating VM using Vmware, and running IT monitoring applications Maintaining the running condition of the Data Centre to follow up SLA. L2/L3 Switch Configuration and Server Management using UCS Manager Vsphere and Load Balancer for Cluster-based Virtualisation monitoring. Tools used: ITOM, AM, SM, UCMDB, AuditVault, EMS, Vmware, Vsphere,Vcentre Server, UCS Manager, ERP, ESXi Bare Metal Hypervisor.\n",
      "\n",
      "3. Deputy Engineer, Bharat Electronics Limited 03/2018  03/2021 | Bangalore, India : 1. Project- Election Commission VVPAT 2014, worked with the module designing team. Planned layout for mass manufacturing of the final product. 2. Kerala Optical Fiber Network & Data Acquisition Project for KERALA under KSITL and KSEB: For around 2yrs worked in a Project Management position for the Optical Fiber Project owned by Kerala State Govt. undertaken by Bharat Electronics Limited, Bangalore. Expertise in: 1. Skilled in OFC Technologies (Optical Fiber Cable properties, Ducts, FDMS, etc in Project and O&M) 2. Experience in leading OFC Network Rollout Project desired. 3. Installation, commissioning, provisioning and acceptance testing of the OFC Network. 4. Installation, commissioning, provisioning and acceptance testing of the OFC Network. 5. Follow the schedule for Timely Completion of the Link survey and Project Deployment. 6. Identification and mitigation of risks during the project and Network, Equipment and Service level faults. 7. Excellent communications and presentation skills. 8. Working knowledge of EMS and NMS for remote management of OFC Network 9. Reporting and taking corrective/preventive action during project and Network, Equipment and Service level faults. 10. Manage assets, materials and contracts for OFC Network Rollout. 11. Monitor performance of fiber network KPIs against defined SLAs 12. Coordinate with network operations center and field team for issue resolution.\n",
      "\n",
      "4. System Engineer, ITI Limited 08/2015  03/2018 | Delhi, India : 1. Project Operations and maintenance, Telecom Switch (OCB-283) All Switching system and PSTN servers running Telephone exchanges of MTNL(Delhi) and BSNL(UP-W). Data Centre and NOC Management for BSNL and MTNL KEY FEATURES OF THE TASK: As a Project cum site in charge of all the regular and annual activities to be done by. Under the Annual Maintenance Contract, we look for all the intrica\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer)\n",
    "model_input = tokenizer(ep,streamer=streamer,return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_prompt = f'''\n",
    "# You are a helpful language model working for a job platform. You will be given the raw \n",
    "#  unstructured text of a user's resume, and the task is to extract the work experience of the \n",
    "#  user from the raw text in the following format: \\n{{work_format}}\\n\n",
    "\n",
    "#  This is the resume text:\\n{{resume_text}}\\n\n",
    "#  This is the output in the required format:\\n\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# work_format = '''{\n",
    "#     'work_experience': [{'company': 'company Name 1',\n",
    "#                          'role': 'job designation 1',\n",
    "#                          'start_date': 'mm/yyyy',\n",
    "#                          'end_date': 'mm/yyyy',\n",
    "#                          'description': 'complete Job description taken from resume'},\n",
    "#                         {'company': 'company name 2',\n",
    "#                          'role': 'job designation 2',\n",
    "#                          'start_date': mm/yyyy',\n",
    "#                          'end_date': 'mm/yyyy',\n",
    "#                          'description': 'complete Job description taken from resume'}]\n",
    "# }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060ff99b9cd8410aaeca05b62bd1ab17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lakshay/linear-work-peft/commit/dd2610e6d1fdc7621e824849db8762d0d6b21200', commit_message='Upload model', commit_description='', oid='dd2610e6d1fdc7621e824849db8762d0d6b21200', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('lakshay/linear-work-peft',token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Information Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75804f0963d448fbb0e7ba1e1d423bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lakshay/llama2-test/commit/9460af41bdcca6c6b9cafac27d3ee09a4bd6c36a', commit_message='Upload model', commit_description='', oid='9460af41bdcca6c6b9cafac27d3ee09a4bd6c36a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('lakshay/llama2-test',token='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', max_shard_size='2GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PI validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('custom_data/validation_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data.resume.values[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "error_list = list()\n",
    "correct_list = list()\n",
    "\n",
    "for uid,rt in tqdm(validation_data[['id','resume']].sample(frac=1).values[:200]):\n",
    "\n",
    "    eval_prompt = pi_eval_prompt.substitute(\n",
    "                pi_format=pi_format,\n",
    "                resume_text=rt)\n",
    "\n",
    "    sample_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            full_document = tokenizer.decode(model.generate(**sample_input, max_new_tokens=200)[0], skip_special_tokens=True)\n",
    "    except:\n",
    "        print('feck')\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        out_str = full_document.replace(eval_prompt,'').replace('$','')\n",
    "        out_json = ast.literal_eval(out_str)\n",
    "        u_info = {}\n",
    "        u_info[uid] = out_json\n",
    "        correct_list.append(u_info)\n",
    "    except:\n",
    "        error_list.append(full_document)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# correct_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hello there, $, yes'.replace('there,','').replace('$','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_list\n",
    "\n",
    "with open('custom_data/validation_output.pkl','wb') as f:\n",
    "    pickle.dump(correct_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b66f79022e4d62b7980aaa0d6e8ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/581 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlakshay/llama2-test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# tokenizer = LlamaTokenizer.from_pretrained(model_id)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhf_rthVXJBMwUqJSEayJxkiKZtRSIwFLEVwot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/modeling_utils.py:3646\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3642\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3643\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   3644\u001b[0m         }\n\u001b[1;32m   3645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 3646\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3647\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3648\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   3649\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   3650\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m             )\n\u001b[1;32m   3656\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   3658\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"lakshay/llama2-test\"\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16, token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
