{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets tqdm vllm\n",
    "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weighjts_to_hf.py')\"`\n",
    "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llama-recipes/src/llama_recipes/utils/dataset_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Experience (Shell Details)\n",
    "This notebook is meant to fine-tune Llama2 on Work Experience Details (all work experience minus the job description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point model_id to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train_data = load_from_disk(\"../custom_data/mistral/work_details.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  6 10:57:36 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   27C    P0              54W / 300W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Important \n",
    "\n",
    "It is important to consider here which model we're using to parse the resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3797c5c1a04442c8528db8ad488f9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map='auto', torch_dtype=torch.float16, token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  6 10:58:44 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   29C    P0              56W / 300W |   5244MiB / 23028MiB |     22%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1344      C   ...oedge/llama-recipes/env/bin/python3     5236MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/SageMaker/llama_root/src')\n",
    "sys.path.append('../llama-recipes/src/llama_recipes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_prompt = f'''\n",
    "You are an accurate agent working for a job platform. You will be given the raw \n",
    "unstructured text of a user's resume, and the task is to extract specific details about the work experience of the \n",
    "user from the resume. The JSON should include a \"work_experience\" key with an array of objects. \n",
    "Each object represents a job and should contain keys for \"company\", \"role\", \"start_date\", \"end_date\".\n",
    "Dates should be in \"mm/yyyy\" format. Please provide the data in a concise JSON format\n",
    "Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces. Here is an example structure::\n",
    "\n",
    "Please follow this structure closely and keep the response within the token limit.\" \\n{{query_format}}\\n\n",
    "\n",
    "This is the resume text:\\n{{resume_text}}\\n\n",
    "This is the output in the required_format:\\n\n",
    "'''\n",
    "\n",
    "work_format = '''\n",
    "[\n",
    "    {\"company\":\"Example Company 1\",\n",
    "    \"role\":\"Example Role 1\",\n",
    "    \"start_date\":\"mm/yyyy\",\n",
    "    \"end_date\":\"mm/yyyy\"},\n",
    "    {\"company\":\"Example Company 2\",\n",
    "    \"role\":\"Example Role 2\",\"\n",
    "    start_date\":\"mm/yyyy\",\n",
    "    \"end_date\":\"mm/yyyy\"}\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "df = pd.read_csv('../custom_data/latest_work_exp_28dec.csv')\n",
    "base_model_test_rt = html.unescape(df['resume'].sample().values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = work_prompt.format(resume_text=base_model_test_rt,\n",
    "                          query_format=work_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "2024-05-06 10:58:55.911342: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-06 10:58:57.045342: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-06 10:58:58.521494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an accurate agent working for a job platform. You will be given the raw \n",
      "unstructured text of a user's resume, and the task is to extract specific details about the work experience of the \n",
      "user from the resume. The JSON should include a \"work_experience\" key with an array of objects. \n",
      "Each object represents a job and should contain keys for \"company\", \"role\", \"start_date\", \"end_date\".\n",
      "Dates should be in \"mm/yyyy\" format. Please provide the data in a concise JSON format\n",
      "Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces. Here is an example structure::\n",
      "\n",
      "Please follow this structure closely and keep the response within the token limit.\" \n",
      "\n",
      "[\n",
      "    {\"company\":\"Example Company 1\",\n",
      "    \"role\":\"Example Role 1\",\n",
      "    \"start_date\":\"mm/yyyy\",\n",
      "    \"end_date\":\"mm/yyyy\"},\n",
      "    {\"company\":\"Example Company 2\",\n",
      "    \"role\":\"Example Role 2\",\"\n",
      "    start_date\":\"mm/yyyy\",\n",
      "    \"end_date\":\"mm/yyyy\"}\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "This is the resume text:\n",
      " Surya Muthukaruppaiah\n",
      " Email: suryamuthukaruppaiah@gmail.com\n",
      " Mobile: +91 7397325680\n",
      "\n",
      " OBJECTIVE\n",
      "\n",
      " To be part of a dynamic and progressive organization, that ofers challenging\n",
      " working environment where one can utilize experience and competencies in a positive\n",
      " direction and contribute towards overall objectives.\n",
      "\n",
      " PROFESSIONAL EXPERIENCE\n",
      " Company Role Period Area of Work\n",
      "\n",
      "Infosys Technology, Functional and Automation\n",
      " Technology May 2021 –\n",
      "Bangalore(Direct Testing,Selenium, Javascript,C#,API\n",
      " Analyst Jan 6 2023\n",
      "Payroll) calls testing, JIRA,Agile methodology\n",
      "\n",
      "\n",
      "BNY Mellon Technology Senior Functional Testing,C#, SQL,SSIS ,\n",
      " July 2014 to\n",
      "India Pvt. Ltd , Chennai Application JIRA, Agile ,Waterfall, Entire SDLC &\n",
      " Apr 2018\n",
      "(Direct Payroll) Tester STLC\n",
      "\n",
      "\n",
      "\n",
      " EXPERIENCE SUMMARY\n",
      "\n",
      "\n",
      " Quality focused automation tester with 4.8 years of qualitative experience in\n",
      " Automation and Manual Testing of API, Database and web applications testing.\n",
      " Possess great knowledge on Investment Banking and Insurance Domains.\n",
      " Automated manual test cases using Selenium webdriver to improve the\n",
      " efficiency of the testing.\n",
      " Expertise in C# programming, OOPS concepts, SQL RDMS knowledge,SSIS and\n",
      " Functional testing.\n",
      " Created unique test scripts, test plans & processes that reduced redundancy\n",
      " while ensuring predictable outcomes.\n",
      " Participated in regular QA Team meetings and discussions.\n",
      " Experience in API Testing through Postman.\n",
      " Verified API calls using Postman.\n",
      " Involved in Test case Review, Project Estimation, and Monthly Meeting\n",
      " Presentation.\n",
      " Possess expertise in functional testing and manual testing. Ran 300+ Regression\n",
      " tests on the selenium framework in avoiding major release issues by identifying\n",
      " major defects early on.\n",
      " Spearheaded the standardization of Test Scripts,Test Plan & Daily Status Reports\n",
      " documents optimizing the processes and increasing productivity by 20%\n",
      ", Implemented Predictive analytics and regression analysis techniques for\n",
      " quantifying durability.\n",
      " Created efective UAT plans and scripts meeting the acceptance criteria\n",
      " Carried out an automation sanity test that preceded every deployment dropping\n",
      " the testing time by almost 40%\n",
      " Strong skills in Analyzing Requirements, Test case development and execution,\n",
      " Defect reporting, Defect management and Status reporting to senior\n",
      " management.\n",
      " Knowledge in Software Testing Life Cycle (STLC) AND Defect Life Cycle process.\n",
      " Attained exposure to entire Software Development Life Cycle including\n",
      " requirements gathering, analysis, design, development, testing and\n",
      " implementation.\n",
      "\n",
      "TECHNICAL COMPETENCIES\n",
      " Languages C#\n",
      " Databases SQL Server,Oracle\n",
      " Web Technologies ASP.Net,Selenium webdriver,BDD Framework\n",
      " ETL & Reporting\n",
      " SSIS\n",
      " Tools\n",
      " Mark-up Languages HTML, XML,Javascript\n",
      "\n",
      "PROJECT EXPERIENCE\n",
      "\n",
      "\n",
      "1.Title : Insurance Company Automation in US\n",
      "Company : Infosys Technology India Pvt Ltd\n",
      "Languages, Tools & Tech : GITHub Documentation, Selenium, C# ,SQL server,\n",
      " JIRA ,Agile\n",
      "Role : Technology Analyst\n",
      "Period : May 2021 – Jan 6 2023\n",
      "\n",
      "\n",
      "Roles & Responsibility:\n",
      " Prepared and Executed Test Cases as per System Requirements.\n",
      " Developed Sound Knowledge on Insurance and Banking Domain\n",
      " Developed the Automation Test Scripts using C# selenium\n",
      " Tested Entire web application end to end both backend and Frontend.\n",
      " Expertise in C# programming, OOPS concepts, SQL RDMS knowledge, SSIS and\n",
      " Functional testing.\n",
      " Performed functional, compatibility testing on diferent browsers like Firefox &\n",
      " Chrome.\n",
      " Performed automation testing using Visual Studio\n",
      " Participated in identifying the Test scenarios and designing the Test cases.\n",
      ", Performed various black box testing Methodologies Like functional testing and\n",
      " Regression Testing.\n",
      " Defect Tracking and Reporting.\n",
      " Executing the Smoke suite after release of every build and sharing the results\n",
      " with the client.\n",
      " Reviewing the scripts which are developed by the Team Members.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.Title :Performance Analytics\n",
      "Client :Bank of New York Mellon\n",
      "Company : Bank of New York Mellon India Technology Ltd\n",
      "Languages, Tools & Tech : ASP.Net, C#,SSIS, SQL, Oracle Testing, Web\n",
      " Application Testing, Agile and Waterfall methodology\n",
      "Role : Senior Application Tester\n",
      "Period : July 2014 – Apr 2018\n",
      "\n",
      "Roles & Responsibility:\n",
      " Involved in Requirements study and understanding the functionality of\n",
      " Investment Banking completely.\n",
      " Involved in API testing using Postman.\n",
      " Involved in Regression Testing of Web application end to end including UI,DB\n",
      " and Webservices.\n",
      " Expertise in C# programming, OOPS concepts, SQL RDMS knowledge,SSIS\n",
      " and Functional testing.\n",
      " Analysing the requirement based on project meeting and demos.\n",
      " Verifying the functionality Re-testing, regression testing.\n",
      " Performed smoke testing Re-testing, regression testing. Test various kind of\n",
      " work fow.\n",
      " Reporting, Analysing and Tracking the defects\n",
      " Executed the test cases to validate the functionality and User interface of the\n",
      " system.\n",
      " Defect reporting and Tracking on Jira. Involved Peer Reviews and Test cases\n",
      " review.\n",
      " Sending weekly status report and attending the weekly status calls.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "EDUCATIONAL CREDENTIALS\n",
      " CLASS/ BOARD OF YEAR OF MARKS%\n",
      " NAME OF THE INSTITUTION\n",
      " COURSE STUDY PASSING /CGPA\n",
      ", B.E.\n",
      "(ELECTRICAL BANNARI AMMAN INSTTITUE 8.60\n",
      " ANNA\n",
      " & OF TECHNOLOGY, 2014 First class with\n",
      " UNIVERSITY\n",
      "ELECTRONIC SATHYAMANGALAM Distinction\n",
      " S ENGG.)\n",
      " VIVEKANANDA MATRIC. HR. 95.0%\n",
      " Xll STATE BOARD 2010\n",
      " SEC. SCHOOL, PALANI. FIRST CLASS\n",
      "\n",
      "\n",
      " VIVEKANANDA MATRIC. HR. MATRICULATIO 94.8%\n",
      " X 2008\n",
      " SEC. SCHOOL, PALANI. N FIRST CLASS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PERSONAL PROFILE\n",
      " Date of Birth 13-Sep-1992\n",
      " Nationality Indian\n",
      " Language Proficiency English, Tamil, Hindi\n",
      " Marital Status Married\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " I hereby confirm that the information given above is true to my knowledge\n",
      "Date:\n",
      "\n",
      "Place:\n",
      "(Surya Muthukaruppaiah)\n",
      "\n",
      "This is the output in the required_format:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"company\": \"Infosys Technology India Pvt Ltd\",\n",
      "        \"role\": \"Technology Analyst\",\n",
      "        \"start_date\": \"May 2021\",\n",
      "        \"end_date\": \"Jan 6 2023\"\n",
      "    },\n",
      "    {\n",
      "        \"company\": \"Bank of New York Mellon India Technology Ltd\",\n",
      "        \"role\": \"Senior Application Tester\",\n",
      "        \"start_date\": \"July 2014\",\n",
      "        \"end_date\": \"Apr 2018\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt,return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.04703666202518836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "    \n",
    "    # peft_config = LoraConfig(\n",
    "    #     task_type=TaskType.CAUSAL_LM,\n",
    "    #     inference_mode=False,\n",
    "    #     r=8,\n",
    "    #     lora_alpha=32,\n",
    "    #     lora_dropout=0.05,\n",
    "    #     target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    # )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 5: Define an optional profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/mistral-output\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 5,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': True,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  4 13:01:59 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   31C    P0              59W / 300W |   6258MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      6218      C   ...oedge/llama-recipes/env/bin/python3     6250MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 13:02:01.505418: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-04 13:02:01.551593: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 13:02:02.231889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 1:12:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.747600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.727700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.749400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.470500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.593900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.443500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.561900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.573500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.511100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.538100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.586700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.544600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.508700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.472300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.522800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.510800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.473300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.648800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>1.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>1.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>1.538700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.443600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  4 15:15:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   31C    P0              60W / 300W |  15624MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      6218      C   ...oedge/llama-recipes/env/bin/python3    15616MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 7:\n",
    "Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.save_pretrained('tmp/llama2/work-details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 8:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../custom_data/iimjobs_eval_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "eval_df = pd.read_csv('../custom_data/iimjobs_eval_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume\n",
      "VIJAY KUMAR\n",
      "Add. V.P.O. Khoh\n",
      "Jhunjhunu, Rajasthan (333053)\n",
      "Contact no-917357654164 /9650260083\n",
      "Email: - Vijaysain505@gmail.com\n",
      "Career Objective:-\n",
      " To work with the best Industry where I can utilize my skills, potential and\n",
      "functional expertise to the maximum and to add value to the organization in\n",
      "turn to elevate my personal capabilities.\n",
      "Work Experience:-\n",
      " Total 4 Years above work experience in Warehouse Inbound -\n",
      "out bound &Logistics.\n",
      " Currently working in VARUNA INTERGRATED L OGISTICS PVT\n",
      "LTD (GGN.)  as a CONTROL-TOWER EXECUTIVE from Dec -2018 to\n",
      "till now.\n",
      " 1 years worked experience with M/s. LAVA mobile company Noida as a\n",
      "Data entry Operator & associate from nov -2017 to dec-2018..\n",
      "Key Responsibilities & Performance Indicators:-\n",
      "CONTROL-TOWER executive VARUNA INTERGRATED L OGISTICS PVT LTD\n",
      "Dec-2018 to till nov.\n",
      " Prepare & Maintain Daily MIS Report.\n",
      " Prepare the other Reports review reports for Management.\n",
      " Responsible to make coordination with warehouse team.\n",
      " Handling Transport activity including E way bill, LR update, vehicle\n",
      "tracking, pod upload and trip closed.\n",
      " Responsible for Invoice matching in the system and to create invoice\n",
      "according to inbound material.\n",
      " Follows up with warehouse team to dispatch the shipments as per TAT.\n",
      " Working Control-Tower department for Secondary Transportation.\n",
      " Escalate DEPS (Damage Excess Pilferage Shortage) cases for all\n",
      "projects (Tares, Battery, and VMM).\n",
      " Managing transportation for dispatch time to time & loading\n",
      "activities.\n",
      " Processing the Orders as per FIFO Basis.\n",
      " Follow up with customer (Regarding order).\n",
      " Vehicle controlling and planning vehicle on time.\n",
      "\",\" Escalate delivery & unloading challenge with client.\n",
      " Filling of documents systematically as per 5S.\n",
      " We have Working Inward dispatch and billing team Transport\n",
      "Tracking for Vehicles\n",
      "1 year LAVA mobile company Noida as a Data entry Operator &\n",
      "associate :-\n",
      " Receiving material from diferent vendors, physical quantity\n",
      "verification, and entry of material as per the receipt quantity.\n",
      " Looking after GRN related to DC Inward and vendor follow up daily\n",
      "basis Responsible for all incoming shipments.\n",
      " Responsible to coordinate with diferent Courier parteners & making\n",
      "coordinate with the ground team for Diferent type Dispatch.\n",
      " Track courier packages. Responsible to take daily reports from the courier\n",
      "partners to manage the transportation facility smoothly.\n",
      " Put away of goods in respective location\n",
      " Making the Pick list for the order to be processed list.\n",
      " Making Challan Copy after Picking done (Qty Change & Batch\n",
      "Change).\n",
      " Physically and Quality Checking the items as per Challan Copy.\n",
      " Generating the Final Invoices after physically and Quality Checking\n",
      "done.\n",
      " Dispatch the material from warehouse to retail & distributors.\n",
      " Inventory management of warehouse stock.\n",
      " Daily Data entry process regarding stock out to diferent retails &\n",
      "distributors.\n",
      " Making Inventory Transactions in SAP.\n",
      " Maintain the 5S in Outward Dispatches.\n",
      " Able to handle the Multitask Operations of Dispatches.\n",
      " Very good knowledge of SAP.\n",
      "Educational Qualifcation\n",
      " B.A. Honors from Rajasthan University in 2013.\n",
      " Senior Secondary from RBSE in Year 2010.\n",
      " Secondary from RBSE in Year 2008.\n",
      "Computer Knowledge\n",
      "Having knowledge of MS Ofce & internet surfing\n",
      "Excel level skills , VLOOKUP, MS word , Pivot Table etc.\n",
      "Knowledge of E-mailing system and Internet.\n",
      "SAP - Confident to any other package ,Oracle retail Invoice\n",
      "matching ,RWMS & ,WAY BILL,ERP .\n",
      "\",\"Personal Profciency:\n",
      "5S, 3R, FIFO,4M & SOP.\n",
      "Personal Details\n",
      "Fathers Name- Mr. Sharwan Kumar Sain\n",
      "Date of Birth- 02.06. 1993\n",
      "Address - AS Above\n",
      "Marital Status - Unmarried\n",
      "Sex - Male\n",
      "Strengths- Self Motivated, Hard worker ,Team Worker.\n",
      "Hobbies- Traveling, playing Cricket.\n",
      "Language know- Hindi & English\n",
      "Nationality - Indian\n",
      "Religion - Hindu\n",
      "I certified that the information stated above is true and correct to the best of\n",
      "my knowledge.\n",
      "Date.\n",
      "Placegurgaon Vijay kumar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import html \n",
    "rt = eval_df.sample()['resume'].values[0]\n",
    "rt = html.unescape(rt)\n",
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_prompt = f'''\n",
    "You are an accurate agent working for a job platform. You will be given the raw \n",
    "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
    "user from the resume. Please provide the data in a concise JSON format. The JSON should include a \n",
    "\"work_experience\" key with an array of objects. Each object represents a job and should contain keys for \n",
    "\"company\", \"role\", \"start_date\", \"end_date\". Dates should be in \"mm/yyyy\" format. \n",
    "Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces.\n",
    "\n",
    "Please follow this structure closely and keep the response within the token limit.\" \\n{{query_format}}\\n\n",
    "\n",
    "This is the resume text:\\n{{resume_text}}\\n\n",
    "This is the output in the required_format\\n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_format = '''\n",
    "[\n",
    "    {\"company\":\"Example Company 1\",\n",
    "    \"role\":\"Example Role 1\",\n",
    "    \"start_date\":\"mm/yyyy\",\n",
    "    \"end_date\":\"mm/yyyy\",\n",
    "    \"description\":\"Example Description 1\"},\n",
    "    {\"company\":\"Example Company 2\",\n",
    "    \"role\":\"Example Role 2\",\"\n",
    "    start_date\":\"mm/yyyy\",\n",
    "    \"end_date\":\"mm/yyyy\",\n",
    "    \"description\":\"Example Description 2\"}\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "eval_prompt = work_prompt.format(\n",
    "            query_format=work_format,\n",
    "            resume_text=rt)\n",
    "\n",
    "sample_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1431"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken :7.73288369178772\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    full_document = tokenizer.decode(model.generate(**sample_input, max_new_tokens=1000)[0], skip_special_tokens=True)\n",
    "print(f'Time taken :{time.time()-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an accurate agent working for a job platform. You will be given the raw \n",
      "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
      "user from the resume. Please provide the data in a concise JSON format. The JSON should include a \n",
      "\"work_experience\" key with an array of objects. Each object represents a job and should contain keys for \n",
      "\"company\", \"role\", \"start_date\", \"end_date\". Dates should be in \"mm/yyyy\" format. \n",
      "Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces.\n",
      "\n",
      "Please follow this structure closely and keep the response within the token limit.\" \n",
      "\n",
      "[\n",
      "    {\"company\":\"Example Company 1\",\n",
      "    \"role\":\"Example Role 1\",\n",
      "    \"start_date\":\"mm/yyyy\",\n",
      "    \"end_date\":\"mm/yyyy\",\n",
      "    \"description\":\"Example Description 1\"},\n",
      "    {\"company\":\"Example Company 2\",\n",
      "    \"role\":\"Example Role 2\",\"\n",
      "    start_date\":\"mm/yyyy\",\n",
      "    \"end_date\":\"mm/yyyy\",\n",
      "    \"description\":\"Example Description 2\"}\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "This is the resume text:\n",
      "Resume\n",
      "VIJAY KUMAR\n",
      "Add. V.P.O. Khoh\n",
      "Jhunjhunu, Rajasthan (333053)\n",
      "Contact no-917357654164 /9650260083\n",
      "Email: - Vijaysain505@gmail.com\n",
      "Career Objective:-\n",
      " To work with the best Industry where I can utilize my skills, potential and\n",
      "functional expertise to the maximum and to add value to the organization in\n",
      "turn to elevate my personal capabilities.\n",
      "Work Experience:-\n",
      " Total 4 Years above work experience in Warehouse Inbound -\n",
      "out bound &Logistics.\n",
      " Currently working in VARUNA INTERGRATED L OGISTICS PVT\n",
      "LTD (GGN.)  as a CONTROL-TOWER EXECUTIVE from Dec -2018 to\n",
      "till now.\n",
      " 1 years worked experience with M/s. LAVA mobile company Noida as a\n",
      "Data entry Operator & associate from nov -2017 to dec-2018..\n",
      "Key Responsibilities & Performance Indicators:-\n",
      "CONTROL-TOWER executive VARUNA INTERGRATED L OGISTICS PVT LTD\n",
      "Dec-2018 to till nov.\n",
      " Prepare & Maintain Daily MIS Report.\n",
      " Prepare the other Reports review reports for Management.\n",
      " Responsible to make coordination with warehouse team.\n",
      " Handling Transport activity including E way bill, LR update, vehicle\n",
      "tracking, pod upload and trip closed.\n",
      " Responsible for Invoice matching in the system and to create invoice\n",
      "according to inbound material.\n",
      " Follows up with warehouse team to dispatch the shipments as per TAT.\n",
      " Working Control-Tower department for Secondary Transportation.\n",
      " Escalate DEPS (Damage Excess Pilferage Shortage) cases for all\n",
      "projects (Tares, Battery, and VMM).\n",
      " Managing transportation for dispatch time to time & loading\n",
      "activities.\n",
      " Processing the Orders as per FIFO Basis.\n",
      " Follow up with customer (Regarding order).\n",
      " Vehicle controlling and planning vehicle on time.\n",
      "\",\" Escalate delivery & unloading challenge with client.\n",
      " Filling of documents systematically as per 5S.\n",
      " We have Working Inward dispatch and billing team Transport\n",
      "Tracking for Vehicles\n",
      "1 year LAVA mobile company Noida as a Data entry Operator &\n",
      "associate :-\n",
      " Receiving material from diferent vendors, physical quantity\n",
      "verification, and entry of material as per the receipt quantity.\n",
      " Looking after GRN related to DC Inward and vendor follow up daily\n",
      "basis Responsible for all incoming shipments.\n",
      " Responsible to coordinate with diferent Courier parteners & making\n",
      "coordinate with the ground team for Diferent type Dispatch.\n",
      " Track courier packages. Responsible to take daily reports from the courier\n",
      "partners to manage the transportation facility smoothly.\n",
      " Put away of goods in respective location\n",
      " Making the Pick list for the order to be processed list.\n",
      " Making Challan Copy after Picking done (Qty Change & Batch\n",
      "Change).\n",
      " Physically and Quality Checking the items as per Challan Copy.\n",
      " Generating the Final Invoices after physically and Quality Checking\n",
      "done.\n",
      " Dispatch the material from warehouse to retail & distributors.\n",
      " Inventory management of warehouse stock.\n",
      " Daily Data entry process regarding stock out to diferent retails &\n",
      "distributors.\n",
      " Making Inventory Transactions in SAP.\n",
      " Maintain the 5S in Outward Dispatches.\n",
      " Able to handle the Multitask Operations of Dispatches.\n",
      " Very good knowledge of SAP.\n",
      "Educational Qualifcation\n",
      " B.A. Honors from Rajasthan University in 2013.\n",
      " Senior Secondary from RBSE in Year 2010.\n",
      " Secondary from RBSE in Year 2008.\n",
      "Computer Knowledge\n",
      "Having knowledge of MS Ofce & internet surfing\n",
      "Excel level skills , VLOOKUP, MS word , Pivot Table etc.\n",
      "Knowledge of E-mailing system and Internet.\n",
      "SAP - Confident to any other package ,Oracle retail Invoice\n",
      "matching ,RWMS & ,WAY BILL,ERP .\n",
      "\",\"Personal Profciency:\n",
      "5S, 3R, FIFO,4M & SOP.\n",
      "Personal Details\n",
      "Fathers Name- Mr. Sharwan Kumar Sain\n",
      "Date of Birth- 02.06. 1993\n",
      "Address - AS Above\n",
      "Marital Status - Unmarried\n",
      "Sex - Male\n",
      "Strengths- Self Motivated, Hard worker ,Team Worker.\n",
      "Hobbies- Traveling, playing Cricket.\n",
      "Language know- Hindi & English\n",
      "Nationality - Indian\n",
      "Religion - Hindu\n",
      "I certified that the information stated above is true and correct to the best of\n",
      "my knowledge.\n",
      "Date.\n",
      "Placegurgaon Vijay kumar\n",
      "\n",
      "\n",
      "This is the output in the required_format\n",
      "\n",
      "[{'company': 'VARUNA INTERGRATED L OGISTICS PVT LTD', 'role': 'CONTROL-TOWER EXECUTIVE', 'start_date': '12/2018', 'end_date': 'present'}, {'company': 'LAVA mobile company Noida', 'role': 'Data entry Operator & associate', 'start_date': '11/2017', 'end_date': '12/2018'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_str = full_document.replace(eval_prompt,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'company': 'VARUNA INTERGRATED L OGISTICS PVT LTD',\n",
       "  'role': 'CONTROL-TOWER EXECUTIVE',\n",
       "  'start_date': '12/2018',\n",
       "  'end_date': 'present'},\n",
       " {'company': 'LAVA mobile company Noida',\n",
       "  'role': 'Data entry Operator & associate',\n",
       "  'start_date': '11/2017',\n",
       "  'end_date': '12/2018'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed54eeaa71c457caf5369d223f18563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/13.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lakshay/mistral-work-peft/commit/d60925978db009605be10bc5126006c66fd80905', commit_message='Upload model', commit_description='', oid='d60925978db009605be10bc5126006c66fd80905', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('lakshay/mistral-work-peft',token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
