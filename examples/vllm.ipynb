{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e59bd2-d0a9-43c0-9961-f06f4ed2dff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc4494f-67c1-4062-9575-809aa2f6b02c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1393e29f-c11b-4972-ad30-af944c451182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45bfa0b7-d161-4b81-a788-ee0f4c9b147a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('custom_data/model_eval_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54e02c-2584-46de-89fa-4dd9ddd998df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e62e081-fdfb-4ff5-be85-d66cb713d5db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neeraj Mourya Professional Summary\n",
      "Software Engineer Analytical and Innovative Software Engineer with 1.5 years of experience in\n",
      " creating robust and efficient software solutions and deploying them on Linux\n",
      "Contact and Windows servers. I'm dedicated to creating clean, scalable, and user-\n",
      " centric solutions. Seeking opportunities to contribute my technical skills and\n",
      "Phone passion for software development to create cutting-edge solutions.\n",
      "+91 9673473826\n",
      "\n",
      "Email\n",
      " Experience [1.5 yrs]\n",
      "neerajmourya9165@gmail.com\n",
      " September 2022 - Present\n",
      "Address Nexgensis Technologies Private Limited l Viman Nagar, Pune\n",
      "Kharadi,Pune-411014 Software Engineer\n",
      " Working on the backend part of the applications in Django using Python and\n",
      "Education maintaining databases.\n",
      " Involved in direct client interaction and in the software development life\n",
      "BE - 2021 74.48% cycle(SDLC) of tracking the requirements gathering, analysis, detailed design,\n",
      "Computer Science and development.\n",
      "Savitribai Phule University Creating automated alerts, notifications, and APIs as per requirement.\n",
      " Working closely with cross-functional teams(DevOps, Frontend,QA,Integration)\n",
      " to improve the user interface and integrate the API&rsquo;s.\n",
      "HSC - 2016 62.62%\n",
      " Writing scripts for data transformation and data generation using numpy and\n",
      "S V Union Jr. College\n",
      " pandas libraries.\n",
      " Handling day-to-day issues and fine-tuning the applications for enhanced\n",
      "SSC - 2014 71.71% performance.\n",
      "Stella Maris High School\n",
      " Projects\n",
      " 1-Raams(Rental and Asset Management Service)\n",
      "Skills Purpose: Providing a web and mobile application for PG property owners to\n",
      " streamline property management.\n",
      " Implemented the MVT Architecture for developing the web application using Django\n",
      " Framework(Backend), ReactJS (web app), and Flutter(Mobile app).\n",
      " C, C++\n",
      " Utilized Django rest Framework for creating REST APIs and collaborated with other\n",
      " Python\n",
      " teams to integrate these API&rsquo;s.\n",
      " Django Framework and DRF\n",
      " Used SQL and Django ORM queries for interacting with the database and\n",
      " SQL, POSTGRESQL\n",
      " performing CRUD operations and providing various analytics.\n",
      " HTML,CSS\n",
      " GCP Dialogflow 2-Manufacturing Industry Plant Analysis\n",
      " GIT (Github)\n",
      " Purpose: Enhancing efficiency and reducing downtime by improving production\n",
      " AWS EC2,S3\n",
      " rates in the plant.\n",
      " Linux\n",
      " Implemented data visualization processes by storing data in databases and\n",
      " Postman\n",
      " creating REST APIs for visualization and comparison of this stored data.\n",
      "Personal Traits Designed and implemented a solution that monitors and analyzes production lines\n",
      " in real-time.\n",
      " Implemented a feature that detects and alerts relevant personnel in case of\n",
      " breakdowns.\n",
      " Continuous Learner Achieved a 20% increase in production efficiency through streamlined processes,\n",
      " Collaborative team player leading to cost savings and higher productivity.\n",
      " Strong problem solving skills Decreased breakdown time by 20-30% through proactive maintenance and\n",
      " Agile Mindset improved troubleshooting techniques.\n",
      "\n",
      " September 2020 - March 2021\n",
      "Language\n",
      " Cloudage l Kondwa, Pune\n",
      " Intern\n",
      " English Implemented a virtual assistant solution for the company that will be available\n",
      " Hindi 24*7 for customer queries resolution using GCP DialogFlow.\n",
      " Marathi Integrated the virtual assistant with the company's website, Google Assistant, and\n",
      " other platforms like Telegram, Whatsapp, etc.\n",
      " Creating Python scripts for data transformation and data cleaning in order to feed\n",
      " the data to the model.\n"
     ]
    }
   ],
   "source": [
    "rt = eval_df.sample()['resume'].values[0]\n",
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f1df32-f2a2-4efd-b427-0e49389925a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_format = '''{\n",
    "    'work_experience': [{'company': 'company Name 1',\n",
    "                         'role': 'job designation 1',\n",
    "                         'start_date': 'mm/yyyy',\n",
    "                         'end_date': 'mm/yyyy',\n",
    "                         'description': 'complete Job description taken from resume'},\n",
    "                        {'company': 'company name 2',\n",
    "                         'role': 'job designation 2',\n",
    "                         'start_date': mm/yyyy',\n",
    "                         'end_date': 'mm/yyyy',\n",
    "                         'description': 'complete Job description taken from resume'}]\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251cfd05-ddd7-416f-92ec-1649da76dd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_prompt = f'''\n",
    "You are a helpful language model working for a job platform. You will be given the raw \n",
    " unstructured text of a user's resume, and the task is to extract the work experience of the \n",
    " user from the raw text in the following format: \\n{{work_format}}\\n\n",
    "\n",
    " This is the resume text:\\n{{resume_text}}\\n\n",
    " This is the output in the required format:\\n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f402be7-4546-4784-a3db-e4ab26ee9344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_prompt = eval_prompt.format(\n",
    "            work_format=work_format,\n",
    "            resume_text=rt)\n",
    "\n",
    "sample_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f18a05-016b-4249-8130-6d9661e2ac46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_input['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd033a6d-dc6b-4e23-8f65-3ee97785be45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a0cac-a249-4bbb-b022-6db45c0b2033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc993f323d9d4c66817590cd31be99ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-02 11:03:06 llm_engine.py:73] Initializing an LLM engine with config: model='lakshay/work-model', tokenizer='lakshay/work-model', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ec656b753445e6a8aa33f02dd81989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd07658bf82483686117a85422c67f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01230bf559b14a7c8d44badfeb4debdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8f3b21f41b45e6bca9a210b9f47505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e7a99c68ac44d6a094f54246dfc3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676c6cdf5f654adcb6f7e6f3f1319f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/2.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5984dec106ca4d2a95c084b817be60bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac19ac6e54e487488311a1c4f4f9690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512028ea68374291aaf57e3fd18bdb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=4096)\n",
    "\n",
    "llm = LLM(model=\"lakshay/work-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3dbb839-2078-4466-a734-ff0797386277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.35s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = llm.generate(eval_prompt, sampling_params)\n",
    "\n",
    "# with open('custom_data/vllm_test.txt','w') as f:\n",
    "#     f.write(str(outputs))\n",
    "    \n",
    "generated_text = outputs[0].outputs[0].text\n",
    "# for output in outputs:\n",
    "#     # prompt = output.prompt\n",
    "#     generated_text = output.outputs[0].text\n",
    "#     print(f\"Prompt: {eval_prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f4c55-4ef1-4d11-9add-230aae623868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d0cc545-896a-416a-ae84-fb2311555053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5016fdd-ad9c-4db9-8bf4-4074e9991104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7685a534-19bc-4685-99ce-f3c599bb6baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'company': 'AAVAS Financiers limited', 'role': 'Senior Quality Analyst', 'start_date': '2022', 'end_date': '2023', 'description': 'Senior Quality Analyst. Proficient in developing, maintaining and executing test cases for different testing methodologies. Good knowledge of functional testing in Testing Approach: Salesforce, API and Mobile Testing. Involved in Test Case Design and Review, Test execution, and Collecting Test Data. Effective Involvement in Risk Analysis meetings, Sprint review meetings, Project Closure/Post Project Review meetings, Daily Standup Meeting. Expertise in Functional Testing, Stanup Meeting, Integration Testing System, Testing Regression Testing, Api Testing using Postman and Load Testing using Jmeter. UAT Testing, Smoke Testing, Browser Compatibility Testing. Worked on Agile/Scrum environment.'}, {'company': 'RG Infotech', 'role': 'Senior Quality Analyst', 'start_date': '2021', 'end_date': '2022', 'description': 'Good knowledge of RG Infotech. Good in identifying Test scenarios & writing Test cases by applying Test Case Design Techniques. Good knowledge in Banking and Finance (Loan Modules). Involved in Test Case Design and Review, Test execution, and Collecting Test Data. Effective Involvement in Risk Analysis meetings, defect review meetings, Project Closure/Post Project Review meetings. Api Testing using Postman and Load Testing using Jmeter. UAT Testing, Smoke Testing, Browser Compatibility Testing.'}, {'company': 'ALEA IT SOLUTIONS', 'role': 'Information Quality Analyst', 'start_date': '2019', 'end_date': '2021', 'description': 'Analyzing and deciding the project requirements. To test all the forms and check the functionality while filling these forms. Executing test cases. Reporting and tracking defects. Preparation of the Root Cause Analysis reports, in an event of site isolation or major service disruption. Conference calls with client to discuss project improvement plan.'}, {'company': 'Precise Automation And Robotics', 'role': 'Quality Analyst', 'start_date': '2018', 'end_date': '2019', 'description': 'Analyzing and deciding the project requirements. To test all the forms and also check the functionality while filling these forms. Executing test cases. Reporting and tracking defects in Trello Tool. Preparation of the Root Cause Analysis reports, in an event of site isolation.'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dfc6c7e-f3cc-4ac4-b884-47dae76c48cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e42c8386-a4ca-4978-b59f-1439f91f8072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'company': 'AAVAS Financiers limited',\n",
       "  'role': 'Senior Quality Analyst',\n",
       "  'start_date': '2022',\n",
       "  'end_date': '2023',\n",
       "  'description': 'Senior Quality Analyst. Proficient in developing, maintaining and executing test cases for different testing methodologies. Good knowledge of functional testing in Testing Approach: Salesforce, API and Mobile Testing. Involved in Test Case Design and Review, Test execution, and Collecting Test Data. Effective Involvement in Risk Analysis meetings, Sprint review meetings, Project Closure/Post Project Review meetings, Daily Standup Meeting. Expertise in Functional Testing, Stanup Meeting, Integration Testing System, Testing Regression Testing, Api Testing using Postman and Load Testing using Jmeter. UAT Testing, Smoke Testing, Browser Compatibility Testing. Worked on Agile/Scrum environment.'},\n",
       " {'company': 'RG Infotech',\n",
       "  'role': 'Senior Quality Analyst',\n",
       "  'start_date': '2021',\n",
       "  'end_date': '2022',\n",
       "  'description': 'Good knowledge of RG Infotech. Good in identifying Test scenarios & writing Test cases by applying Test Case Design Techniques. Good knowledge in Banking and Finance (Loan Modules). Involved in Test Case Design and Review, Test execution, and Collecting Test Data. Effective Involvement in Risk Analysis meetings, defect review meetings, Project Closure/Post Project Review meetings. Api Testing using Postman and Load Testing using Jmeter. UAT Testing, Smoke Testing, Browser Compatibility Testing.'},\n",
       " {'company': 'ALEA IT SOLUTIONS',\n",
       "  'role': 'Information Quality Analyst',\n",
       "  'start_date': '2019',\n",
       "  'end_date': '2021',\n",
       "  'description': 'Analyzing and deciding the project requirements. To test all the forms and check the functionality while filling these forms. Executing test cases. Reporting and tracking defects. Preparation of the Root Cause Analysis reports, in an event of site isolation or major service disruption. Conference calls with client to discuss project improvement plan.'},\n",
       " {'company': 'Precise Automation And Robotics',\n",
       "  'role': 'Quality Analyst',\n",
       "  'start_date': '2018',\n",
       "  'end_date': '2019',\n",
       "  'description': 'Analyzing and deciding the project requirements. To test all the forms and also check the functionality while filling these forms. Executing test cases. Reporting and tracking defects in Trello Tool. Preparation of the Root Cause Analysis reports, in an event of site isolation.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c15f5d-0616-4b88-9994-7a0be73907d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
