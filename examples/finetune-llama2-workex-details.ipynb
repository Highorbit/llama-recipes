{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets tqdm vllm\n",
    "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weighjts_to_hf.py')\"`\n",
    "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llama-recipes/src/llama_recipes/utils/dataset_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Experience (Shell Details)\n",
    "This notebook is meant to fine-tune Llama2 on Work Experience Details (all work experience minus the job description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point model_id to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train_data = load_from_disk(\"custom_data/llama2/work_details.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 12 12:15:01 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   29C    P0              58W / 300W |      4MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Important \n",
    "\n",
    "It is important to consider here which model we're using to parse the resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688f3b24ac4b47ba8dcad4ecd5f8852f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16, token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 12 12:16:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   32C    P0              61W / 300W |   7516MiB / 23028MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3462      C   ...oedge/llama-recipes/env/bin/python3     7504MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/SageMaker/llama_root/src')\n",
    "sys.path.append('../llama-recipes/src/llama_recipes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_prompt = f'''\n",
    "You are an accurate agent working for a job platform. You will be given the raw \n",
    "unstructured text of a user's resume, and the task is to extract specific details about the work experience of the \n",
    "user from the resume. The JSON should include a \"work_experience\" key with an array of objects. \n",
    "Each object represents a job and should contain keys for \"company\", \"role\", \"start_date\", \"end_date\".\n",
    "Dates should be in \"mm/yyyy\" format. Please provide the data in a concise JSON format\n",
    "Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces. Here is an example structure::\n",
    "\n",
    "Please follow this structure closely and keep the response within the token limit.\" \\n{{query_format}}\\n\n",
    "\n",
    "This is the resume text:\\n{{resume_text}}\\n\n",
    "This is the output in the required_format:\\n\n",
    "'''\n",
    "\n",
    "work_format = '''\n",
    "[\n",
    "    {\"company\":\"Example Company 1\",\n",
    "    \"role\":\"Example Role 1\",\n",
    "    \"start_date\":\"mm/yyyy\",\n",
    "    \"end_date\":\"mm/yyyy\"},\n",
    "    {\"company\":\"Example Company 2\",\n",
    "    \"role\":\"Example Role 2\",\"\n",
    "    start_date\":\"mm/yyyy\",\n",
    "    \"end_date\":\"mm/yyyy\"}\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "df = pd.read_csv('custom_data/latest_work_exp_28dec.csv')\n",
    "base_model_test_rt = html.unescape(df['resume'].sample().values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = work_prompt.format(resume_text=base_model_test_rt,\n",
    "                          query_format=work_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an accurate agent working for a job platform. You will be given the raw \n",
      "unstructured text of a user's resume, and the task is to extract specific details about the work experience of the \n",
      "user from the resume. The JSON should include a \"work_experience\" key with an array of objects. \n",
      "Each object represents a job and should contain keys for \"company\", \"role\", \"start_date\", \"end_date\".\n",
      "Dates should be in \"mm/yyyy\" format. Please provide the data in a concise JSON format\n",
      "Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces. Here is an example structure::\n",
      "\n",
      "Please follow this structure closely and keep the response within the token limit.\" \n",
      "\n",
      "[\n",
      "    {\"company\":\"Example Company 1\",\n",
      "    \"role\":\"Example Role 1\",\n",
      "    \"start_date\":\"mm/yyyy\",\n",
      "    \"end_date\":\"mm/yyyy\"},\n",
      "    {\"company\":\"Example Company 2\",\n",
      "    \"role\":\"Example Role 2\",\"\n",
      "    start_date\":\"mm/yyyy\",\n",
      "    \"end_date\":\"mm/yyyy\"}\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "This is the resume text:\n",
      "LAVANYA .B\n",
      "\n",
      "Mobile: +91 9346399481;\n",
      "\n",
      "Email:- bondhililavanyasingh@gmail.com.\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " Having 3.11 + years of experience in Storage Area Network working with different SAN\n",
      "technologies like DELL EMC & Veritas Backup Tool . Now seeking a position to get associated with\n",
      "a progressive organization that gives scope to apply my knowledge and skills, and to be a part of a\n",
      "team that dynamically works towards growth of the organization\n",
      "\n",
      "Technical Skills:\n",
      "\n",
      "Storage Arrays:\n",
      "\n",
      "DELL EMC VMAX, POWER MAX, UNITY.\n",
      "\n",
      "BACK UP TOOLS :-\n",
      "\n",
      "VERITAS BACKUP\n",
      "\n",
      "San Switches: Brocade switches.\n",
      "\n",
      "Storage Software/Tools:\n",
      "\n",
      "Unisphere, Symcl\n",
      "\n",
      "Operating Systems:\n",
      "\n",
      "Windows, Linux\n",
      "\n",
      "Educational Qualifications:\n",
      "\n",
      "\n",
      " • KITS college of Engineering and Technology, Btech: - 2014-2018\n",
      "\n",
      "\n",
      "\n",
      "Professional Experience:\n",
      "\n",
      " (1) Employer : ACCENTURE SOLUTIONS PVT LTD\n",
      " Period : October 2018 – February 2022.\n",
      " Role : Storage Administrator.\n",
      ", (2) Employer : TCS (TATA CONSULTANCY SERVICES)\n",
      " Period : September 2022 – April 2023.\n",
      " Support : Level 1 Technical.\n",
      " Role : Storage Administrator\n",
      "\n",
      "\n",
      "Professional Summary:\n",
      "\n",
      " Working experience on EMC Vmax, PowerMax & Unity.\n",
      " Good Working Experience on Brocade Switches\n",
      " Working experience in Unity.\n",
      " Working Experience in EMC PowerMax\n",
      " Working Experience on Linux & Windows OS platform administration.\n",
      " Independent problem-solving attitude and always willing to learn.\n",
      " Working experience on Backup Tools like Veritas Backup tool.\n",
      "\n",
      "\n",
      "Key Work Profile:\n",
      "\n",
      " Handling on ITIL Policies of Incident Management, Change Management and Problem\n",
      " Management\n",
      " strictly following the process and service level Agreements (SLA) which has been agreed\n",
      " with Client\n",
      "\n",
      "\n",
      "Symmetrix Vmax, HP3PAR, PowerMax, Unity & VERITAS Backup Responsibilities:\n",
      "\n",
      "\n",
      " • Good knowledge about EMC Powermax, Vmax & Unity Storage systems\n",
      " architecture.\n",
      " • Monitoring regular health checks for the storage arrays on daily basis (EMC Vmax,\n",
      " Unity & Powermax)\n",
      " • Attending the DR activities for Failover and Failback\n",
      " • Approach the vendor to replace any faulty device\n",
      " • Experience in administrating, maintaining and troubleshooting EMC Vmax, Unity.\n",
      " • Regular health checks for the Brocade FC Switches and pull out logs from the device\n",
      " and send to the support teams for review and action plan\n",
      " • Configuring the Switches\n",
      " • Performing Zoning activity for new server builds and update the zoning for path\n",
      " issues\n",
      " • Good Experience in Brocade Zoning.\n",
      " • Zoning for Initiators and Targets on CLI\n",
      " • Responsibility includes analyzing the space in the storage array and allocating LUN’s\n",
      " accordingly for the request from the Server Team’s.\n",
      ", • Troubleshooting errors in Storage and Switch Level.\n",
      " • Expanding the Existing LUN’s as per the Request.\n",
      " • Un mapping the LUN’s , Remove zones for decommissioned servers .\n",
      " • Uploading logs from devices and sent to the vendor.\n",
      " • Attend the CAB meetings for Change approvals.\n",
      " • Troubleshoot and resolve the Storage related issues generated in the production\n",
      " environment Worked closely with clients.\n",
      " • Investigating various Issues like drive failures, Power supply issues, Battery issues\n",
      " and connectivity issues Failed disk replacements and fixing the hardware faults in co-\n",
      " ordination with Vendor support and field engineers\n",
      " • Troubleshooting Brocade Fabrics to identify switch related issues\n",
      "\n",
      "Back Up Tools Responsibilities: -\n",
      "\n",
      " • Check the Backup failure reports of Symantic Net Backup\n",
      " • Create the Backup Policy\n",
      " • Install Backup Agent\n",
      " • Schedule the Backup Report\n",
      "\n",
      "\n",
      "\n",
      "I declare that the above information is correct and complete to the best of my knowledge and belief.\n",
      "\n",
      "\n",
      "\n",
      "Date :\n",
      "Place: Hyderabad LAVANYA.B\n",
      "\n",
      "This is the output in the required_format:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"work_experience\": [\n",
      "            {\n",
      "                \"company\": \"Accenture Solutions Pvt Ltd\",\n",
      "                \"role\": \"Storage Administrator\",\n",
      "                \"start_date\": \"October 2018\",\n",
      "                \"end_date\": \"February 2022\"\n",
      "            },\n",
      "            {\n",
      "                \"company\": \"TCS (TATA CONSULTANCY SERVICES)\",\n",
      "                \"role\": \"Support Level 1 Technical\",\n",
      "                \"start_date\": \"September 2022\",\n",
      "                \"end_date\": \"April 2023\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "\n",
      "Note: Please let me know if you need any changes in the output format or if you have any questions.\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt,return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "    \n",
    "    # peft_config = LoraConfig(\n",
    "    #     task_type=TaskType.CAUSAL_LM,\n",
    "    #     inference_mode=False,\n",
    "    #     r=8,\n",
    "    #     lora_alpha=32,\n",
    "    #     lora_dropout=0.05,\n",
    "    #     target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    # )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 5: Define an optional profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/llama-output\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 3,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 12 12:39:02 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   33C    P0              61W / 300W |  10258MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3462      C   ...oedge/llama-recipes/env/bin/python3    10246MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/ubuntu/infoedge/llama-recipes/env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='258' max='258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [258/258 1:01:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.727100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.603900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.523600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.404100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.565400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.588800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.563900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.549300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.463400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('done, on the 26th of december, the year of our lord 2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 7:\n",
    "Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('tmp/llama2/work-details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 8:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('custom_data/model_eval_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Anurag Kundra\n",
      " SOFTWARE ENGINEER 2\n",
      "https://www.linkedin.com/in/anurag-kundra-955b081b6/ | +91 9999305968 | anurag.kundra21@gmail.com\n",
      "\n",
      "Summary\n",
      "\n",
      "• Skilled and dedicated Software Development Engineer working in fast paced startup environment for\n",
      " last 2 years.\n",
      "• Experienced in developing multiple applications with Agile methodologies with a strong focus on\n",
      " following best practices like writing optimized and clean code.\n",
      "• Proficient in multiple programming languages, frameworks, and technologies including Python,\n",
      " MySQL, Django and Django REST.\n",
      "• Objective is to obtain a creative and challenging position in an organization that gives an opportunity\n",
      " for self-improvement and leadership, while contributing to the symbolic growth of the organization\n",
      " with technical, innovative, and logical skills.\n",
      "\n",
      "\n",
      "\n",
      "Skills\n",
      "\n",
      " • Django • Django Elasticsearch • Celery\n",
      " • Django REST DSL DRF • Redis\n",
      " • Elasticsearch • Python • RabbitMq\n",
      " • Django Elasticsearch • MySql • Node.js\n",
      " DSL • Postgres • Git\n",
      "\n",
      "\n",
      "\n",
      "Experience\n",
      "\n",
      "TravClan Technology India Private Limited\n",
      "Software Engineer 2 April 2023 - Present\n",
      " • Involved in designing the architecture of software systems.\n",
      " • Working closely with cross functional teams, including product managers, QA to deliver projects\n",
      " on time.\n",
      " • Mentoring junior developers and doing code reviews therefore ensuring the codebase adheres to\n",
      " coding standards, best practices, and quality guidelines.\n",
      " • Debugging and solving major production issues.\n",
      "\n",
      "\n",
      "Software Engineer 1 July 2022 - March 2023\n",
      " • Working as a Full time SDE on Django and Node Js with MySQL, Elasticsearch and Postgres\n",
      " databases.\n",
      " • Worked on optimizing the core system by reducing SQL queries and CPU time which is further\n",
      " helping other departments to use system with full eciency.\n",
      " • Building and leading a project from scratch using Elasticsearch with the help of packages like\n",
      " django-elasticsearch-dsl and django-elasticsearch-dsl-drf, Node.js and Postgres for internal\n",
      " teams to handle their workflows and work in faster way.\n",
      " • Used Celery and RabbitMq for queue management and execution of asynchronous tasks and\n",
      " nearly attained real time syncing between MySQL and elastic search with it.\n",
      ",Software Engineer Trainee Sept 2021 - June 2022\n",
      " • Majorly worked on company's core system built on Django and Django-REST Framework.\n",
      " • Built numerous REST APIs for client side.\n",
      " • Developed pending bookings feature on Django Admin so as to solve for capturing international\n",
      " package bookings which are confirmed over a due course of time by the travel agent, extensively\n",
      " utilizing all the core functionalities of Django Admin\n",
      " • Integrate Third Party Applications like SendGrid and Interakt for sending Email and WhatsApp\n",
      " messages respectively.\n",
      "\n",
      "\n",
      "\n",
      "Projects\n",
      "\n",
      "LMS/BMS\n",
      " • Company’s core existing project.\n",
      " • Made life easier for travel agents by developing Login/Signup, KYC, Pending Bookings and Post\n",
      " Booking Operations (PBOs) etc. with the help of APIs using Django-REST Framework\n",
      "\n",
      "Aryabhatta\n",
      " • Developed a One Stop Solution system based on Elasticsearch and Django which solves\n",
      " multiple workflows of our internal teams like finance, business, package experts etc.\n",
      " • Solved for data syncing through Celery & RabbitMQ across all internal microservices so as to\n",
      " present the data in a beautified manner via numerous REST APIs to help increase business &\n",
      " finance teams productivity.\n",
      "\n",
      "Yotta\n",
      " • Developed a node-based engine with postgres with timescale db as a plugin on it which\n",
      " solves time based queries to show analytics data like bookings, searches etc for all the travel\n",
      " agents to their particular account managers so that account managers can easily target their\n",
      " agents to do more business with us.\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "Master of Computer Application 2022\n",
      "\n",
      "University School of Information Technology &Communication, GGSIPU, New Delhi\n",
      "\n",
      "\n",
      "BSc Hons Computer Science 2019\n",
      "Shaheed Sukhdev College of Business Studies,Delhi University, New Delhi\n"
     ]
    }
   ],
   "source": [
    "import html \n",
    "rt = eval_df.sample()['resume'].values[0]\n",
    "rt = html.unescape(rt)\n",
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_prompt = f'''\n",
    "# You are a helpful language model working for a job platform. You will be given the raw \n",
    "#  unstructured text of a user's resume, and the task is to extract the work experience of the \n",
    "#  user from the raw text in the following format: \\n{{work_format}}\\n\n",
    "\n",
    "#  This is the resume text:\\n{{resume_text}}\\n\n",
    "#  This is the output in the required format:\\n\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# work_format = '''{\n",
    "#     'work_experience': [{'company': 'company Name 1',\n",
    "#                          'role': 'job designation 1',\n",
    "#                          'start_date': 'mm/yyyy',\n",
    "#                          'end_date': 'mm/yyyy',\n",
    "#                          'description': 'complete Job description taken from resume'},\n",
    "#                         {'company': 'company name 2',\n",
    "#                          'role': 'job designation 2',\n",
    "#                          'start_date': mm/yyyy',\n",
    "#                          'end_date': 'mm/yyyy',\n",
    "#                          'description': 'complete Job description taken from resume'}]\n",
    "# }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from string import Template\n",
    "edu_eval_prompt = f'''\n",
    "You are a helpful language model working for a job platform. You will be given the raw \n",
    " unstructured text of a user's resume, and the task is to extract the \n",
    " educational details (undergraduate/postgraduate degrees, name of programs, certifications) of the \n",
    " user from the raw text in the following format: \\n {{edu_format}}\\n\n",
    " If the information for a certain field is not available, return 'NA'\n",
    " This is the resume text:\\n{{rt}}\\n\n",
    " This is the output in the required_format:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edu_format = '''[\n",
    "    {\n",
    "        \"institution\": \"put name of educational institution here\"\n",
    "        \"program\" : \"name of degree/certification/diploma as given in the resume\"\n",
    "        \"start_date\" : \"start date in dd/mm/yyyy format\"\n",
    "        \"end_date\" : \"end date in dd/mm/yyyy format\"\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        \"institution\": \"put name of educational institution here\"\n",
    "        \"program\" : \"name of degree/certification/diploma as given in the resume\"\n",
    "        \"start_date\" : \"start date in dd/mm/yyyy format\"\n",
    "        \"end_date\" : \"end date in dd/mm/yyyy format\"\n",
    "        \n",
    "    }\n",
    "]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_prompt = edu_prompt.substitute(\n",
    "#         edu_format=edu_format,\n",
    "#         resume_text=rt\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# eval_prompt = edu_eval_prompt.substitute(\n",
    "#             edu_format=edu_format,\n",
    "#             resume_text=rt)\n",
    "\n",
    "\n",
    "eval_prompt = edu_eval_prompt.format(\n",
    "            edu_format=edu_format,\n",
    "            rt=rt)\n",
    "\n",
    "sample_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    full_document = tokenizer.decode(model.generate(**sample_input, max_new_tokens=1000)[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_str = full_document.replace(eval_prompt,\"\")\n",
    "print(f'Time taken :{time.time()-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Ex Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRADEEP KUMAR\n",
      "Project Manager- IT Infra\n",
      " pkindians@gmail.com\n",
      " 9910393860\n",
      " 1097/2 FF Pinewood Enclave Sec-2 Wave City Ghaziabad U.P.-201002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Professional Experience\n",
      "Project Manager-IT Infra, 04/2022 &ndash; present | Noida, India\n",
      "iBoss Tech Solutions Private Limited\n",
      " &bull;Work closely with IT Director to leverage IT for business benefit.\n",
      " &bull;Build system and process for smooth operations.\n",
      " &bull;Run projects to ensure that they meet deadline, customer requirements and organizational goals in\n",
      " efficient manner.\n",
      " &bull;Plan, schedule and supervise the work of each tech team to ensure the services are provided on time and\n",
      " in efficient manner\n",
      " &bull;Evaluate, plan and procure, operationalize and retire appropriate technology solutions.\n",
      " &bull;Mange relevant contracts and ensure compliance and governance.\n",
      " &bull;Control cost and budgeting regarding IT systems.\n",
      " &bull;Responsible for managing Operations and Projects ensure highest uptime of IT services. Service call\n",
      " closure to meet business SLAs and ensure all systems are as per latest standards.\n",
      " &bull;Conduct Kick-off meetings to review Project Objectives, SoW, Change Control Policy.\n",
      " &bull;Take care of Vendor Management, Incident Management, Change Management and Provisioning\n",
      " requests related to inter dept work.\n",
      "\n",
      "IT PROJECT COORDINATOR, Bharat Electronics Limited 07/2021 &ndash; 04/2022 | GHAZIABAD, India\n",
      "IT-Administration and DataBase Management for Command Center.\n",
      "Monitoring and Maintenance of ERP/non-ERP hardware consisting of servers, storage system at data\n",
      "center including replication at DR site.\n",
      "Monitoring of Data Center IT Infrastructure and coordination with different OEM in case of maintenance like\n",
      "breakdown/preventive, up-gradation of system hardware/firmware/OS, etc as well as installation and\n",
      "commissioning of new systems at the Data Center.\n",
      "Key Tasks:\n",
      "ESXi installation, creating VM using Vmware, and running IT monitoring applications\n",
      "Maintaining the running condition of the Data Centre to follow up SLA.\n",
      "L2/L3 Switch Configuration and Server Management using UCS Manager\n",
      "Vsphere and Load Balancer for Cluster-based Virtualisation monitoring.\n",
      "Tools used: ITOM, AM, SM, UCMDB, AuditVault, EMS, Vmware, Vsphere,Vcentre Server, UCS Manager, ERP,\n",
      "ESXi Bare Metal Hypervisor.\n",
      "\n",
      "Deputy Engineer, Bharat Electronics Limited 03/2018 &ndash; 03/2021 | Bangalore, India\n",
      "1.Project- Election Commission VVPAT 2014, worked with the module designing team. Planned layout for\n",
      "mass manufacturing of the final product.\n",
      "2. Kerala Optical Fiber Network &amp; Data Acquisition Project for KERALA under KSITL and KSEB:\n",
      "For around 2yrs worked in a Project Management position for the Optical Fiber Project owned by Kerala\n",
      "State Govt. undertaken by Bharat Electronics Limited, Bangalore.\n",
      "Expertise in:\n",
      "1. Skilled in OFC Technologies (Optical Fiber Cable properties, Ducts, FDMS, etc in Project and O&amp;M)\n",
      "2. Experience in leading OFC Network Rollout Project desired. 3. Installation, commissioning, provisioning\n",
      "and acceptance testing of the OFC Network.\n",
      "4. Installation, commissioning, provisioning and acceptance testing of the OFC Network.\n",
      "5. Follow the schedule for Timely Completion of the Link survey and Project Deployment.\n",
      "6. Identification and mitigation of risks during the project\n",
      "7. Excellent communications and presentation skills.\n",
      "8. Working knowledge of EMS and NMS for remote management of OFC Network\n",
      "9. Reporting and taking corrective/preventive action during project and Network, Equipment and Service\n",
      "level faults.\n",
      ",10. Manage assets, materials and contracts for OFC Network Rollout.\n",
      "11. Monitor performance of fiber network KPIs against defined SLAs\n",
      "12. Coordinate with network operations center and field team for issue resolution.\n",
      "13. Ensure sufficient material in the Central/Zonal warehouse during OFC Network rollout and O&amp;M.\n",
      "14. Ensure implementation of quality, health, safety, environmental, and fire policies\n",
      "15. Maintain quality of fiber network and ensure the quality of work during fault restoration.\n",
      "\n",
      "System Engineer, ITI Limited 08/2015 &ndash; 03/2018 | Delhi, India\n",
      "1. Project Operations and maintenance, Telecom Switch (OCB-283) All Switching system and\n",
      "PSTN servers running Telephone exchanges of MTNL(Delhi) and BSNL(UP-W).\n",
      "Data Centre and NOC Management for BSNL and MTNL\n",
      "KEY FEATURES OF THE TASK:\n",
      "As a Project cum site in charge of all the regular and annual activities to be done by.\n",
      "Under the Annual Maintenance Contract, we look for all the intricacies related to hardware and\n",
      "software.\n",
      "i. CCR (Call completion ratio)\n",
      "ii. Buffer storage of the stations.\n",
      "iii. Hard disk status in the maintenance station(SMM)\n",
      "2. Project GPON (Gigabyte passive optical network)\n",
      "Under this project, all the rural regions of the country are connected to high-speed broadband networks.\n",
      "This is\n",
      "under the initiative of the ministry running a project named NOFN.\n",
      "Under NOFN all the local villages under Gram Panchayat are agreed to be connected to high-speed\n",
      "broadband network so that the internet facility could be easily accessible to all the local villages of India.\n",
      "We\n",
      "as a team completed AT in UP(W) and other regions of North India for OLT and ONT installed by the\n",
      "consortium partner.\n",
      "\n",
      "Education\n",
      "B-Tech, Uttar Pradesh Technical University 06/2010 &ndash; 07/2014 | Ghaziabad, India\n",
      "\n",
      "Skills\n",
      "Telecom O&amp;M Service Level Maintenance\n",
      "\n",
      "Production Control Project Coordination\n",
      "\n",
      "Vendor Management System Integrator\n",
      "\n",
      "Maintenance of FFTH N/w Team Management\n",
      "\n",
      "Electronics Measuring Instruments PCB Troubuleshooting\n",
      "\n",
      "Data centre Supervision, Coordination &amp; Tech\n",
      " Support\n",
      "Liasoning\n",
      "local authority/building owners etc OTDR and LPSM\n",
      "\n",
      "GPON, OLT &amp; ONT RHEL\n",
      "\n",
      "EMS Virtual Machines/ Vmware sphere\n",
      "\n",
      "NMS Asset Manager\n",
      "\n",
      "CISCO Blade server Shell Scripting, Patch Upgradation for DC/DR,\n",
      " SLA Management and Preventing Down Time\n",
      "\n",
      "Languages\n",
      "Shell Scripting\n",
      ",Certificates\n",
      " PSTN Installation and Maintenance SPV Designing, Planning, Installation and\n",
      " Maintenance\n",
      " Agile Scrum Foundation\n",
      " PMP Basics\n",
      " Agile Scrum Master\n",
      " Introduction to Six Sigma\n",
      " Project Execution: Running the Project - Google\n",
      " Project Planning: Putting it all together- Google\n",
      " Program Stake Holder Mgt\n",
      "\n",
      "Interests\n",
      "Reading, Writing\n",
      "\n",
      "Operations and Projects\n",
      "Kerala Fiber Optics N/w 01/2020 &ndash; 03/2021\n",
      "Optical Fiber Project owned by Kerala State Govt. undertaken by Bharat Electronics Limited, Bangalore. The\n",
      "project aims to provide connectivity to 30K govt. and educational institutes as part of the Right to Internet\n",
      "Access.\n",
      "Taking from route survey to capturing the GIS coordinates and handling out NOC installation process with\n",
      "network element integration comes under the roles and responsibilities of District In charge.\n",
      "\n",
      "Project Small Arms Training System 09/2018 &ndash; 12/2019\n",
      "Designed and developed Electronics Target System under Project Small Arms Training Simulator as a Firing\n",
      "range simulation to be used in Armed forces.\n",
      "System sizing and selection of components for the up-gradation of old model has also been done with the\n",
      "Simulator team.\n",
      "\n",
      "Voter Verified Paper Audit Trial 04/2018 &ndash; 08/2018\n",
      "For the Election Commission VVPAT 2014 Project, worked with the module designing team. Planned layout\n",
      "for mass manufacturing of the final product.Finalized assembly line work structure and handled supply\n",
      "chain management and quality management.\n",
      "\n",
      "GPON 06/2016 &ndash; 03/2018\n",
      "Under this project, all the rural regions of the country are connected to high-speed broadband networks.\n",
      "This is\n",
      "under the initiative of the ministry running a project named NOFN.\n",
      "Under NOFN all the local villages under Gram Panchayat are agreed to be connected to high-speed\n",
      "broadband network so that the internet facility could be easily accessible to all the local villages of India.\n",
      "We\n",
      "as a team completed AT in UP(W) and other regions of North India for OLT and ONT installed by the\n",
      "consortium partner.\n",
      "\n",
      "PSTN Switch Preventive Maintenance 08/2015 &ndash; 05/2016\n",
      "Project Installation and maintenance of Telecom Switch (OCB-283).\n",
      "Annual Maintenance Contract for H/w and S/w of All Switching systems and PSTN servers running under\n",
      "the roof of MTNL(Delhi) and BSNL(UP-W) telephone exchanges. Maintaining the running condition of the\n",
      "systems and managing call traffic flow through tandem exchanges. Provisioning and implementing\n",
      "connectivity of Remote Subscriber Unit to the main telephone exchanges\n",
      "\n",
      "Declaration\n",
      "\n",
      "PRADEEP&nbsp;KUMAR\n",
      "GHAZIABAD\n"
     ]
    }
   ],
   "source": [
    "rt = eval_df.sample()['resume'].values[0]\n",
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_format = '''{\n",
    "    'work_experience': [{'company': 'company Name 1',\n",
    "                         'role': 'job designation 1',\n",
    "                         'start_date': 'mm/yyyy',\n",
    "                         'end_date': 'mm/yyyy',\n",
    "                         'description': 'complete Job description taken from resume'},\n",
    "                        {'company': 'company name 2',\n",
    "                         'role': 'job designation 2',\n",
    "                         'start_date': mm/yyyy',\n",
    "                         'end_date': 'mm/yyyy',\n",
    "                         'description': 'complete Job description taken from resume'}]\n",
    "}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_prompt = f'''\n",
    "You are a helpful language model working for a job platform. You will be given the raw \n",
    " unstructured text of a user's resume, and the task is to extract the work experience of the \n",
    " user from the raw text in the following format: \\n{{work_format}}\\n\n",
    "\n",
    " This is the resume text:\\n{{resume_text}}\\n\n",
    " This is the output in the required format:\\n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_prompt = eval_prompt.format(\n",
    "            work_format=work_format,\n",
    "            resume_text=rt)\n",
    "\n",
    "sample_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    full_document = tokenizer.decode(model.generate(**sample_input, max_new_tokens=2000)[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful language model working for a job platform. You will be given the raw \n",
      " unstructured text of a user's resume, and the task is to extract the work experience of the \n",
      " user from the raw text in the following format: \n",
      "{\n",
      "    'work_experience': [{'company': 'company Name 1',\n",
      "                         'role': 'job designation 1',\n",
      "                         'start_date': 'mm/yyyy',\n",
      "                         'end_date': 'mm/yyyy',\n",
      "                         'description': 'complete Job description taken from resume'},\n",
      "                        {'company': 'company name 2',\n",
      "                         'role': 'job designation 2',\n",
      "                         'start_date': mm/yyyy',\n",
      "                         'end_date': 'mm/yyyy',\n",
      "                         'description': 'complete Job description taken from resume'}]\n",
      "}\n",
      "\n",
      "\n",
      " This is the resume text:\n",
      "PRADEEP KUMAR\n",
      "Project Manager- IT Infra\n",
      " pkindians@gmail.com\n",
      " 9910393860\n",
      " 1097/2 FF Pinewood Enclave Sec-2 Wave City Ghaziabad U.P.-201002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Professional Experience\n",
      "Project Manager-IT Infra, 04/2022 &ndash; present | Noida, India\n",
      "iBoss Tech Solutions Private Limited\n",
      " &bull;Work closely with IT Director to leverage IT for business benefit.\n",
      " &bull;Build system and process for smooth operations.\n",
      " &bull;Run projects to ensure that they meet deadline, customer requirements and organizational goals in\n",
      " efficient manner.\n",
      " &bull;Plan, schedule and supervise the work of each tech team to ensure the services are provided on time and\n",
      " in efficient manner\n",
      " &bull;Evaluate, plan and procure, operationalize and retire appropriate technology solutions.\n",
      " &bull;Mange relevant contracts and ensure compliance and governance.\n",
      " &bull;Control cost and budgeting regarding IT systems.\n",
      " &bull;Responsible for managing Operations and Projects ensure highest uptime of IT services. Service call\n",
      " closure to meet business SLAs and ensure all systems are as per latest standards.\n",
      " &bull;Conduct Kick-off meetings to review Project Objectives, SoW, Change Control Policy.\n",
      " &bull;Take care of Vendor Management, Incident Management, Change Management and Provisioning\n",
      " requests related to inter dept work.\n",
      "\n",
      "IT PROJECT COORDINATOR, Bharat Electronics Limited 07/2021 &ndash; 04/2022 | GHAZIABAD, India\n",
      "IT-Administration and DataBase Management for Command Center.\n",
      "Monitoring and Maintenance of ERP/non-ERP hardware consisting of servers, storage system at data\n",
      "center including replication at DR site.\n",
      "Monitoring of Data Center IT Infrastructure and coordination with different OEM in case of maintenance like\n",
      "breakdown/preventive, up-gradation of system hardware/firmware/OS, etc as well as installation and\n",
      "commissioning of new systems at the Data Center.\n",
      "Key Tasks:\n",
      "ESXi installation, creating VM using Vmware, and running IT monitoring applications\n",
      "Maintaining the running condition of the Data Centre to follow up SLA.\n",
      "L2/L3 Switch Configuration and Server Management using UCS Manager\n",
      "Vsphere and Load Balancer for Cluster-based Virtualisation monitoring.\n",
      "Tools used: ITOM, AM, SM, UCMDB, AuditVault, EMS, Vmware, Vsphere,Vcentre Server, UCS Manager, ERP,\n",
      "ESXi Bare Metal Hypervisor.\n",
      "\n",
      "Deputy Engineer, Bharat Electronics Limited 03/2018 &ndash; 03/2021 | Bangalore, India\n",
      "1.Project- Election Commission VVPAT 2014, worked with the module designing team. Planned layout for\n",
      "mass manufacturing of the final product.\n",
      "2. Kerala Optical Fiber Network &amp; Data Acquisition Project for KERALA under KSITL and KSEB:\n",
      "For around 2yrs worked in a Project Management position for the Optical Fiber Project owned by Kerala\n",
      "State Govt. undertaken by Bharat Electronics Limited, Bangalore.\n",
      "Expertise in:\n",
      "1. Skilled in OFC Technologies (Optical Fiber Cable properties, Ducts, FDMS, etc in Project and O&amp;M)\n",
      "2. Experience in leading OFC Network Rollout Project desired. 3. Installation, commissioning, provisioning\n",
      "and acceptance testing of the OFC Network.\n",
      "4. Installation, commissioning, provisioning and acceptance testing of the OFC Network.\n",
      "5. Follow the schedule for Timely Completion of the Link survey and Project Deployment.\n",
      "6. Identification and mitigation of risks during the project\n",
      "7. Excellent communications and presentation skills.\n",
      "8. Working knowledge of EMS and NMS for remote management of OFC Network\n",
      "9. Reporting and taking corrective/preventive action during project and Network, Equipment and Service\n",
      "level faults.\n",
      ",10. Manage assets, materials and contracts for OFC Network Rollout.\n",
      "11. Monitor performance of fiber network KPIs against defined SLAs\n",
      "12. Coordinate with network operations center and field team for issue resolution.\n",
      "13. Ensure sufficient material in the Central/Zonal warehouse during OFC Network rollout and O&amp;M.\n",
      "14. Ensure implementation of quality, health, safety, environmental, and fire policies\n",
      "15. Maintain quality of fiber network and ensure the quality of work during fault restoration.\n",
      "\n",
      "System Engineer, ITI Limited 08/2015 &ndash; 03/2018 | Delhi, India\n",
      "1. Project Operations and maintenance, Telecom Switch (OCB-283) All Switching system and\n",
      "PSTN servers running Telephone exchanges of MTNL(Delhi) and BSNL(UP-W).\n",
      "Data Centre and NOC Management for BSNL and MTNL\n",
      "KEY FEATURES OF THE TASK:\n",
      "As a Project cum site in charge of all the regular and annual activities to be done by.\n",
      "Under the Annual Maintenance Contract, we look for all the intricacies related to hardware and\n",
      "software.\n",
      "i. CCR (Call completion ratio)\n",
      "ii. Buffer storage of the stations.\n",
      "iii. Hard disk status in the maintenance station(SMM)\n",
      "2. Project GPON (Gigabyte passive optical network)\n",
      "Under this project, all the rural regions of the country are connected to high-speed broadband networks.\n",
      "This is\n",
      "under the initiative of the ministry running a project named NOFN.\n",
      "Under NOFN all the local villages under Gram Panchayat are agreed to be connected to high-speed\n",
      "broadband network so that the internet facility could be easily accessible to all the local villages of India.\n",
      "We\n",
      "as a team completed AT in UP(W) and other regions of North India for OLT and ONT installed by the\n",
      "consortium partner.\n",
      "\n",
      "Education\n",
      "B-Tech, Uttar Pradesh Technical University 06/2010 &ndash; 07/2014 | Ghaziabad, India\n",
      "\n",
      "Skills\n",
      "Telecom O&amp;M Service Level Maintenance\n",
      "\n",
      "Production Control Project Coordination\n",
      "\n",
      "Vendor Management System Integrator\n",
      "\n",
      "Maintenance of FFTH N/w Team Management\n",
      "\n",
      "Electronics Measuring Instruments PCB Troubuleshooting\n",
      "\n",
      "Data centre Supervision, Coordination &amp; Tech\n",
      " Support\n",
      "Liasoning\n",
      "local authority/building owners etc OTDR and LPSM\n",
      "\n",
      "GPON, OLT &amp; ONT RHEL\n",
      "\n",
      "EMS Virtual Machines/ Vmware sphere\n",
      "\n",
      "NMS Asset Manager\n",
      "\n",
      "CISCO Blade server Shell Scripting, Patch Upgradation for DC/DR,\n",
      " SLA Management and Preventing Down Time\n",
      "\n",
      "Languages\n",
      "Shell Scripting\n",
      ",Certificates\n",
      " PSTN Installation and Maintenance SPV Designing, Planning, Installation and\n",
      " Maintenance\n",
      " Agile Scrum Foundation\n",
      " PMP Basics\n",
      " Agile Scrum Master\n",
      " Introduction to Six Sigma\n",
      " Project Execution: Running the Project - Google\n",
      " Project Planning: Putting it all together- Google\n",
      " Program Stake Holder Mgt\n",
      "\n",
      "Interests\n",
      "Reading, Writing\n",
      "\n",
      "Operations and Projects\n",
      "Kerala Fiber Optics N/w 01/2020 &ndash; 03/2021\n",
      "Optical Fiber Project owned by Kerala State Govt. undertaken by Bharat Electronics Limited, Bangalore. The\n",
      "project aims to provide connectivity to 30K govt. and educational institutes as part of the Right to Internet\n",
      "Access.\n",
      "Taking from route survey to capturing the GIS coordinates and handling out NOC installation process with\n",
      "network element integration comes under the roles and responsibilities of District In charge.\n",
      "\n",
      "Project Small Arms Training System 09/2018 &ndash; 12/2019\n",
      "Designed and developed Electronics Target System under Project Small Arms Training Simulator as a Firing\n",
      "range simulation to be used in Armed forces.\n",
      "System sizing and selection of components for the up-gradation of old model has also been done with the\n",
      "Simulator team.\n",
      "\n",
      "Voter Verified Paper Audit Trial 04/2018 &ndash; 08/2018\n",
      "For the Election Commission VVPAT 2014 Project, worked with the module designing team. Planned layout\n",
      "for mass manufacturing of the final product.Finalized assembly line work structure and handled supply\n",
      "chain management and quality management.\n",
      "\n",
      "GPON 06/2016 &ndash; 03/2018\n",
      "Under this project, all the rural regions of the country are connected to high-speed broadband networks.\n",
      "This is\n",
      "under the initiative of the ministry running a project named NOFN.\n",
      "Under NOFN all the local villages under Gram Panchayat are agreed to be connected to high-speed\n",
      "broadband network so that the internet facility could be easily accessible to all the local villages of India.\n",
      "We\n",
      "as a team completed AT in UP(W) and other regions of North India for OLT and ONT installed by the\n",
      "consortium partner.\n",
      "\n",
      "PSTN Switch Preventive Maintenance 08/2015 &ndash; 05/2016\n",
      "Project Installation and maintenance of Telecom Switch (OCB-283).\n",
      "Annual Maintenance Contract for H/w and S/w of All Switching systems and PSTN servers running under\n",
      "the roof of MTNL(Delhi) and BSNL(UP-W) telephone exchanges. Maintaining the running condition of the\n",
      "systems and managing call traffic flow through tandem exchanges. Provisioning and implementing\n",
      "connectivity of Remote Subscriber Unit to the main telephone exchanges\n",
      "\n",
      "Declaration\n",
      "\n",
      "PRADEEP&nbsp;KUMAR\n",
      "GHAZIABAD\n",
      "\n",
      " This is the output in the required format:\n",
      "\n",
      "[{'company': 'iBoss Tech Solutions Private Limited', 'role': 'Project Manager- IT Infra', 'start_date': '04/2022', 'end_date': 'present', 'description': 'Work closely with IT Director to leverage IT for business benefit. Build system and process for smooth operations. Run projects to ensure that they meet deadline, customer requirements and organizational goals in efficient manner. Plan, schedule and supervise the work of each tech team to ensure the services are provided on time and in efficient manner. Evaluate, plan and procure, operationalize and retire appropriate technology solutions. Manage relevant contracts and ensure compliance and governance. Control cost and budgeting regarding IT systems. Responsible for managing Operations and Projects ensure highest uptime of IT services. Service call closure to meet business SLAs and ensure all systems are as per latest standards. Conduct Kick-off meetings to review Project Objectives, SoW, Change Control Policy.'}, {'company': 'Bharat Electronics Limited', 'role': 'Deputy Engineer', 'start_date': '03/2018', 'end_date': '03/2021', 'description': 'Project- Election Commission VVPAT 2014, worked with the module designing team. Planned layout for mass manufacturing of the final product. Key Tasks: Expertise in: 1. Skilled in OFC Technologies (Optical Fiber Cable properties, Ducts, FDMS, etc in Project and O&M) 2. Experience in leading OFC Network Rollout Project desired. 3. Installation, commissioning, provisioning and acceptance testing of the OFC Network. 4. Installation, commissioning, provisioning and acceptance testing of the OFC Network. 5. Follow the schedule for Timely Completion of the Link survey and Project Deployment. 6. Identification and mitigation of risks during the project and Network, Equipment and Service level faults. 7. Ensure sufficient material in the Central/Zonal warehouse during OFC Network rollout and O&M. 8. Ensure implementation of quality, health, safety, environmental, and fire policies. 9. Maintain quality of fiber network and ensure the quality of work during fault restoration.'}, {'company': 'ITI Limited', 'role': 'System Engineer', 'start_date': '08/2015', 'end_date': '03/2018', 'description': 'Project Operations and maintenance, Telecom Switch (OCB-283) All Switching system and PSTN servers running Telephone exchanges of MTNL(Delhi) and BSNL(UP-W). Key Features of the Task: As a Project cum site in charge of all the regular and annual activities to be done by. Under the Annual Maintenance Contract, we look for all the intricacies related to hardware and software. i. CCR (Call completion ratio) ii. Buffer storage of the stations. iii. Hard disk status in the maintenance station(SMM) 2. Project GPON (Gigabyte passive optical network) Under this project, all the rural regions of the country are connected to high-speed broadband networks. This is under the initiative of the ministry running a project named NOFN. Under NOFN all the local villages under Gram Panchayat are agreed to be connected to high-speed broadband network so that the internet facility could be easily accessible to all the local villages of India. We as a team completed AT in UP(W) and other regions of North India for OLT and ONT installed by the consortium partner.'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9937b942474ae5a869af70a85051ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lakshay/work-peft/commit/a3673b3eb8a49faac85c10153da3e43b6323e623', commit_message='Upload model', commit_description='', oid='a3673b3eb8a49faac85c10153da3e43b6323e623', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('lakshay/work-peft',token='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', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Information Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from string import Template\n",
    "pi_eval_prompt = Template('''You are a helpful language model working for a job platform. You will be given the raw \n",
    " unstructured text of a user's resume, and the task is to extract the personal information (name, phone number, email ID and the location) of the \n",
    " user from the raw text in the following format: \\n${pi_format}\\n\n",
    " If the information is not available, return 'NA'\n",
    " This is the resume text:\\n${resume_text}\\n\n",
    " This is the output in the required_format:\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pi_format = '''{\n",
    "    'personal_information': {'name': \"Name\",\n",
    "                         'email_id': \"Valid Email ID\",\n",
    "                         'phone_number': \"10 Digit phone number\",\n",
    "                         'location': \"User's current location\"}\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rt = eval_df.sample()['resume'].values[0]\n",
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_prompt = pi_eval_prompt.substitute(\n",
    "            pi_format=pi_format,\n",
    "            resume_text=rt)\n",
    "\n",
    "sample_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    full_document = tokenizer.decode(model.generate(**sample_input, max_new_tokens=200)[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75804f0963d448fbb0e7ba1e1d423bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lakshay/llama2-test/commit/9460af41bdcca6c6b9cafac27d3ee09a4bd6c36a', commit_message='Upload model', commit_description='', oid='9460af41bdcca6c6b9cafac27d3ee09a4bd6c36a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('lakshay/llama2-test',token='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', max_shard_size='2GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PI validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('custom_data/validation_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data.resume.values[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "error_list = list()\n",
    "correct_list = list()\n",
    "\n",
    "for uid,rt in tqdm(validation_data[['id','resume']].sample(frac=1).values[:200]):\n",
    "\n",
    "    eval_prompt = pi_eval_prompt.substitute(\n",
    "                pi_format=pi_format,\n",
    "                resume_text=rt)\n",
    "\n",
    "    sample_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            full_document = tokenizer.decode(model.generate(**sample_input, max_new_tokens=200)[0], skip_special_tokens=True)\n",
    "    except:\n",
    "        print('feck')\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        out_str = full_document.replace(eval_prompt,'').replace('$','')\n",
    "        out_json = ast.literal_eval(out_str)\n",
    "        u_info = {}\n",
    "        u_info[uid] = out_json\n",
    "        correct_list.append(u_info)\n",
    "    except:\n",
    "        error_list.append(full_document)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# correct_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hello there, $, yes'.replace('there,','').replace('$','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_list\n",
    "\n",
    "with open('custom_data/validation_output.pkl','wb') as f:\n",
    "    pickle.dump(correct_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b66f79022e4d62b7980aaa0d6e8ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/581 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlakshay/llama2-test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# tokenizer = LlamaTokenizer.from_pretrained(model_id)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhf_rthVXJBMwUqJSEayJxkiKZtRSIwFLEVwot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/modeling_utils.py:3646\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3642\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3643\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   3644\u001b[0m         }\n\u001b[1;32m   3645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 3646\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3647\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3648\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   3649\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   3650\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m             )\n\u001b[1;32m   3656\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   3658\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"lakshay/llama2-test\"\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16, token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
