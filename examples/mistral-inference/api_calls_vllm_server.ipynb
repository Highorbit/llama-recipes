{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c141ca86-c173-41cc-b357-5324318aa727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "login(token='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')\n",
    "\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35fb721-b69b-4592-ace5-ef7f73435658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fb561a-c409-4bce-8c79-c84b266d20dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import time, json, ast, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cfedcc-3516-44fc-991a-f159abf1850e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sampling_params = SamplingParams(temperature=0, max_tokens=2000)\n",
    "# llm = LLM(model=\"lakshay/work-details-mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0df83f-69df-479d-a112-8db4746cf28d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da953dee-842d-4544-9696-f8ec20a5fd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../custom_data/mistral/new_users_march24.json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b6b02c-8d12-4034-a160-efd253d9dae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for user_id, resume_text in data.items():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d541ce12-6d66-4dfc-813f-c57b7048e680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame([data]).T.reset_index()\n",
    "res_df = res_df.rename(columns={'index': 'user_id', 0: 'resume_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7481de-57e5-4409-a4f4-1ca63e073877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>resume_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2826298</td>\n",
       "      <td>William Joseph Kakkassery\\nwilliamjosephk1992@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2826299</td>\n",
       "      <td>URMEET SINGH\\n+91 9307444144 | urmeet522@gmail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2826304</td>\n",
       "      <td>Rupesh Gulyani\\nContact: +91 - 9990510412\\nE-M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2826305</td>\n",
       "      <td>Govind Sai Bisai, Data Analyst\\nWhitefield, Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2826309</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>2827743</td>\n",
       "      <td>PROFILE SUMMARY:\\nAn Enthusiastic and highly m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>2827745</td>\n",
       "      <td>Pleasant Kemisola Folorunso\\nAddress : N o 6 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>2827746</td>\n",
       "      <td>Nikitasa Nanda\\nPhone No. : +919113889161\\nEma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>2827748</td>\n",
       "      <td>GET IN CONTACT\\nMobile: +91-8660946102\\nEmail:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>2827751</td>\n",
       "      <td>CURRICULUM VITAE\\nASHISH BHARATRAO KOLHE\\nRoya...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id                                        resume_text\n",
       "0    2826298  William Joseph Kakkassery\\nwilliamjosephk1992@...\n",
       "1    2826299  URMEET SINGH\\n+91 9307444144 | urmeet522@gmail...\n",
       "2    2826304  Rupesh Gulyani\\nContact: +91 - 9990510412\\nE-M...\n",
       "3    2826305  Govind Sai Bisai, Data Analyst\\nWhitefield, Be...\n",
       "4    2826309                                                   \n",
       "..       ...                                                ...\n",
       "715  2827743  PROFILE SUMMARY:\\nAn Enthusiastic and highly m...\n",
       "716  2827745  Pleasant Kemisola Folorunso\\nAddress : N o 6 3...\n",
       "717  2827746  Nikitasa Nanda\\nPhone No. : +919113889161\\nEma...\n",
       "718  2827748  GET IN CONTACT\\nMobile: +91-8660946102\\nEmail:...\n",
       "719  2827751  CURRICULUM VITAE\\nASHISH BHARATRAO KOLHE\\nRoya...\n",
       "\n",
       "[720 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b03d2-65bc-4ae1-ac82-07fa479637db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e3c342-fe42-4781-a154-31c9b6970d52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def workex_linear_prompt(rt):\n",
    "\n",
    "    '''This function creates a prompt to extract work experience from a mistral instruction tuned model '''\n",
    "    \n",
    "    \n",
    "    workex_prompt = f'''\n",
    "    You are an accurate agent working for a job platform. You will be given the raw \n",
    "    unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
    "    user from the resume. The response should be presented into a numbered list with each item of the list \n",
    "    being an unbroken line of text containing the complete and accurate information about the work experience of the users. \n",
    "    Here is an example structure:\\n\n",
    "    1. Designation 1 @ Company 1 [From \"mm/yyy\" to \"mm/yyyy\"] :\\n\n",
    "    2. Designation 2 @ Company 2 [From \"mm/yyy\" to \"mm/yyyy\"] :\\n\n",
    "    Please follow this structure accurately and keep the response within the token limit.\" \n",
    "    \n",
    "    This is the resume text:\\n{{resume_text}}\\n\n",
    "    This is the output in the required_format:\\n'''\n",
    "\n",
    "    return workex_prompt.format(\n",
    "            resume_text=rt)\n",
    "\n",
    "def edu_linear_prompt(rt):\n",
    "\n",
    "    '''This function creates a prompt to extract educational details from a mistral instruction tuned model '''\n",
    "    \n",
    "    edu_prompt = f'''\n",
    "    [INST]You are an accurate agent working for a job platform. You will be given the raw \n",
    "    unstructured text of a user's resume, and the task is to extract the education history of the \n",
    "    user at the graduate and post graduate level only. The response should be broken into a numbered list with each item of the list \n",
    "    containing the complete and accurate information about the education of the users. Here is an example structure:\\n\n",
    "    1. Degree/Program Name 1 @ Institute 1 [From \"mm/yyyy\" to \"mm/yyyy\"] :\\n\n",
    "    2. Degree/Program Name 2 @ Institute 2 [From \"mm/yyyy\" to \"mm/yyyy\"] :\\n\n",
    "    Please follow this structure closely and keep the response within the token limit[\\INST] \n",
    "    \n",
    "    This is the resume text:\\n{{resume_text}}\\n\n",
    "    This is the output in the required_format:\\n\n",
    "    '''\n",
    "    \n",
    "    return edu_prompt.format(resume_text=rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd56e8a-83f2-4d07-a9f9-9bebad0a9006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b3ef08e-e84b-4c01-9c27-3b9b7d2bbdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_workex_prompt(rt):\n",
    "\n",
    "    '''This function creates a prompt to extract work experience from a mistral instruction tuned model '''\n",
    "    \n",
    "    prompt_text = f'''You are an accurate agent working for a job platform. You will be given the raw \n",
    "                unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
    "                user from the resume. Please provide the data in a concise JSON format. The JSON should include a \n",
    "                \"work_experience\" key with an array of objects. Each object represents a job and should contain keys for \n",
    "                \"company\", \"role\", \"start_date\", \"end_date\". Dates should be in \"mm/yyyy\" format. \n",
    "                Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces.\n",
    "\n",
    "                This is the resume text:\\n{{resume_text}}\\n\n",
    "                This is the output in the required_format\\n\n",
    "                '''\n",
    "    \n",
    "    work_format = '''\n",
    "                [\n",
    "                    {\"company\":\"Example Company 1\",\n",
    "                    \"role\":\"Example Role 1\",\n",
    "                    \"start_date\":\"mm/yyyy\",\n",
    "                    \"end_date\":\"mm/yyyy\"},\n",
    "                    {\"company\":\"Example Company 2\",\n",
    "                    \"role\":\"Example Role 2\",\"\n",
    "                    start_date\":\"mm/yyyy\",\n",
    "                    \"end_date\":\"mm/yyyy\"}\n",
    "                ]\n",
    "                '''\n",
    "    \n",
    "    return prompt_text.format(\n",
    "            query_format=work_format,\n",
    "            resume_text=rt)\n",
    "\n",
    "def make_edu_prompt(rt):\n",
    "\n",
    "    '''This function creates a prompt to extract educational details from a mistral instruction tuned model '''\n",
    "    \n",
    "    edu_prompt = f'''\n",
    "    [INST]You are an accurate agent working for a job platform. You will be given the raw \n",
    "    unstructured text of a user's resume, and the task is to extract the educational details of a \n",
    "    user's starting from their graduation (include post graduation and further education ONLY). \n",
    "    Please provide the data in a concise JSON format. The response should be a python list \n",
    "    of dictionaries (JSON-formatted). Each dictionary represents an educational experience and should contain keys for \n",
    "    \"institution_name\", \"degree\", \"start_date\", \"end_date\". Dates should be in \"mm/yyyy\" format.\n",
    "    Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces.\n",
    "    \n",
    "    Please follow this structure closely and keep the response within the token limit.\" \\n{{query_format}}\\n[/INST]\n",
    "    \n",
    "    This is the resume text:\\n{{resume_text}}\\n\n",
    "    This is the output in the required_format\\n\n",
    "    '''\n",
    "    \n",
    "    edu_format = '''\n",
    "                    [\n",
    "                        {\"institution\":\"Example Institution 1\",\n",
    "                        \"degree\":\"name of example degree 1\",\n",
    "                        \"start_date\":\"mm/yyyy\",\n",
    "                        \"end_date\":\"mm/yyyy\"},\n",
    "                        {\"institution\":\"Example Institution 2\",\n",
    "                        \"degree\":\"name of example degree 2\",\"\n",
    "                        start_date\":\"mm/yyyy\",\n",
    "                        \"end_date\":\"mm/yyyy\"}\n",
    "                    ]\n",
    "                    '''\n",
    "    \n",
    "    return edu_prompt.format(\n",
    "            query_format=edu_format,\n",
    "            resume_text=rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e67961-66f4-412c-811e-69ced1fbea92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f7a50-bc3b-436d-bed8-4962c0623b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95c9b2-333a-4177-ac19-3df5375fab59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc5805-b644-4805-9d85-398fc1b0b401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f36de6b-1c34-433d-80d0-f75d13bd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_text = '''\n",
    "Prakritidev Verma\\nLead Backend Developer @ Infoedge India Ltd. | Building Scalable Systems, Streamlining Job Search\\n New Delhi      xxxxx      prakritidevverma@gmail.com        \n",
    "https://github.com/prakritidev\\n\\nSummary\\n\\nExperienced\n",
    "in advanced backend development and system architecture, I am a technology polyglot with 6 years of expertise in building\\ndistributed systems, Search Recommendation, Financial Machine Learning, DevOps and System Architecture with Exposure of AWS Cloud.\\n\\nExperience\\n\\nInfoEdge (IIMJOBS/HIRIST)                                                                                                                    01/08/2022 - Present\\nLead Java Backend Developer                                                                                                                           New Delhi\\n\n",
    "https://www.hirist.com/\\n\n",
    "     Lead a team managing Elasticsearch, Kibana, and MongoDB-powered data services for hirist.com and iimjobs.com, with 120+ EC2 instances\\n     and 20+ Spring Boot apps handling 20-25 million daily requests.\\n     Advised and implemented the EFK stack for enhanced system observability, optimizing ElasticSearch and Spring Boot APIs of certain\\n     domain. This strategy led to a 30% EC2 machines reduction and increased request handling from 250 to 450 per second on a 4GB instance\\n     resulting 10% AWS cost reduction. Tech Used: [ShellScript, Linux Utils, FileBeat, Supervisorctl, EC2, AWS Security Groups, Kibana,\\n     ElasticSearch, S3]\\n     Developed advanced query and vector matching to refine resume and job searches, implemented a Neo4j graph for skill-job-user mapping,\\n     and utilized clustering algorithms for query expansion. Working on integration of ScaNN and FAISS with ElasticSearch to enhance search\\n     precision. Tech used: [Python, Pandas, Spring Boot, ElasticSearch, BERT, HuggingFace, SQS, Neo4j APOC, Docker]\\n     Led a POC for email campaign services, transitioning from Python to Spring WebFlux and implementing an exponential backoff algorithm for\\n     inactive users to not send emails, drastically reducing processing time from 7 to 1 hour with Kafka and ElasticSearch. Also established a\\n     robust 30-day Kafka audit pipeline for enhanced data analysis and user behaviors pattern analysis. Tech Used: [Kafka, WebFlux, Reactjs,\\n     Elasticsearch, Docker, EC2]\\n\\nSteppingCloud                                                                                                                            01/03/2018 - 31/07/2022\\nSoftware Engineer                                                                                                                                      New Delhi\\n     Led a product development team of three, designing and architecting a multi-tenant onboarding and registration system using Spring Boot,\\n     enabling billing for multiple products per tenant.\\n     Developed a multi-tenant client onboarding system, inspired by SAP CloudFoundry, using microservice architecture and Spring Netflix Zuul\\n     API Gateway for tenant-specific routing. Integrated Redis for rule caching and MongoDB for data storage, employing technologies like Spring\\n     Boot, Java, and AWS. Tech Used: [AWS, MongoDB, Spring Boot, Node.js, Zuul, Edureka, Redis, Atlas, ShellScript, RBAC]\\n\\nGoeventz                                                                                                                                 01/07/2017 - 01/03/2018\\nMachine Learning Engineer                                                                                                                              Gurugram\\n     Worked as a Machine learning engineer, proposed a recommendation system solution based on text, location and user&rsquo;s preferences.\\n     Created content-based event recommendations using doc2vec, stored results in Neo4j, and developed Flask APIs for result delivery.\\n     Implemented using Python 3, NLTK, Gensim, and Pandas.\\n\\nProjects\\n\\nFinanceKernel\\n                                                                                                                                            01-01-2019 - Present\\nPrivate Algo Trading System\\n     Developed a Python scraper for 5000+ equities, capturing Intraday, Daily, Historical data, and company info, and automated CSV processing\\n     to AWS S3 and SQS. Tech Used : [Concurrent.Futures, requests, lxml, pandas]\\n     Optimized data processing by transitioning from Python to GoLang for SQS message polling and streaming CSVs from S3 to MongoDB,\\n     cutting processing time from 5 minutes to 40 seconds on a 2GB EC2 machine. Tech Used : [GoRoutines, Channels, Generator concurrency\\n     pattern, MongoDB]\\n     Implemented a Python Dask-based distributed financial feature extraction pipeline for 5000+ equities, leveraging AWS spot instances for\\n     database updates. Tech Used: [Dask, Python]\\n     Engineered a Machine Learning Stock Screener, achieving a 0.97 F1 Score, effectively predicting next-day stock performance. Tech Used:\\n     [Scikit-Learn, Pandas, Python]\\n\\nResearchKernel\\n                                                                                                                                             09/01/2018 - Present\\nOpen Source Project for Researcher\\n\n",
    "https://github.com/ResearchKernel\\n\n",
    "     Started the open source project as a community-driven interface for search research papers and generating recommendation with Github\\n     integrated to research papers. Tech Used: [Python, AWS CLI, Neo4j, Elasticsearch, Node.js, React.js, Airflow, Tensorflow, Keras]\\n     Developed an ETL pipeline for extracting texts from PDF using python multiprocessing framework, resolving the Cold Start issue by training\\n     Doc2vec on one million papers synced to s3 [Github]. Tech Used: [Python, Multiprocessing, AWS CLI, S3, Terraform]\\n     Implemented Neo4j for ML result storage and crafted a Python backend for recommendation delivery and Reduced infrastructure costs by\\n     60% using AWS spot instances. Tech Used: [Python, Keras, Doc2Vec, ShellScript, Neo4j]\\n\\nSkills\\n\\nProgramming Languages\\nJava, Python, Go, Lua, Javascript\\n\\nFramework and Technology\\nSpring Boot, Express.js, Flask, Mux, Spring WebFlux, Celery, Airflow, Kafka, SQS, Redis Bull Queue, Node.js\\n\\nDatabase\\nElasticsearch, MongoDB, MYSQL, SAP HANA, Neo4j\\n\\nMachine Learning\\nMLFlow, Scikit-learn, Pandas, Numpy, Keras, Tensorflow\\n\\nCloud Technologies\\nAWS, S3, SQS, CloudFoundry, ECS, SNS, EC2, EBS, SAP HANA\\n\\nOther Tech\\nnvim, grep, awk, GNU Parallel, pyfolio, backtrader, jupyter notebook, Docker Compose, Kibana, Logstash, Filebeat, Nginx, XGBoost, Celery, REST APIs, WebSockets,\\nObject-Oriented Design, Design Patterns, Data Structure\\n\\nEducation\\n\\nBharati Vidyapeeth College of Engineering (IP University)                                                                                01/08/2013 - 01/07/2017\\nComputer Science and Engineering                                                                                                                          B.Tech\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336b0d2f-3d09-413f-b1e9-a69800e21d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_text = eval_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb740ea-1505-416e-923c-a8b03c354407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009b2d7-2354-4683-93fa-cd7e408bdab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c58b1fed-8e29-45fa-9bc5-02756beda422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_text = res_df['resume_text'].sample().values[0]\n",
    "work_prompt = html.unescape(make_workex_prompt(eval_text))\n",
    "edu_prompt = html.unescape(make_edu_prompt(eval_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405f8de-1972-47ea-85a4-d100bf74e543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af03910-b5ad-4dcf-b3b4-f2575b65193f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7026ef89-442e-4ed9-9a7e-cc341fabaedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an accurate agent working for a job platform. You will be given the raw \\n                unstructured text of a user\\'s resume, and the task is to extract the entire work experience of the \\n                user from the resume. Please provide the data in a concise JSON format. The JSON should include a \\n                \"work_experience\" key with an array of objects. Each object represents a job and should contain keys for \\n                \"company\", \"role\", \"start_date\", \"end_date\". Dates should be in \"mm/yyyy\" format. \\n                Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces.\\n\\n                This is the resume text:\\nPrakritidev Verma\\nLead Backend Developer @ Infoedge India Ltd. | Building Scalable Systems, Streamlining Job Search\\n New Delhi      xxxxx      prakritidevverma@gmail.com        \\nhttps://github.com/prakritidev\\n\\nSummary\\n\\nExperienced\\nin advanced backend development and system architecture, I am a technology polyglot with 6 years of expertise in building\\ndistributed systems, Search Recommendation, Financial Machine Learning, DevOps and System Architecture with Exposure of AWS Cloud.\\n\\nExperience\\n\\nInfoEdge (IIMJOBS/HIRIST)                                                                                                                    01/08/2022 - Present\\nLead Java Backend Developer                                                                                                                           New Delhi\\n\\nhttps://www.hirist.com/\\n\\n     Lead a team managing Elasticsearch, Kibana, and MongoDB-powered data services for hirist.com and iimjobs.com, with 120+ EC2 instances\\n     and 20+ Spring Boot apps handling 20-25 million daily requests.\\n     Advised and implemented the EFK stack for enhanced system observability, optimizing ElasticSearch and Spring Boot APIs of certain\\n     domain. This strategy led to a 30% EC2 machines reduction and increased request handling from 250 to 450 per second on a 4GB instance\\n     resulting 10% AWS cost reduction. Tech Used: [ShellScript, Linux Utils, FileBeat, Supervisorctl, EC2, AWS Security Groups, Kibana,\\n     ElasticSearch, S3]\\n     Developed advanced query and vector matching to refine resume and job searches, implemented a Neo4j graph for skill-job-user mapping,\\n     and utilized clustering algorithms for query expansion. Working on integration of ScaNN and FAISS with ElasticSearch to enhance search\\n     precision. Tech used: [Python, Pandas, Spring Boot, ElasticSearch, BERT, HuggingFace, SQS, Neo4j APOC, Docker]\\n     Led a POC for email campaign services, transitioning from Python to Spring WebFlux and implementing an exponential backoff algorithm for\\n     inactive users to not send emails, drastically reducing processing time from 7 to 1 hour with Kafka and ElasticSearch. Also established a\\n     robust 30-day Kafka audit pipeline for enhanced data analysis and user behaviors pattern analysis. Tech Used: [Kafka, WebFlux, Reactjs,\\n     Elasticsearch, Docker, EC2]\\n\\nSteppingCloud                                                                                                                            01/03/2018 - 31/07/2022\\nSoftware Engineer                                                                                                                                      New Delhi\\n     Led a product development team of three, designing and architecting a multi-tenant onboarding and registration system using Spring Boot,\\n     enabling billing for multiple products per tenant.\\n     Developed a multi-tenant client onboarding system, inspired by SAP CloudFoundry, using microservice architecture and Spring Netflix Zuul\\n     API Gateway for tenant-specific routing. Integrated Redis for rule caching and MongoDB for data storage, employing technologies like Spring\\n     Boot, Java, and AWS. Tech Used: [AWS, MongoDB, Spring Boot, Node.js, Zuul, Edureka, Redis, Atlas, ShellScript, RBAC]\\n\\nGoeventz                                                                                                                                 01/07/2017 - 01/03/2018\\nMachine Learning Engineer                                                                                                                              Gurugram\\n     Worked as a Machine learning engineer, proposed a recommendation system solution based on text, location and user’s preferences.\\n     Created content-based event recommendations using doc2vec, stored results in Neo4j, and developed Flask APIs for result delivery.\\n     Implemented using Python 3, NLTK, Gensim, and Pandas.\\n\\nProjects\\n\\nFinanceKernel\\n                                                                                                                                            01-01-2019 - Present\\nPrivate Algo Trading System\\n     Developed a Python scraper for 5000+ equities, capturing Intraday, Daily, Historical data, and company info, and automated CSV processing\\n     to AWS S3 and SQS. Tech Used : [Concurrent.Futures, requests, lxml, pandas]\\n     Optimized data processing by transitioning from Python to GoLang for SQS message polling and streaming CSVs from S3 to MongoDB,\\n     cutting processing time from 5 minutes to 40 seconds on a 2GB EC2 machine. Tech Used : [GoRoutines, Channels, Generator concurrency\\n     pattern, MongoDB]\\n     Implemented a Python Dask-based distributed financial feature extraction pipeline for 5000+ equities, leveraging AWS spot instances for\\n     database updates. Tech Used: [Dask, Python]\\n     Engineered a Machine Learning Stock Screener, achieving a 0.97 F1 Score, effectively predicting next-day stock performance. Tech Used:\\n     [Scikit-Learn, Pandas, Python]\\n\\nResearchKernel\\n                                                                                                                                             09/01/2018 - Present\\nOpen Source Project for Researcher\\n\\nhttps://github.com/ResearchKernel\\n\\n     Started the open source project as a community-driven interface for search research papers and generating recommendation with Github\\n     integrated to research papers. Tech Used: [Python, AWS CLI, Neo4j, Elasticsearch, Node.js, React.js, Airflow, Tensorflow, Keras]\\n     Developed an ETL pipeline for extracting texts from PDF using python multiprocessing framework, resolving the Cold Start issue by training\\n     Doc2vec on one million papers synced to s3 [Github]. Tech Used: [Python, Multiprocessing, AWS CLI, S3, Terraform]\\n     Implemented Neo4j for ML result storage and crafted a Python backend for recommendation delivery and Reduced infrastructure costs by\\n     60% using AWS spot instances. Tech Used: [Python, Keras, Doc2Vec, ShellScript, Neo4j]\\n\\nSkills\\n\\nProgramming Languages\\nJava, Python, Go, Lua, Javascript\\n\\nFramework and Technology\\nSpring Boot, Express.js, Flask, Mux, Spring WebFlux, Celery, Airflow, Kafka, SQS, Redis Bull Queue, Node.js\\n\\nDatabase\\nElasticsearch, MongoDB, MYSQL, SAP HANA, Neo4j\\n\\nMachine Learning\\nMLFlow, Scikit-learn, Pandas, Numpy, Keras, Tensorflow\\n\\nCloud Technologies\\nAWS, S3, SQS, CloudFoundry, ECS, SNS, EC2, EBS, SAP HANA\\n\\nOther Tech\\nnvim, grep, awk, GNU Parallel, pyfolio, backtrader, jupyter notebook, Docker Compose, Kibana, Logstash, Filebeat, Nginx, XGBoost, Celery, REST APIs, WebSockets,\\nObject-Oriented Design, Design Patterns, Data Structure\\n\\nEducation\\n\\nBharati Vidyapeeth College of Engineering (IP University)                                                                                01/08/2013 - 01/07/2017\\nComputer Science and Engineering                                                                                                                          B.Tech\\n\\n                This is the output in the required_format\\n\\n                '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b6147-db87-427c-bc5a-1d3123650288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748273e5-611c-485c-9da3-8cbe9b972a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c79db5e6-c05f-461b-92ef-a7be9fca38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = f'''<s>You are an accurate agent working for a job platform. You will be given the raw \n",
    "unstructured text of a user's resume, and you have to return the total professional experience (in years) for \n",
    "the given user.[INST] Return a single number only [\\INST]. Here is the resume text:\\n\\n {{resume_text}}\\n\\n\n",
    "Here is the answer in one word:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4c9d8c8-d0a0-4aaa-a3c1-274a3ffd9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sample_prompt.format(resume_text=eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b331646-f1ec-4e12-952b-7821c55a23a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906342b-1095-4bb3-b2f9-475504d41648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fe9e32c-5eda-4e7a-9eb1-2e96bf3370bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion result: Completion(id='cmpl-f0cf58e1a9b64525ad139aff268cc27a', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='10', stop_reason=None)], created=1715584498, model='lakshay/work-details-mistral', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=2, prompt_tokens=1839, total_tokens=1841))\n",
      "time taken : 0.6007382869720459\n"
     ]
    }
   ],
   "source": [
    "# using vLLM, we've started an openAI compatible server \n",
    "# which just means that we can query our hosted model \n",
    "# like we would query openAI's models like gpt3.5, gpt-4 etc\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "start_time = time.time()\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "# for this request, we just want to know the total years of experience for a user\n",
    "completion = client.completions.create(model=\"lakshay/work-details-mistral\",\n",
    "                                      prompt=sp,\n",
    "                                      max_tokens=2)\n",
    "print(\"Completion result:\", completion)\n",
    "\n",
    "print(f'time taken : {time.time()-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8d8f1-5ea1-43df-abf6-9abbc40627d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9684f-fa11-4f64-aace-34fcdd190452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65c2ff-b5b1-4943-bfcf-a4bfec17d4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42a909e1-d120-4057-bf32-5d5b4dbacc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an accurate agent working for a job platform. You will be given the raw \\n                unstructured text of a user\\'s resume, and the task is to extract the entire work experience of the \\n                user from the resume. Please provide the data in a concise JSON format. The JSON should include a \\n                \"work_experience\" key with an array of objects. Each object represents a job and should contain keys for \\n                \"company\", \"role\", \"start_date\", \"end_date\". Dates should be in \"mm/yyyy\" format. \\n                Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces.\\n\\n                This is the resume text:\\nPrakritidev Verma\\nLead Backend Developer @ Infoedge India Ltd. | Building Scalable Systems, Streamlining Job Search\\n New Delhi      xxxxx      prakritidevverma@gmail.com        \\nhttps://github.com/prakritidev\\n\\nSummary\\n\\nExperienced\\nin advanced backend development and system architecture, I am a technology polyglot with 6 years of expertise in building\\ndistributed systems, Search Recommendation, Financial Machine Learning, DevOps and System Architecture with Exposure of AWS Cloud.\\n\\nExperience\\n\\nInfoEdge (IIMJOBS/HIRIST)                                                                                                                    01/08/2022 - Present\\nLead Java Backend Developer                                                                                                                           New Delhi\\n\\nhttps://www.hirist.com/\\n\\n     Lead a team managing Elasticsearch, Kibana, and MongoDB-powered data services for hirist.com and iimjobs.com, with 120+ EC2 instances\\n     and 20+ Spring Boot apps handling 20-25 million daily requests.\\n     Advised and implemented the EFK stack for enhanced system observability, optimizing ElasticSearch and Spring Boot APIs of certain\\n     domain. This strategy led to a 30% EC2 machines reduction and increased request handling from 250 to 450 per second on a 4GB instance\\n     resulting 10% AWS cost reduction. Tech Used: [ShellScript, Linux Utils, FileBeat, Supervisorctl, EC2, AWS Security Groups, Kibana,\\n     ElasticSearch, S3]\\n     Developed advanced query and vector matching to refine resume and job searches, implemented a Neo4j graph for skill-job-user mapping,\\n     and utilized clustering algorithms for query expansion. Working on integration of ScaNN and FAISS with ElasticSearch to enhance search\\n     precision. Tech used: [Python, Pandas, Spring Boot, ElasticSearch, BERT, HuggingFace, SQS, Neo4j APOC, Docker]\\n     Led a POC for email campaign services, transitioning from Python to Spring WebFlux and implementing an exponential backoff algorithm for\\n     inactive users to not send emails, drastically reducing processing time from 7 to 1 hour with Kafka and ElasticSearch. Also established a\\n     robust 30-day Kafka audit pipeline for enhanced data analysis and user behaviors pattern analysis. Tech Used: [Kafka, WebFlux, Reactjs,\\n     Elasticsearch, Docker, EC2]\\n\\nSteppingCloud                                                                                                                            01/03/2018 - 31/07/2022\\nSoftware Engineer                                                                                                                                      New Delhi\\n     Led a product development team of three, designing and architecting a multi-tenant onboarding and registration system using Spring Boot,\\n     enabling billing for multiple products per tenant.\\n     Developed a multi-tenant client onboarding system, inspired by SAP CloudFoundry, using microservice architecture and Spring Netflix Zuul\\n     API Gateway for tenant-specific routing. Integrated Redis for rule caching and MongoDB for data storage, employing technologies like Spring\\n     Boot, Java, and AWS. Tech Used: [AWS, MongoDB, Spring Boot, Node.js, Zuul, Edureka, Redis, Atlas, ShellScript, RBAC]\\n\\nGoeventz                                                                                                                                 01/07/2017 - 01/03/2018\\nMachine Learning Engineer                                                                                                                              Gurugram\\n     Worked as a Machine learning engineer, proposed a recommendation system solution based on text, location and user’s preferences.\\n     Created content-based event recommendations using doc2vec, stored results in Neo4j, and developed Flask APIs for result delivery.\\n     Implemented using Python 3, NLTK, Gensim, and Pandas.\\n\\nProjects\\n\\nFinanceKernel\\n                                                                                                                                            01-01-2019 - Present\\nPrivate Algo Trading System\\n     Developed a Python scraper for 5000+ equities, capturing Intraday, Daily, Historical data, and company info, and automated CSV processing\\n     to AWS S3 and SQS. Tech Used : [Concurrent.Futures, requests, lxml, pandas]\\n     Optimized data processing by transitioning from Python to GoLang for SQS message polling and streaming CSVs from S3 to MongoDB,\\n     cutting processing time from 5 minutes to 40 seconds on a 2GB EC2 machine. Tech Used : [GoRoutines, Channels, Generator concurrency\\n     pattern, MongoDB]\\n     Implemented a Python Dask-based distributed financial feature extraction pipeline for 5000+ equities, leveraging AWS spot instances for\\n     database updates. Tech Used: [Dask, Python]\\n     Engineered a Machine Learning Stock Screener, achieving a 0.97 F1 Score, effectively predicting next-day stock performance. Tech Used:\\n     [Scikit-Learn, Pandas, Python]\\n\\nResearchKernel\\n                                                                                                                                             09/01/2018 - Present\\nOpen Source Project for Researcher\\n\\nhttps://github.com/ResearchKernel\\n\\n     Started the open source project as a community-driven interface for search research papers and generating recommendation with Github\\n     integrated to research papers. Tech Used: [Python, AWS CLI, Neo4j, Elasticsearch, Node.js, React.js, Airflow, Tensorflow, Keras]\\n     Developed an ETL pipeline for extracting texts from PDF using python multiprocessing framework, resolving the Cold Start issue by training\\n     Doc2vec on one million papers synced to s3 [Github]. Tech Used: [Python, Multiprocessing, AWS CLI, S3, Terraform]\\n     Implemented Neo4j for ML result storage and crafted a Python backend for recommendation delivery and Reduced infrastructure costs by\\n     60% using AWS spot instances. Tech Used: [Python, Keras, Doc2Vec, ShellScript, Neo4j]\\n\\nSkills\\n\\nProgramming Languages\\nJava, Python, Go, Lua, Javascript\\n\\nFramework and Technology\\nSpring Boot, Express.js, Flask, Mux, Spring WebFlux, Celery, Airflow, Kafka, SQS, Redis Bull Queue, Node.js\\n\\nDatabase\\nElasticsearch, MongoDB, MYSQL, SAP HANA, Neo4j\\n\\nMachine Learning\\nMLFlow, Scikit-learn, Pandas, Numpy, Keras, Tensorflow\\n\\nCloud Technologies\\nAWS, S3, SQS, CloudFoundry, ECS, SNS, EC2, EBS, SAP HANA\\n\\nOther Tech\\nnvim, grep, awk, GNU Parallel, pyfolio, backtrader, jupyter notebook, Docker Compose, Kibana, Logstash, Filebeat, Nginx, XGBoost, Celery, REST APIs, WebSockets,\\nObject-Oriented Design, Design Patterns, Data Structure\\n\\nEducation\\n\\nBharati Vidyapeeth College of Engineering (IP University)                                                                                01/08/2013 - 01/07/2017\\nComputer Science and Engineering                                                                                                                          B.Tech\\n\\n                This is the output in the required_format\\n\\n                '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483f03d-26f1-482c-a77b-5dc448646f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d1fb5-ec28-4db1-8029-a48265342024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7449dc2d-4313-42be-ad48-793192153915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "def get_output(eval_prompt):\n",
    "\n",
    "    '''\n",
    "    This function takes in an evaluation prompt, and returns the output generated \n",
    "    by the model_name LLM. We can also use max_tokens as a parameter\n",
    "    '''\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = \"http://localhost:8000/v1\"\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    \n",
    "    completion = client.completions.create(model=\"lakshay/work-details-mistral\",\n",
    "                                          prompt=work_prompt,\n",
    "                                          max_tokens=512)\n",
    "\n",
    "    print(\"Completion result:\\n\", completion.choices[0].text)    \n",
    "    print(f'time taken : {time.time()-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c7e43-018b-4046-8ccc-5f23150977af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25142b89-ca0e-4712-b9a0-3e7acdd71d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion result:\n",
      "             [{\"company\":\"InfoEdge (IIMJOBS/HIRIST)\", \"role\":\"Lead Java Backend Developer\", \"start_date\":\"01/08/2022\", \"end_date\":\"present\"}, {\"company\":\"StepingCloud\", \"role\":\"Software Engineer\", \"start_date\":\"01/03/2018\", \"end_date\":\"31/07/2022\"}]\n",
      "\n",
      "\n",
      "time taken : 3.9034011363983154\n"
     ]
    }
   ],
   "source": [
    "get_output(work_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a9c3bc8-3884-41bb-b4c6-de2d789db055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion result:\n",
      "     [\n",
      "                        {\"company\":\"InfoEdge (IIMJOBS/HIRIST)\", \"role\":\"Lead Java Backend Developer\", \"start_date\":\"01/2022\", \"end_date\":\"present\"},\n",
      "                        {\"company\":\"SteppingCloud\", \"role\":\"Software Engineer\", \"start_date\":\"01/2018\", \"end_date\":\"07/2022\"},\n",
      "                        {\"company\":\"Goeventz\", \"role\":\"Machine Learning Engineer\", \"start_date\":\"07/2017\", \"end_date\":\"03/2018\"}\n",
      "                    ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This is the  expected_output_format:\n",
      "  \"content\": [\n",
      "    {\"company\": \"company1\", \"role\": \"role1\", \"start_date\": \"mm/yyyy\", \"end_date\": \"mm/yyyy\"},\n",
      "    {\"company\": \"company2\", \"role\": \"role2\", \"start_date\": \"mm/yyyy\", \"end_date\": \"mm/yyyy\"}\n",
      "    ],\n",
      "    \"resume_format\": \"json\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Please convert the above data into the required output format and Please add the required\n",
      "\n",
      "\n",
      "\n",
      "This is the output in the required_format:\n",
      " [\n",
      "    {\"company\":\"Infoedge (IIMJOBS/HIRIST)\",\"role\":\"Lead Java Backend Developer\",\"start_date\":\"01/2022\",\"end_date\":\"present\"},\n",
      "    {\"company\":\"SteppingCloud\",\"role\":\"Software Engineer\",\"start_date\":\"01/2018\",\"end_date\":\"07/2022\"},\n",
      "    {\"company\":\"Goeventz\",\"role\":\"Machine Learning Engineer\",\"start_date\":\"01/2017\",\"end_date\":\"03/2018\"}\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "Please act as a productive agent and provide the output in the required format along with the\n",
      "required details. Here is the required output format:\n",
      "\n",
      "This is the  expected_output_format:\n",
      "  \"content\": [\n",
      "    {\"company\": \"company1\", \"role\": \"role1\", \"start_date\": \"mm/yyyy\", \"end_date\": \"mm/yyyy\"},\n",
      "    {\"company\": \"\n",
      "time taken : 17.54282283782959\n"
     ]
    }
   ],
   "source": [
    "get_output(edu_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca6699-6efb-4038-a50e-247a2ffb96d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208f21b-d03a-49ea-af9b-283e131d43df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "554cda1e-3e4a-454e-8cf5-ed97aaae5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_text = res_df['resume_text'].sample().values[0]\n",
    "\n",
    "work_prompt = html.unescape(make_workex_prompt(eval_text))\n",
    "edu_prompt = html.unescape(make_edu_prompt(eval_text))\n",
    "\n",
    "s_prompt = [work_prompt,edu_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d3788f4-f68e-43d6-b3c9-a4857a08d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = res_df['user_id'].sample(4).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99bf62f3-93de-4139-a2c5-a8dc7677b467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2826852', '2827173', '2826837', '2826359'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6284fc9-78ed-4b29-bd27-9cf5fd3e4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "works = [make_workex_prompt(rt) for rt in sample_ids]\n",
    "edus = [make_edu_prompt(rt) for rt in sample_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b4c57-2629-46d1-9186-022687d5cfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364de776-5b03-40bb-8263-82ff74ec50a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "63e1e1a7-d5f8-4a46-8b9b-ddf4e73ecd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_output(eval_prompt):\n",
    "    '''\n",
    "    This function takes in an evaluation prompt, and returns the output generated \n",
    "    by the model_name LLM. We can also use max_tokens as a parameter\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    # Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = \"http://localhost:8000/v1\"\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    \n",
    "    completion = client.completions.create(model=\"lakshay/work-details-mistral\",\n",
    "                                          prompt=eval_prompt,\n",
    "                                          max_tokens=512)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return completion.choices[0].text, elapsed_time\n",
    "\n",
    "def main():\n",
    "    # Define the evaluation prompt\n",
    "    # eval_prompt = \"Your evaluation prompt here\"\n",
    "    eval_text = res_df['resume_text'].sample().values[0]\n",
    "    eval_prompt = html.unescape(make_workex_prompt(eval_text))\n",
    "\n",
    "    # Define the number of concurrent requests to make\n",
    "    num_requests = 10\n",
    "\n",
    "    # Create a ThreadPoolExecutor with max_workers equal to the number of concurrent requests\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
    "        # Submit concurrent requests\n",
    "        futures = [executor.submit(get_output, eval_prompt) for _ in range(num_requests)]\n",
    "        \n",
    "        # Wait for all requests to complete\n",
    "        concurrent.futures.wait(futures)\n",
    "\n",
    "        # Process the results\n",
    "        its = []\n",
    "        for future in futures:\n",
    "            result, elapsed_time = future.result()\n",
    "            # print(\"Completion result:\\n\", result)\n",
    "            # print(f\"Time taken: {elapsed_time} seconds\")\n",
    "            its.append(elapsed_time)\n",
    "            print(\"-\" * 50)\n",
    "    return its\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "daf4aa41-8de9-4c3c-8f96-56a6f383e611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b435553c53f40878e9bcaffcca93ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "d = [main() for _ in tqdm(range(10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf6043-0a54-4e7c-85e0-8610615d1cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c98019e6-0af8-45ea-a3af-4ab4f614c87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.853334250450136"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(ts) for ts in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff3c4f-2ae4-40d2-a80b-c8185e6b820d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba340b0f-3198-42e2-abf6-150f7eb9ce1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e04cf-3833-4018-836a-8aae4b844be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e860d365-0c4f-4be6-a0a8-55215907767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     p = Pool(4)\n",
    "#     p.map(get_output,s_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b23b35-de3f-44ab-99a0-3fbaaf6fccbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ff6a626-aec6-4026-96b0-ef688d583785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = [make_input_prompt(j) for j in res_df['resume_text'].values[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f54a0c15-9340-4918-b79f-2f7ebcfe50ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dd880-a602-4897-b234-db0456c9d6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dd52dd3-7c06-45c3-b068-ab248ee4e1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = {}\n",
    "\n",
    "# for D in batches[:1]:\n",
    "#     res_texts = D[:,1]\n",
    "#     input_prompts = [make_input_prompt(x) for x in res_texts]\n",
    "    \n",
    "#     mistral_outputs = llm.generate(input_prompts, sampling_params)\n",
    "    \n",
    "#     for userid,J in zip(D[:,0],mistral_outputs):\n",
    "#         results[userid] = J.outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb2cf5-48ad-453e-aade-e91521f11003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31b30e-233b-4c1b-9851-884ef77930da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96581f26-3aa9-4846-a44d-6956d220e872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2407741-dcf9-42a4-94f5-bcf00af086da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fecd41fe-2210-40fc-a870-04ead9a2a7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>resume_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2826298</td>\n",
       "      <td>William Joseph Kakkassery\\nwilliamjosephk1992@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2826299</td>\n",
       "      <td>URMEET SINGH\\n+91 9307444144 | urmeet522@gmail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2826304</td>\n",
       "      <td>Rupesh Gulyani\\nContact: +91 - 9990510412\\nE-M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2826305</td>\n",
       "      <td>Govind Sai Bisai, Data Analyst\\nWhitefield, Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2826309</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>2827743</td>\n",
       "      <td>PROFILE SUMMARY:\\nAn Enthusiastic and highly m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>2827745</td>\n",
       "      <td>Pleasant Kemisola Folorunso\\nAddress : N o 6 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>2827746</td>\n",
       "      <td>Nikitasa Nanda\\nPhone No. : +919113889161\\nEma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>2827748</td>\n",
       "      <td>GET IN CONTACT\\nMobile: +91-8660946102\\nEmail:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>2827751</td>\n",
       "      <td>CURRICULUM VITAE\\nASHISH BHARATRAO KOLHE\\nRoya...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id                                        resume_text\n",
       "0    2826298  William Joseph Kakkassery\\nwilliamjosephk1992@...\n",
       "1    2826299  URMEET SINGH\\n+91 9307444144 | urmeet522@gmail...\n",
       "2    2826304  Rupesh Gulyani\\nContact: +91 - 9990510412\\nE-M...\n",
       "3    2826305  Govind Sai Bisai, Data Analyst\\nWhitefield, Be...\n",
       "4    2826309                                                   \n",
       "..       ...                                                ...\n",
       "715  2827743  PROFILE SUMMARY:\\nAn Enthusiastic and highly m...\n",
       "716  2827745  Pleasant Kemisola Folorunso\\nAddress : N o 6 3...\n",
       "717  2827746  Nikitasa Nanda\\nPhone No. : +919113889161\\nEma...\n",
       "718  2827748  GET IN CONTACT\\nMobile: +91-8660946102\\nEmail:...\n",
       "719  2827751  CURRICULUM VITAE\\nASHISH BHARATRAO KOLHE\\nRoya...\n",
       "\n",
       "[720 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b3e06-2296-4dab-9a77-4e023337afe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e22ff-ae2d-4469-828b-d773dfccf01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07a2a8-81cf-45fd-b2aa-00625eca2f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bfc0b-971a-4d97-9870-fd07dd90d94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ee357-8785-40a0-a319-e032e15ffce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
