{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c141ca86-c173-41cc-b357-5324318aa727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "login(token='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')\n",
    "\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35fb721-b69b-4592-ace5-ef7f73435658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fb561a-c409-4bce-8c79-c84b266d20dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import time, json, ast, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cfedcc-3516-44fc-991a-f159abf1850e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-29 12:49:40 llm_engine.py:73] Initializing an LLM engine with config: model='lakshay/mistral-work-desc', tokenizer='lakshay/mistral-work-desc', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 198.12 MiB is free. Process 2770 has 17.75 GiB memory in use. Including non-PyTorch memory, this process has 4.03 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 13.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlakshay/mistral-work-desc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/entrypoints/llm.py:105\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     88\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     89\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     90\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    104\u001b[0m )\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/engine/llm_engine.py:250\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    247\u001b[0m distributed_init_method, placement_group \u001b[38;5;241m=\u001b[39m initialize_cluster(\n\u001b[1;32m    248\u001b[0m     parallel_config)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdistributed_init_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m             \u001b[49m\u001b[43mplacement_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/engine/llm_engine.py:110\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, distributed_init_method, placement_group, log_stats)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_workers_ray(placement_group)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistributed_init_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_cache()\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/engine/llm_engine.py:146\u001b[0m, in \u001b[0;36mLLMEngine._init_workers\u001b[0;34m(self, distributed_init_method)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mappend(worker)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    144\u001b[0m     get_all_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    145\u001b[0m )\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_all_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_concurrent_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_parallel_loading_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/engine/llm_engine.py:755\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, get_all_outputs, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m     work_groups \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers]\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m workers \u001b[38;5;129;01min\u001b[39;00m work_groups:\n\u001b[1;32m    754\u001b[0m     all_outputs\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 755\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers_in_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_all_outputs:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_outputs\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/engine/llm_engine.py:729\u001b[0m, in \u001b[0;36mLLMEngine._run_workers_in_batch\u001b[0;34m(self, workers, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m         executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(worker, method)\n\u001b[0;32m--> 729\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m     all_outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mworker_use_ray:\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/worker/worker.py:79\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/worker/model_runner.py:57\u001b[0m, in \u001b[0;36mModelRunner.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/model_executor/model_loader.py:65\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_default_torch_dtype(model_config\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Create a model instance.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# The weights will be initialized as empty tensors.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 65\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mload_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdummy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;66;03m# NOTE(woosuk): For accurate performance evaluation, we assign\u001b[39;00m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;66;03m# random values to the weights.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         initialize_dummy_weights(model)\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:271\u001b[0m, in \u001b[0;36mMistralForCausalLM.__init__\u001b[0;34m(self, config, linear_method)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_method \u001b[38;5;241m=\u001b[39m linear_method\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mMistralModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m ParallelLMHead(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler \u001b[38;5;241m=\u001b[39m Sampler(config\u001b[38;5;241m.\u001b[39mvocab_size)\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:233\u001b[0m, in \u001b[0;36mMistralModel.__init__\u001b[0;34m(self, config, linear_method)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    230\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    231\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    232\u001b[0m )\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    234\u001b[0m     MistralDecoderLayer(config, linear_method)\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    236\u001b[0m ])\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:234\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    230\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    231\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m--> 234\u001b[0m     \u001b[43mMistralDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    236\u001b[0m ])\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:177\u001b[0m, in \u001b[0;36mMistralDecoderLayer.__init__\u001b[0;34m(self, config, linear_method)\u001b[0m\n\u001b[1;32m    168\u001b[0m rope_theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_theta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m MistralAttention(\n\u001b[1;32m    170\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    171\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     linear_method\u001b[38;5;241m=\u001b[39mlinear_method,\n\u001b[1;32m    176\u001b[0m     sliding_window\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39msliding_window)\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mMistralMLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinear_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    184\u001b[0m                                eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    186\u001b[0m                                         eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:62\u001b[0m, in \u001b[0;36mMistralMLP.__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act, linear_method)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     56\u001b[0m     hidden_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     linear_method: Optional[LinearMethodBase] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_up_proj \u001b[38;5;241m=\u001b[39m \u001b[43mMergedColumnParallelLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinear_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m RowParallelLinear(intermediate_size,\n\u001b[1;32m     67\u001b[0m                                        hidden_size,\n\u001b[1;32m     68\u001b[0m                                        bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     69\u001b[0m                                        linear_method\u001b[38;5;241m=\u001b[39mlinear_method)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hidden_act \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:256\u001b[0m, in \u001b[0;36mMergedColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, linear_method)\u001b[0m\n\u001b[1;32m    254\u001b[0m tp_size \u001b[38;5;241m=\u001b[39m get_tensor_model_parallel_world_size()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(output_size \u001b[38;5;241m%\u001b[39m tp_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output_size \u001b[38;5;129;01min\u001b[39;00m output_sizes)\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgather_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mskip_bias_add\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:176\u001b[0m, in \u001b[0;36mColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, linear_method)\u001b[0m\n\u001b[1;32m    174\u001b[0m     linear_method \u001b[38;5;241m=\u001b[39m UnquantizedLinearMethod()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_method \u001b[38;5;241m=\u001b[39m linear_method\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weight, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:55\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.create_weights\u001b[0;34m(self, input_size_per_partition, output_size_per_partition, input_size, output_size, params_dtype)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     52\u001b[0m                    output_size_per_partition: \u001b[38;5;28mint\u001b[39m, input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     53\u001b[0m                    output_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     54\u001b[0m                    params_dtype: torch\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 55\u001b[0m     weight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     59\u001b[0m                        requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     60\u001b[0m     set_weight_attrs(weight, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: weight}\n",
      "File \u001b[0;32m~/infoedge/llama-recipes/env/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 198.12 MiB is free. Process 2770 has 17.75 GiB memory in use. Including non-PyTorch memory, this process has 4.03 GiB memory in use. Of the allocated memory 3.58 GiB is allocated by PyTorch, and 13.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=2000)\n",
    "llm = LLM(model=\"lakshay/mistral-work-desc\", max_model_len=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0df83f-69df-479d-a112-8db4746cf28d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## initialise and run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd4f08-5c80-4e07-a1ea-a940a9e2a5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da953dee-842d-4544-9696-f8ec20a5fd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/test_user_resume.json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b6b02c-8d12-4034-a160-efd253d9dae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for user_id, resume_text in data.items():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d541ce12-6d66-4dfc-813f-c57b7048e680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame([data]).T.reset_index()\n",
    "res_df = res_df.rename(columns={'index': 'user_id', 0: 'resume_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7481de-57e5-4409-a4f4-1ca63e073877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b03d2-65bc-4ae1-ac82-07fa479637db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e3c342-fe42-4781-a154-31c9b6970d52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_prompt = f'''\n",
    "You are an accurate agent working for a job platform. You will be given the raw \n",
    "unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
    "user from the resume. Please provide the data in a concise JSON format. The JSON should include a \n",
    "\"work_experience\" key with an array of objects. Each object represents a job and should contain keys for \n",
    "\"company\", \"role\", \"start_date\", \"end_date\". Dates should be in \"mm/yyyy\" format. \n",
    "Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces.\n",
    "\n",
    "Please follow this structure closely and keep the response within the token limit.\" \\n{{query_format}}\\n\n",
    "\n",
    "This is the resume text:\\n{{resume_text}}\\n\n",
    "This is the output in the required_format\\n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd56e8a-83f2-4d07-a9f9-9bebad0a9006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_format = '''\n",
    "[\n",
    "    {\"company\":\"Example Company 1\",\n",
    "    \"role\":\"Example Role 1\",\n",
    "    \"start_date\":\"mm/yyyy\",\n",
    "    \"end_date\":\"mm/yyyy\",\n",
    "    \"description\":\"Example Description 1\"},\n",
    "    {\"company\":\"Example Company 2\",\n",
    "    \"role\":\"Example Role 2\",\"\n",
    "    start_date\":\"mm/yyyy\",\n",
    "    \"end_date\":\"mm/yyyy\",\n",
    "    \"description\":\"Example Description 2\"}\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b3ef08e-e84b-4c01-9c27-3b9b7d2bbdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_input_prompt(rt):\n",
    "    \n",
    "    prompt_text = f'''You are an accurate agent working for a job platform. You will be given the raw \n",
    "                unstructured text of a user's resume, and the task is to extract the entire work experience of the \n",
    "                user from the resume. Please provide the data in a concise JSON format. The JSON should include a \n",
    "                \"work_experience\" key with an array of objects. Each object represents a job and should contain keys for \n",
    "                \"company\", \"role\", \"start_date\", \"end_date\". Dates should be in \"mm/yyyy\" format. \n",
    "                Ensure the JSON syntax is correct, with proper use of quotes, commas, and braces.\n",
    "\n",
    "                Please follow this structure closely and keep the response within the token limit.\" \\n{{query_format}}\\n\n",
    "\n",
    "                This is the resume text:\\n{{resume_text}}\\n\n",
    "                This is the output in the required_format\\n\n",
    "                '''\n",
    "    \n",
    "    work_format = '''\n",
    "                [\n",
    "                    {\"company\":\"Example Company 1\",\n",
    "                    \"role\":\"Example Role 1\",\n",
    "                    \"start_date\":\"mm/yyyy\",\n",
    "                    \"end_date\":\"mm/yyyy\",\n",
    "                    \"description\":\"Example Description 1\"},\n",
    "                    {\"company\":\"Example Company 2\",\n",
    "                    \"role\":\"Example Role 2\",\"\n",
    "                    start_date\":\"mm/yyyy\",\n",
    "                    \"end_date\":\"mm/yyyy\",\n",
    "                    \"description\":\"Example Description 2\"}\n",
    "                ]\n",
    "                '''\n",
    "    \n",
    "    return prompt_text.format(\n",
    "            query_format=work_format,\n",
    "            resume_text=rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e67961-66f4-412c-811e-69ced1fbea92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff6a626-aec6-4026-96b0-ef688d583785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = [make_input_prompt(j) for j in res_df['resume_text'].values[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54a0c15-9340-4918-b79f-2f7ebcfe50ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f622e527-261a-42d5-a4c7-11c3a16e488d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# outputs = llm.generate(v,sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097bb82e-11e2-4ebb-80c4-e6dc2d09d6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "out = outputs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3044b5af-3562-4e6a-8c35-8a8478b7c588",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ast\u001b[38;5;241m.\u001b[39mliteral_eval(html\u001b[38;5;241m.\u001b[39munescape(\u001b[43mout\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "ast.literal_eval(html.unescape(out[0].outputs[0].text.replace('\\n','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b21ea9-8e89-4a7e-9ab6-3aeeb472aee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecba18b-0765-484b-a2e9-2f35c339077a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fa928-b7a5-4490-a360-37f51f1c8704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a91bd-0f04-47da-a584-526c7c267cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6675c1-96de-4b2d-98fa-d6921538fff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for j in v:\n",
    "#     llm.generate(j, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fd93202-2092-4948-ba9e-5863dd434407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85824e9f-1a2a-46a7-bb65-2998256cfa2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010445ee-be7b-4038-9f47-3b8455d8a163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d712e218-6110-47c5-9cc8-7b5e85c99fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# res_df.values.tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c949cbd-2813-423e-9fb4-d798c4977c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = res_df.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19e8af95-e026-47e4-8071-f97a6450b97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batches = [s[i:i + 5] for i in range(0, len(s), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa810f5-8136-451c-b0d0-2a55be5826af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1dead3-8ddc-4d31-a4a4-e861973f22f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dd880-a602-4897-b234-db0456c9d6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dd52dd3-7c06-45c3-b068-ab248ee4e1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = {}\n",
    "\n",
    "# for D in batches[:1]:\n",
    "#     res_texts = D[:,1]\n",
    "#     input_prompts = [make_input_prompt(x) for x in res_texts]\n",
    "    \n",
    "#     mistral_outputs = llm.generate(input_prompts, sampling_params)\n",
    "    \n",
    "#     for userid,J in zip(D[:,0],mistral_outputs):\n",
    "#         results[userid] = J.outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb2cf5-48ad-453e-aade-e91521f11003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31b30e-233b-4c1b-9851-884ef77930da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2407741-dcf9-42a4-94f5-bcf00af086da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8e156-a3d8-4525-81d4-21a6a57b1427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2b2f64d2144cfe934c732e1ea2fb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.25s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.31s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.12s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.82s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:46<00:00, 46.88s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:33<00:00, 33.29s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.36s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.81s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 23.00s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.60s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.42s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.61s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.89s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.92s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.86s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:34<00:00, 34.18s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.10s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.06s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:43<00:00, 43.18s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [01:05<00:00, 65.27s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:38<00:00, 38.44s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.87s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.95s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.06s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.60s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.30s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:34<00:00, 34.25s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.79s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.28s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.28s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.22s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.92s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.86s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.09s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:30<00:00, 30.40s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:47<00:00, 47.11s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.47s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.76s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.13s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.59s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:42<00:00, 42.55s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.51s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:29<00:00, 29.68s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.68s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.73s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.38s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.76s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.27s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.21s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.53s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.56s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.35s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.16s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.62s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.66s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.57s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.19s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [01:05<00:00, 65.94s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.83s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:45<00:00, 45.25s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.90s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:36<00:00, 36.89s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:40<00:00, 40.61s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.33s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-27 12:55:32 scheduler.py:195] Input prompt (4593 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 1696.04it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.28s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [01:05<00:00, 65.26s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:33<00:00, 33.44s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.66s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:33<00:00, 33.82s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.56s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.89s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.77s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.46s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:40<00:00, 40.27s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.73s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:47<00:00, 47.88s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.22s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.74s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.54s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.85s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:55<00:00, 55.56s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.82s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.06s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.15s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.12s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.12s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:37<00:00, 37.21s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.88s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:29<00:00, 29.63s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [01:02<00:00, 62.31s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.22s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.07s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.83s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.65s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.97s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.05s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.27s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.70s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.73s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.52s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:44<00:00, 44.02s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.54s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:49<00:00, 49.34s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.64s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:29<00:00, 29.28s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.27s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:45<00:00, 45.45s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.06s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.61s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.46s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.71s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.16s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.95s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:41<00:00, 41.59s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.10s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.58s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.68s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-27 15:14:17 scheduler.py:195] Input prompt (4608 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 1746.90it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.85s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:37<00:00, 37.24s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.68s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:34<00:00, 34.52s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.50s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.72s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.19s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.83s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.13s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.16s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.31s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.87s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.06s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.91s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.42s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:45<00:00, 45.45s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-27 15:20:18 scheduler.py:195] Input prompt (178229 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 487.71it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.75s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.96s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.07s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.67s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.36s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.30s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:40<00:00, 40.82s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.97s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.92s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:34<00:00, 34.36s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.72s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:34<00:00, 34.86s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.12s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.19s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.31s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.77s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:44<00:00, 44.35s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.62s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.78s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.60s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.57s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.51s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-27 15:27:13 scheduler.py:195] Input prompt (4389 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 1832.37it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.42s/it]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-27 15:27:30 scheduler.py:195] Input prompt (8195 tokens) is too long and exceeds limit of 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 1685.81it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}    \n",
    "errors = []\n",
    "for userid, resume_text in tqdm(res_df.values.tolist()):\n",
    "    \n",
    "    try:\n",
    "        rt = html.unescape(resume_text)\n",
    "        eval_prompt = work_prompt.format(\n",
    "                query_format=work_format,\n",
    "                resume_text=rt)\n",
    "\n",
    "        outputs = llm.generate(eval_prompt, sampling_params)\n",
    "\n",
    "        results[userid] = outputs[0].outputs[0].text\n",
    "    except:\n",
    "        errors.append(userid)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da324f56-92ce-40ed-912c-b14043a059b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d977f60f-d5ad-418c-8fe8-8a8a45f078b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('data/test_results.json','w') as f:\n",
    "#     json.dump(results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bddb2-7050-4b47-9893-63fb9717ae25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1218f-e50e-46af-a482-eeefdcb8947d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ee357-8785-40a0-a319-e032e15ffce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
